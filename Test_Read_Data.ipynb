{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Reading and Manipulation of Data\n",
    "\n",
    "This is a testbed Jupyter notebook to read in the data we need for the COVID Data Vizualization Project.\n",
    "\n",
    "- Possible data sources that we are considering are imported and manipulated below just to provide demonstrations of how it is done.  \n",
    "\n",
    "- I am currently using `Pandas` (aka Pythan Data Analysis Library, see https://pandas.pydata.org) to read in the CSV files and manipualte them.  This has advantages and annoyances, there may be much better ways to do this, but I was giving this a try for now.\n",
    "\n",
    "- Some of the data includes FIPS codes (a standard geographic identifier) which should ease the process of cross-matching of data.  Clay County is 27027 and Cass County is 38017.  Minnesota is 27, North Dakota is 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import git\n",
    "import requests\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables of interest below\n",
    "data_dir = 'our_data/'    # Data directory for files we created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Census Data on Populations of States/Counties \n",
    "\n",
    "This is data from the US Census Bureau estimating the population in July 2019.  Description of the file format is at https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf\n",
    "\n",
    "- **County Level Data**: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\n",
    "- **State Level Data**: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve state level data (restricted to certain columns)\n",
    "## When I retrieved the files, I got an error that `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 2: invalid continuation byte`, turns out it is encoded `latin-1`.\n",
    "#census_state_csv = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\"\n",
    "#state_columns_of_interest = {'STATE', 'NAME', 'CENSUS2010POP', 'N_POPCHG2019', 'POPESTIMATE2019'}\n",
    "#census_state_df = pd.read_csv(census_state_csv, usecols=state_columns_of_interest, encoding='latin-1')    # County totals\n",
    "\n",
    "# Create pandas dataframes containing the selected population data for each state/county\n",
    "census_county_csv = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\"\n",
    "county_columns_of_interest = {'STATE', 'COUNTY', 'STNAME', 'CTYNAME', 'NPOPCHG_2019', 'POPESTIMATE2019'}\n",
    "census_county_df = pd.read_csv(census_county_csv,usecols=county_columns_of_interest, encoding='latin-1')  \n",
    "\n",
    "# Separate state level data from county level data (by creating separate copies in memory)\n",
    "county_data_df = census_county_df[census_county_df['COUNTY'] != 0].copy()\n",
    "state_data_df = census_county_df[census_county_df['COUNTY'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate the state-leve data\n",
    "\n",
    "# Add FIPS column for state data then DROP county data and move FIPS to first column before exporting\n",
    "state_data_df['FIPS'] = state_data_df['STATE']\n",
    "state_data_df.drop(columns=['STATE','COUNTY','CTYNAME'], inplace=True)\n",
    "state_data_df = state_data_df.reindex(columns=(['FIPS'] + list([col for col in state_data_df.columns if col != 'FIPS']) ))\n",
    "\n",
    "# Compute percent change in population in 2018-19\n",
    "state_data_df['PPOPCHG_2019'] = 100*(state_data_df['NPOPCHG_2019']/state_data_df['POPESTIMATE2019'])\n",
    "\n",
    "# We may want to do a daily extrapolation of population since POPESTIMATE2019 is est. population on July 1, 2019 \n",
    "# and NPOPCHG_2019 is the estimated change between July 1, 2018 and July 1, 2019.  Realistically, this is probably\n",
    "# overkill since the increased deaths from Coronavirus are not taken into account in such an extrapolation.\n",
    "\n",
    "# Save the processed data file\n",
    "out_states = data_dir + \"population_data_states.csv\"\n",
    "state_data_df.to_csv(out_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In county data create FIPS column, remove redundant columns, and then move FIPS columns to first column\n",
    "county_data_df['FIPS'] = county_data_df['STATE']*1000 + county_data_df['COUNTY']\n",
    "county_data_df.drop(columns=['STATE','COUNTY'], inplace=True)\n",
    "county_data_df = county_data_df.reindex(columns=(['FIPS'] + list([col for col in county_data_df.columns if col != 'FIPS']) ))\n",
    "\n",
    "# Compute percent change in population in 2018-19\n",
    "county_data_df['PPOPCHG_2019'] = 100*(county_data_df['NPOPCHG_2019']/county_data_df['POPESTIMATE2019'])\n",
    "\n",
    "# We may want to do a daily extrapolation of population since POPESTIMATE2019 is est. population on July 1, 2019 \n",
    "# and NPOPCHG_2019 is the estimated change between July 1, 2018 and July 1, 2019.  Realistically, this is probably\n",
    "# overkill since the increased deaths from Coronavirus are not taken into account in such an extrapolation.\n",
    "\n",
    "# Save the processed data file\n",
    "out_counties = data_dir + \"population_data_counties.csv\"\n",
    "county_data_df.to_csv(out_counties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_county_df[(census_county_df['FIPS'] == 27027) | (census_county_df['FIPS'] == 38017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_county_df[(census_county_df['CTYNAME'] == 'Minnesota') | (census_county_df['CTYNAME'] == 'North Dakota')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Novel Coronavirus (COVID-19) Cases Data (FIPS Present)\n",
    "    - https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases\n",
    "\n",
    "This dataset is part of COVID-19 Pandemic Novel Corona Virus (COVID-19) epidemiological data since 22 January 2020. The data is compiled by the Johns Hopkins University Center for Systems Science and Engineering (JHU CCSE) from various sources including the World Health Organization (WHO), DXY.cn, BNO News, National Health Commission of the Peopleâ€™s Republic of China (NHC), China CDC (CCDC), Hong Kong Department of Health, Macau Government, Taiwan CDC, US CDC, Government of Canada, Australia Government Department of Health, European Centre for Disease Prevention and Control (ECDC), Ministry of Health Singapore (MOH), and others. JHU CCSE maintains the data on the 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository on Github (https://github.com/CSSEGISandData/COVID-19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we have current data, I went to the directory containing this Jupyter script and \n",
    "# issued a \"git clone https://github.com/CSSEGISandData/COVID-19.git\" to\n",
    "# pull the complete dataset from GitHub.  That created a COVID-19 directory, \n",
    "# which I renamed \"JH_Data\". Now it should be a short \"git pull\" command to keep the data up to date.\n",
    "\n",
    "JHdata_dir = \"JH_Data/\"\n",
    "g = git.cmd.Git(JHdata_dir)\n",
    "status = g.pull()  # We should check status to see everything is good eventually, for now, I am using this to hide the status message from GitPython module\n",
    "\n",
    "# Another choice would be to autogenerate the CSV URLs and retrieve them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csse_covid_19_data/csse_covid_19_daily_reports contains a daily CSV with a list of confirmed/deaths/recovered for each admin unit (in the US, that's county) for each day.\n",
    "# csse_covid_19_data/csse_covid_19_daily_reports_us contains a daily CSV with list of confirmed/deaths/recovered totaled for each state (somewhat redundant, but avoids recomputation I suppose)\n",
    "\n",
    "# Construct filename of yesterday's CSV datafile\n",
    "yesterday = date.today() - timedelta(days = 1) \n",
    "yesterday_csv = f\"{yesterday.month:02d}-{yesterday.day:02d}-{yesterday.year:04d}.csv\"\n",
    "\n",
    "# Build daily data filenames\n",
    "daily_world_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports/\"+yesterday_csv\n",
    "daily_us_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports_us/\"+yesterday_csv\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files (contains number of confirmed/deaths/recovered on that date)\n",
    "daily_world_df = pd.read_csv(daily_world_csv)   # County/Admin totals\n",
    "daily_us_df = pd.read_csv(daily_us_csv)         # State totals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_world_df[ (daily_world_df['FIPS'] == 1001) | (daily_world_df['FIPS'] == 38017) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_us_df[ (daily_us_df['FIPS'] == 27) | (daily_us_df['FIPS'] == 38) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the time series datafiles to experiment with them.\n",
    "\n",
    "# Create pandas dataframes containing time-series data (We could reconstruct this by looping through all the daily data, since this is missing number of recovered)\n",
    "ts_us_dead_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n",
    "ts_us_confirmed_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n",
    "ts_us_dead_df = pd.read_csv(ts_us_dead_csv)            # Deaths in time series\n",
    "ts_us_confirmed_df = pd.read_csv(ts_us_confirmed_csv)  # Confirmed in time series\n",
    "\n",
    "# We could transpose the dataframe to allow easier extraction of time series data on a per county level\n",
    "tmp_df = ts_us_confirmed_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"confirmed\" }, inplace = True)\n",
    "confirmed_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')]  # Use pattern matching to find real dates and include\n",
    "\n",
    "tmp_df = ts_us_dead_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"dead\" }, inplace = True)\n",
    "dead_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')] # Use pattern matching to find real dates and include\n",
    "\n",
    "# Merge the confirmed ill and dead into one dataframe (would like recovered too, but that's not in\n",
    "# these times series files)\n",
    "merged_clay = confirmed_clay.merge(dead_clay, left_index=True, right_index=True)\n",
    "merged_clay.plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_clay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Mobility Data (NO FIPS Present)\n",
    "\n",
    "This data is described at https://www.google.com/covid19/mobility/ and can be downloaded in a single monolithic CSV file at https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n",
    "\n",
    "> The data shows how visitors to (or time spent in) categorized places change compared to our baseline days. A baseline day represents a normal value for that day of the week. The baseline day is the median value from the 5â€‘week period Jan 3 â€“ Feb 6, 2020.\n",
    "\n",
    "> For each region-category, the baseline isnâ€™t a single valueâ€”itâ€™s 7 individual values. The same number of visitors on 2 different days of the week, result in different percentage changes. So, we recommend the following:\n",
    "1. Donâ€™t infer that larger changes mean more visitors or smaller changes mean less visitors.\n",
    "2. Avoid comparing day-to-day changes. Especially weekends with weekdays. (https://support.google.com/covid19-mobility/answer/9824897?hl=en&ref_topic=9822927)\n",
    "\n",
    "> Note, *Parks* typically means official national parks and not the general outdoors found in rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Mobility Data URL\n",
    "goog_mobility_csv_url = \"https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\"\n",
    "goog_mobility_df=pd.read_csv(goog_mobility_csv_url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_mobility_clay = goog_mobility_df[ (goog_mobility_df['sub_region_1'] == 'Minnesota') & (goog_mobility_df['sub_region_2'] == 'Clay County')]\n",
    "\n",
    "# Notice for Clay county we have NaN reported for Parks (see note above) and Transit Stations\n",
    "goog_mobility_clay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Mobility Data (NO FIPS Present)\n",
    "\n",
    "This data is described at https://www.apple.com/covid19/mobility and can be downloaded in a single monolithic CSV file at https://covid19-static.cdn-apple.com/covid19-mobility-data/2008HotfixDev42/v3/en-us/applemobilitytrends-2020-05-24.csv (That URL is hidden in the mobility page link and appears to be updated regularly.  We may need to scrape the page to identify the link).\n",
    "\n",
    "Apple tracks three kinds of Apple Maps routing requests: Driving, Walking, Transit.  In some areas not all of these types of data are available, for example, in Clay County, only Driving route request data is available.  In fact, it looks like only driving data is available at the county level regardless of the county (at least in Minnesota). Cities can contain additional formation, although only very large cities appear to be in Apple's data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the original Apple page was proving tricky as it had a bunch of javascript used to generate the URL, so I poked around and found a reference \n",
    "# at https://www.r-bloggers.com/get-apples-mobility-data/ to a JSON file at a stable URL that can be used to construct the appropriate URL for the current\n",
    "# datafile.\n",
    "\n",
    "aapl_mobility_json = \"https://covid19-static.cdn-apple.com/covid19-mobility-data/current/v3/index.json\"\n",
    "aapl_server = \"https://covid19-static.cdn-apple.com/\"\n",
    "result = requests.get(aapl_mobility_json)\n",
    "# Proceed if we successfully pulled the page (HTTP status code 200)\n",
    "if (result.status_code == 200):\n",
    "    # Apple Mobility Data URL\n",
    "    jsondata = result.json()\n",
    "    aapl_mobility_csv_url = aapl_server+jsondata['basePath']+jsondata['regions']['en-us']['csvPath']\n",
    "    aapl_mobility_df=pd.read_csv(aapl_mobility_csv_url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just showing how I can get clay county data specifically\n",
    "aapl_mobility_minneapolis = aapl_mobility_df[(aapl_mobility_df['region'] == 'Minneapolis') & (aapl_mobility_df['sub-region'] == 'Minnesota')]\n",
    "aapl_mobility_clay = aapl_mobility_df[(aapl_mobility_df['region'] == 'Clay County') & (aapl_mobility_df['sub-region'] == 'Minnesota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice only driving information is available here\n",
    "aapl_mobility_clay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice additional information is available for larger cities\n",
    "aapl_mobility_cities = aapl_mobility_df[(aapl_mobility_df['geo_type'] == 'city') & (aapl_mobility_df['country'] == 'United States')]\n",
    "aapl_mobility_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMHE Data on Local Resources\n",
    "\n",
    "There is IMHE data on local resources at http://www.healthdata.org/covid/data-downloads although I am not sure that data is available with county level resolution as I haven't fully investigated it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Times Data on Probable Deaths/Cases (FIPS Present)\n",
    "\n",
    "The NY Times has assembled data on COVID in a GitHub repository at https://github.com/nytimes/covid-19-data.  I have not examined that data yet, but it may well be interesting.\n",
    "\n",
    "Note their statement requiring credit:\n",
    "\n",
    "> In light of the current public health emergency, The New York Times Company is\n",
    "providing this database under the following free-of-cost, perpetual,\n",
    "non-exclusive license. Anyone may copy, distribute, and display the database, or\n",
    "any part thereof, and make derivative works based on it, provided  (a) any such\n",
    "use is for non-commercial purposes only and (b) credit is given to The New York\n",
    "Times in any public display of the database, in any publication derived in part\n",
    "or in full from the database, and in any other public use of the data contained\n",
    "in or derived from the database.\n",
    "\n",
    "Data is available at county, state, and national levels for live numbers (current cases/deaths as well as probable cases/deaths, updated daily).  That said, at least locally I don't think Probable cases are really making a difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the NYT Datafiles\n",
    "NYTdata_dir = \"NYT_Data/\"\n",
    "g = git.cmd.Git(NYTdata_dir)\n",
    "status = g.pull()  # We should check status to see everything is good eventually, for now, I am using this to hide the status message from GitPython module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the live data files\n",
    "live_county_csv = NYTdata_dir+\"live/us-counties.csv\"\n",
    "live_state_csv = NYTdata_dir+\"live/us-states.csv\"\n",
    "live_us_csv = NYTdata_dir+\"live/us.csv\"\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files (contains number of confirmed/deaths/recovered on that date)\n",
    "live_county_df = pd.read_csv(live_county_csv)   # County totals\n",
    "live_state_df = pd.read_csv(live_state_csv)    # State totals\n",
    "live_us_df = pd.read_csv(live_us_csv)       # National totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_county_df[ ((live_county_df['state'] == 'Minnesota') & (live_county_df['county'] == 'Clay')) | ((live_county_df['state'] == 'North Dakota') & (live_county_df['county'] == 'Cass')) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_state_df[ (live_state_df['state'] == 'Minnesota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
