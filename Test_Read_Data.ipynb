{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Reading of Data\n",
    "\n",
    "This is a testbed Jupyter notebook to read in the data we need for the COVID Data Vizualization Project.\n",
    "\n",
    "- Possible data sources that we are considering:\n",
    "    - https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases ",
    "\n",
    "    - https://covid19api.com   (This is just a JSON representation of John Hopkins Data above)\n",
    "    - https://www.google.com/covid19/mobility/ ",
    "\n",
    "    - https://www.apple.com/covid19/mobility\n",
    "    - http://www.healthdata.org/covid/data-downloads\n",
    "    - https://github.com/nytimes/covid-19-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import git\n",
    "import requests\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Novel Coronavirus (COVID-19) Cases Data\n",
    "    - https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases\n",
    "\n",
    "This dataset is part of COVID-19 Pandemic Novel Corona Virus (COVID-19) epidemiological data since 22 January 2020. The data is compiled by the Johns Hopkins University Center for Systems Science and Engineering (JHU CCSE) from various sources including the World Health Organization (WHO), DXY.cn, BNO News, National Health Commission of the People’s Republic of China (NHC), China CDC (CCDC), Hong Kong Department of Health, Macau Government, Taiwan CDC, US CDC, Government of Canada, Australia Government Department of Health, European Centre for Disease Prevention and Control (ECDC), Ministry of Health Singapore (MOH), and others. JHU CCSE maintains the data on the 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository on Github (https://github.com/CSSEGISandData/COVID-19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we have current data, I went to the directory containing this Jupyter script and \n",
    "# issued a \"git clone https://github.com/CSSEGISandData/COVID-19.git\" to\n",
    "# pull the complete dataset from GitHub.  That created a COVID-19 directory, \n",
    "# which I renamed \"JH_Data\". Now it should be a short \"git pull\" command to keep the data up to date.\n",
    "\n",
    "JHdata_dir = \"JH_Data/\"\n",
    "g = git.cmd.Git(JHdata_dir)\n",
    "status = g.pull()  # We should check status to see everything is good eventually, for now, I am using this to hide the status message from GitPython module\n",
    "\n",
    "# Another choice would be to autogenerate the CSV URLs and retrieve them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csse_covid_19_data/csse_covid_19_daily_reports contains a daily CSV with a list of confirmed/deaths/recovered for each admin unit (in the US, that's county) for each day.\n",
    "# csse_covid_19_data/csse_covid_19_daily_reports_us contains a daily CSV with list of confirmed/deaths/recovered totaled for each state (somewhat redundant, but avoids recomputation I suppose)\n",
    "\n",
    "# Construct filename of yesterday's CSV datafile\n",
    "yesterday = date.today() - timedelta(days = 1) \n",
    "yesterday_csv = f\"{yesterday.month:02d}-{yesterday.day:02d}-{yesterday.year:04d}.csv\"\n",
    "\n",
    "# Build daily data filenames\n",
    "daily_world_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports/\"+yesterday_csv\n",
    "daily_us_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports_us/\"+yesterday_csv\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files (contains number of confirmed/deaths/recovered on that date)\n",
    "daily_world_df = pd.read_csv(daily_world_csv)   # County/Admin totals\n",
    "daily_us_df = pd.read_csv(daily_us_csv)         # State totals\n",
    "\n",
    "# csse_covid_19_data/csse_covid_19_time_series contains time-series (so the location, the COVID status, and the number, one column for each date) \n",
    "#                                                  time_series_covid19_confirmed_US.csv, time_series_covid19_deaths_US.csv listing county-level data in the US and \n",
    "#                                                  time_series_covid19_confirmed_global.csv, time_series_covid19_deaths_global.csv, and time_series_covid19_recovered_global.csv for the world.\n",
    "\n",
    "# Create pandas dataframes containing time-series data (We could reconstruct this by looping through all the daily data, since this is missing number of recovered)\n",
    "ts_us_dead_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n",
    "ts_us_confirmed_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n",
    "ts_us_dead_df = pd.read_csv(ts_us_dead_csv)            # Deaths in time series\n",
    "ts_us_confirmed_df = pd.read_csv(ts_us_confirmed_csv)  # Confirmed in time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could transpose the dataframe to allow easier extraction of time series data on a per county level\n",
    "tmp_df = ts_us_confirmed_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"confirmed\" }, inplace = True)\n",
    "confirmed_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')]  # Use pattern matching to find real dates and include\n",
    "\n",
    "tmp_df = ts_us_dead_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"dead\" }, inplace = True)\n",
    "dead_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')] # Use pattern matching to find real dates and include\n",
    "\n",
    "# Merge the confirmed ill and dead into one dataframe\n",
    "merged_clay = confirmed_clay.merge(dead_clay, left_index=True, right_index=True)\n",
    "merged_clay.plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Mobility Data\n",
    "\n",
    "This data is described at https://www.google.com/covid19/mobility/ and can be downloaded in a single monolithic CSV file at https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Mobility Data URL\n",
    "goog_mobility_csv_url = \"https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\"\n",
    "goog_mobility_df=pd.read_csv(goog_mobility_csv_url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_mobility_clay = goog_mobility_df[ (goog_mobility_df['sub_region_1'] == 'Minnesota') & (goog_mobility_df['sub_region_2'] == 'Clay County')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Mobility Data\n",
    "\n",
    "This data is described at https://www.apple.com/covid19/mobility and can be downloaded in a single monolithic CSV file at https://covid19-static.cdn-apple.com/covid19-mobility-data/2008HotfixDev42/v3/en-us/applemobilitytrends-2020-05-24.csv (That URL is hidden in the mobility page link and appears to be updated regularly.  We may need to scrape the page to identify the link).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the original Apple page was proving tricky as it had a bunch of javascript used to generate the URL, so I poked around and found a reference \n",
    "# at https://www.r-bloggers.com/get-apples-mobility-data/ to a JSON file at a stable URL that can be used to construct the appropriate URL for the current\n",
    "# datafile.\n",
    "\n",
    "aapl_mobility_json = \"https://covid19-static.cdn-apple.com/covid19-mobility-data/current/v3/index.json\"\n",
    "aapl_server = \"https://covid19-static.cdn-apple.com/\"\n",
    "result = requests.get(aapl_mobility_json)\n",
    "# Proceed if we successfully pulled the page (HTTP status code 200)\n",
    "if (result.status_code == 200):\n",
    "    # Apple Mobility Data URL\n",
    "    jsondata = result.json()\n",
    "    aapl_mobility_csv_url = aapl_server+jsondata['basePath']+jsondata['regions']['en-us']['csvPath']\n",
    "    aapl_mobility_df=pd.read_csv(aapl_mobility_csv_url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just showing how I can get clay county data specifically\n",
    "aapl_mobility_clay = aapl_mobility_df[(aapl_mobility_df['region'] == 'Clay County') & (aapl_mobility_df['sub-region'] == 'Minnesota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_mobility_clay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMHE Data on Local Resources\n",
    "\n",
    "There is IMHE data on local resources at http://www.healthdata.org/covid/data-downloads although I am not sure that data is available with county level resolution as I haven't fully investigated it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Times Data on COVID\n",
    "\n",
    "The NY Times has assembled data on COVID in a GitHub repository at https://github.com/nytimes/covid-19-data.  I have not examined that data yet, but it may well be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
