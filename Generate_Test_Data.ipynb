{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test COVID Data\n",
    "\n",
    "This Jupyter notebook uses the 'real' COVID data file format but constructs much simpler \"test\" datasets to allow QA of the routines to read in the COVID data.\n",
    "\n",
    "It uses the same library used to retrieve the original data, then pulls out subsets of the data and replaces the real data with known quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This forces a reload of any external library file if it changes.  \n",
    "# Useful when developing external libraries since otherwise Jupyter \n",
    "# will not re-import any library without restarting the python kernel.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# Import COVID data retrieval routines from external python library\n",
    "import COVIDlib.collectors as COVIDdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the start of processing\n",
    "start = time.perf_counter()\n",
    "\n",
    "## Define variables of interest below\n",
    "data_dir = 'our_data/'    # Data directory for the COVID datafiles\n",
    "test_dir = 'test_data/'   # Data directory for storing test datafiles\n",
    "\n",
    "## Define FIPS corresponding to various local areas\n",
    "ClayFIPS = 27027\n",
    "CassFIPS = 38017\n",
    "MNFIPS = 27\n",
    "NDFIPS = 38\n",
    "\n",
    "## Check if data directory exists, if not, create it\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "        \n",
    "## Create test directory if necessary\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## This generator function was added for creating testing data\n",
    "##\n",
    "def daterange(date1, date2):\n",
    "    # Initial version of Generator Code for date range grabbed from \n",
    "    # https://www.w3resource.com/python-exercises/date-time-exercise/python-date-time-exercise-50.php\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield (date1 + timedelta(n)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Test John Hopkins Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Retrieve the US Census data and then John Hopkins data\n",
    "##\n",
    "\n",
    "# Retrieve needed Census Bureau data first\n",
    "(cnty_pop_df, state_pop_df) = COVIDdata.retrieve_census_population_data()\n",
    "\n",
    "# Retrieve John Hopkins data\n",
    "(ts_us_confirmed_df, ts_us_dead_df, combined_cnty_df, combined_state_df) = COVIDdata.retrieve_John_Hopkins_data(cnty_pop_df, state_pop_df)\n",
    "\n",
    "# Do additional post-processing on John Hopkins data (adding US level data, additional variables, etc.)\n",
    "combined_state_df = COVIDdata.cleanJHdata(combined_state_df)\n",
    "combined_cnty_df = COVIDdata.cleanJHdata(combined_cnty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create test county-level dataset by limiting combined_cnty_df to Cass and Clay counties and replacing real data with\n",
    "## generated data for the two counties.\n",
    "##\n",
    "    \n",
    "test_combined_cnty_df = combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS) | (combined_cnty_df['FIPS'] == CassFIPS)].copy()\n",
    "\n",
    "# Generate a test dataset\n",
    "date1 = date(2020, 3, 22)\n",
    "date2 = date(2020, 6, 1)\n",
    "dates_str_list = list(daterange(date1, date2))\n",
    "dates = [datetime.fromisoformat(day).date() for day in dates_str_list]\n",
    "\n",
    "# Assume an unreal situation of 10 new cases a day from March 22 to June 1 \n",
    "# to allow easy check of read in numbers.\n",
    "confirmed_list = len(dates)*[10]\n",
    "confirmed_CLAY = np.array(confirmed_list)  # New infections each day\n",
    "cumilative_CLAY = np.cumsum(confirmed_CLAY)  # Cumilative number of infections\n",
    "cumilative_CLAY_list = cumilative_CLAY.tolist()\n",
    "\n",
    "# To compute 'recovered' folks assume 90% of those known to be infected survive and recover after 2 weeks\n",
    "delay = 14 # days\n",
    "survival_rate = 0.9 # Assume 10% of known infections die\n",
    "twowks_CLAY = np.array(delay*[0] + cumilative_CLAY_list[0:-delay])  # Compute number who are 2 weeks past infection\n",
    "recovered_CLAY = np.floor(survival_rate*twowks_CLAY).astype(int)\n",
    "recovered_CLAY_list = recovered_CLAY.tolist()\n",
    "deaths_CLAY =  twowks_CLAY - recovered_CLAY\n",
    "deaths_CLAY_list = deaths_CLAY.tolist()\n",
    "\n",
    "# Assume Cass County has twice the cases\n",
    "confirmed_CASS = 2*confirmed_CLAY\n",
    "cumilative_CASS = np.cumsum(confirmed_CASS)  # Cumilative number of infections\n",
    "cumilative_CASS_list = cumilative_CASS.tolist()\n",
    "twowks_CASS = np.array(delay*[0] + cumilative_CASS_list[0:-delay])  # Compute number who are 2 weeks past infection\n",
    "recovered_CASS = np.floor(survival_rate*twowks_CASS).astype(int)\n",
    "recovered_CASS_list = recovered_CASS.tolist()\n",
    "deaths_CASS =  twowks_CASS - recovered_CASS\n",
    "deaths_CASS_list = deaths_CASS.tolist()\n",
    "\n",
    "# Save Dates, Confirmed, Deaths, Recovered numbers\n",
    "test_combined_cnty_df['dates'] = 2*[dates]\n",
    "test_combined_cnty_df['Confirmed'] = [cumilative_CLAY_list, cumilative_CASS_list ]\n",
    "test_combined_cnty_df['Deaths'] = [deaths_CLAY_list, deaths_CASS_list ]\n",
    "test_combined_cnty_df['Recovered'] = [recovered_CLAY_list, recovered_CASS_list ]\n",
    "\n",
    "##\n",
    "## Compute derivatives and second derivatives\n",
    "##\n",
    "\n",
    "# Convert the list of dates into numpy array of days since Jan. 1, 2020 for each observation\n",
    "dates = test_combined_cnty_df[test_combined_cnty_df['FIPS'] == ClayFIPS]['dates'].tolist()[0]\n",
    "dates_list = []\n",
    "for dat in dates:\n",
    "    dates_list.append( COVIDdata.days_since(dat) )\n",
    "dates_arr = np.array([dates_list]*len(test_combined_cnty_df))\n",
    "\n",
    "# Convert confirmed/deaths/recovered into arrays\n",
    "confirmed_arr = np.array(test_combined_cnty_df['Confirmed'].values.tolist())\n",
    "deaths_arr = np.array(test_combined_cnty_df['Deaths'].values.tolist())\n",
    "\n",
    "# Compute the derivatives (using forward derivative approach)\n",
    "dconfirmed_arr = COVIDdata.derivative(dates_arr, confirmed_arr)\n",
    "ddeaths_arr = COVIDdata.derivative(dates_arr, deaths_arr)\n",
    "dconfirmed7_arr = COVIDdata.derivative_ndays(dates_arr, confirmed_arr, 7)\n",
    "ddeaths7_arr = COVIDdata.derivative_ndays(dates_arr, deaths_arr, 7)\n",
    "    \n",
    "# Compute the second derivatives (a bit hinky to use forward derivative again, but...)\n",
    "d2confirmed_arr = COVIDdata.derivative(dates_arr, dconfirmed_arr)\n",
    "d2deaths_arr = COVIDdata.derivative(dates_arr, ddeaths_arr)\n",
    "d2confirmed7_arr = COVIDdata.derivative_ndays(dates_arr, dconfirmed7_arr, 7)\n",
    "d2deaths7_arr = COVIDdata.derivative_ndays(dates_arr, ddeaths7_arr, 7)\n",
    "    \n",
    "# Convert numpy arrays to lists of lists for storage in combined dataframe\n",
    "test_combined_cnty_df['dConfirmed'] = dconfirmed_arr.tolist()\n",
    "test_combined_cnty_df['d2Confirmed'] = d2confirmed_arr.tolist()\n",
    "test_combined_cnty_df['dDeaths'] = ddeaths_arr.tolist()\n",
    "test_combined_cnty_df['d2Deaths'] = d2deaths_arr.tolist()\n",
    "test_combined_cnty_df['dConfirmedWk'] = dconfirmed7_arr.tolist()\n",
    "test_combined_cnty_df['d2ConfirmedWk'] = d2confirmed7_arr.tolist()\n",
    "test_combined_cnty_df['dDeathsWk'] = ddeaths7_arr.tolist()\n",
    "test_combined_cnty_df['d2DeathsWk'] = d2deaths7_arr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Create test state-level dataset by limiting combined_state_df to Minnesota and North Dakora and replacing real data with\n",
    "## generated data for the two states.\n",
    "##\n",
    "    \n",
    "test_combined_states_df = combined_state_df[(combined_state_df['FIPS'] == MNFIPS) | (combined_state_df['FIPS'] == NDFIPS)].copy()\n",
    "\n",
    "# Generate a test dataset\n",
    "date1 = date(2020, 3, 22)\n",
    "date2 = date(2020, 6, 1)\n",
    "dates_str_list = list(daterange(date1, date2))\n",
    "dates = [datetime.fromisoformat(day).date() for day in dates_str_list]\n",
    "\n",
    "# Assume an unreal situation of 10 additional new cases a day from March 22 to June 1 \n",
    "# to allow easy check of read in numbers.\n",
    "rise = (10*np.arange(len(dates))).tolist()\n",
    "confirmed_list = rise + (len(dates) - len(rise))*[rise[-1]]\n",
    "confirmed_MN = np.array(confirmed_list)  # New infections each day\n",
    "cumilative_MN = np.cumsum(confirmed_MN)  # Cumilative number of infections\n",
    "cumilative_MN_list = cumilative_MN.tolist()\n",
    "\n",
    "# To compute 'recovered' folks assume 90% of those known to be infected survive and recover after 2 weeks\n",
    "delay = 14 # days\n",
    "survival_rate = 0.9 # Assume 10% of known infections die\n",
    "twowks_MN= np.array(delay*[0] + cumilative_MN_list[0:-delay])  # Compute number who are 2 weeks past infection\n",
    "recovered_MN = np.floor(survival_rate*twowks_MN).astype(int)\n",
    "recovered_MN_list = recovered_MN.tolist()\n",
    "deaths_MN =  twowks_MN - recovered_MN\n",
    "deaths_MN_list = deaths_MN.tolist()\n",
    "\n",
    "# Assume North Dakota has half the cases\n",
    "confirmed_ND = 0.5*confirmed_MN\n",
    "cumilative_ND = np.cumsum(confirmed_ND)  # Cumilative number of infections\n",
    "cumilative_ND_list = cumilative_ND.tolist()\n",
    "twowks_ND = np.array(delay*[0] + cumilative_ND_list[0:-delay])  # Compute number who are 2 weeks past infection\n",
    "recovered_ND = np.floor(survival_rate*twowks_ND).astype(int)\n",
    "recovered_ND_list = recovered_ND.tolist()\n",
    "deaths_ND =  twowks_ND - recovered_ND\n",
    "deaths_ND_list = deaths_ND.tolist()\n",
    "\n",
    "# Save Dates, Confirmed, Deaths, Recovered numbers\n",
    "test_combined_states_df['dates'] = 2*[dates]\n",
    "test_combined_states_df['Confirmed'] = [cumilative_MN_list, cumilative_ND_list ]\n",
    "test_combined_states_df['Deaths'] = [deaths_MN_list, deaths_ND_list ]\n",
    "test_combined_states_df['Recovered'] = [recovered_MN_list, recovered_ND_list ]\n",
    "\n",
    "##\n",
    "## Compute derivatives and second derivatives\n",
    "##\n",
    "\n",
    "# Convert the list of dates into numpy array of days since Jan. 1, 2020 for each observation\n",
    "dates = test_combined_states_df[test_combined_states_df['FIPS'] == MNFIPS]['dates'].tolist()[0]\n",
    "dates_list = []\n",
    "for dat in dates:\n",
    "    dates_list.append( COVIDdata.days_since(dat) )\n",
    "dates_arr = np.array([dates_list]*len(test_combined_states_df))\n",
    "\n",
    "# Generate incident rates and death rates per 100000\n",
    "pop_arr = np.array(test_combined_states_df['PopEst2019'].values.tolist())\n",
    "test_combined_states_df['Incident_Rate'] = np.around(100000*(confirmed_arr/pop_arr[:, np.newaxis]), decimals=3).tolist()\n",
    "test_combined_states_df['Mortality_Rate'] = np.around(100000*(deaths_arr/pop_arr[:, np.newaxis]), decimals=3).tolist()\n",
    "\n",
    "# Generate people hospitalized amd hospitalization rate\n",
    "# Assume 10% of those currently sick are in the hospital\n",
    "hospitalized_MN = np.floor(0.1*(cumilative_MN - recovered_MN)).astype(int).tolist()\n",
    "hospitalized_ND = np.floor(0.1*(cumilative_ND - recovered_ND)).astype(int).tolist()  \n",
    "                           \n",
    "# Generate people tested\n",
    "test_combined_states_df['People_Hospitalized'] = [ hospitalized_MN, hospitalized_ND]\n",
    "hospitalized_arr = np.array(test_combined_states_df['People_Hospitalized'].values.tolist())\n",
    "test_combined_states_df['Hospitalization_Rate'] = np.around(100000*(hospitalized_arr/pop_arr[:, np.newaxis]), decimals=3).tolist()\n",
    "\n",
    "# Generate testing rate\n",
    "# Assume completely bogus flat rates of 5000 people a day tested\n",
    "tested_list = len(dates)*[5000]\n",
    "tested_arr = np.array(tested_list)  # New tests each day\n",
    "cumilative_tested_arr = np.cumsum(tested_arr)  # Cumilative number of tests\n",
    "cumilative_tested_list = cumilative_tested_arr.tolist()\n",
    "test_combined_states_df['People_Tested'] = 2*[cumilative_tested_list]\n",
    "test_combined_states_df['Testing_Rate'] = np.around(100000*(cumilative_tested_arr/pop_arr[:, np.newaxis]), decimals=3).tolist()\n",
    "\n",
    "# Convert confirmed/deaths/recovered into arrays\n",
    "confirmed_arr = np.array(test_combined_states_df['Confirmed'].values.tolist())\n",
    "deaths_arr = np.array(test_combined_states_df['Deaths'].values.tolist())\n",
    "tested_arr = np.array(test_combined_states_df['People_Tested'].values.tolist())\n",
    "\n",
    "# Compute the derivatives (using forward derivative approach)\n",
    "dconfirmed_arr = COVIDdata.derivative(dates_arr, confirmed_arr)\n",
    "ddeaths_arr = COVIDdata.derivative(dates_arr, deaths_arr)\n",
    "dtested_arr = COVIDdata.derivative(dates_arr, tested_arr)\n",
    "dconfirmed7_arr = COVIDdata.derivative_ndays(dates_arr, confirmed_arr, 7)\n",
    "ddeaths7_arr = COVIDdata.derivative_ndays(dates_arr, deaths_arr, 7)\n",
    "dtested7_arr = COVIDdata.derivative_ndays(dates_arr, tested_arr, 7)\n",
    "    \n",
    "# Compute the second derivatives (a bit hinky to use forward derivative again, but...)\n",
    "d2confirmed_arr = COVIDdata.derivative(dates_arr, dconfirmed_arr)\n",
    "d2deaths_arr = COVIDdata.derivative(dates_arr, ddeaths_arr)\n",
    "d2confirmed7_arr = COVIDdata.derivative_ndays(dates_arr, dconfirmed7_arr, 7)\n",
    "d2deaths7_arr = COVIDdata.derivative_ndays(dates_arr, ddeaths7_arr, 7)\n",
    "    \n",
    "# Convert numpy arrays to lists of lists for storage in combined dataframe\n",
    "test_combined_states_df['dConfirmed'] = dconfirmed_arr.tolist()\n",
    "test_combined_states_df['d2Confirmed'] = d2confirmed_arr.tolist()\n",
    "test_combined_states_df['dDeaths'] = ddeaths_arr.tolist()\n",
    "test_combined_states_df['d2Deaths'] = d2deaths_arr.tolist()\n",
    "test_combined_states_df['dTested'] = dtested_arr.tolist()\n",
    "test_combined_states_df['dConfirmedWk'] = dconfirmed7_arr.tolist()\n",
    "test_combined_states_df['d2ConfirmedWk'] = d2confirmed7_arr.tolist()\n",
    "test_combined_states_df['dDeathsWk'] = ddeaths7_arr.tolist()\n",
    "test_combined_states_df['d2DeathsWk'] = d2deaths7_arr.tolist()\n",
    "test_combined_states_df['dTestedWk'] = dtested7_arr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices before exporting\n",
    "test_combined_cnty_df.reset_index(drop=True, inplace=True)\n",
    "test_combined_states_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# Save the same data to pickle files\n",
    "#\n",
    "combined_datafile = test_dir + \"TEST_countylevel_combinedCDR.p\"\n",
    "print(\" - John Hopkins county level test data exported to \", combined_datafile)\n",
    "with open(combined_datafile, 'wb') as pickle_file:\n",
    "    pickle.dump(test_combined_cnty_df, pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "combined_datafile = test_dir + \"TEST_statelevel_combinedCDR.p\"\n",
    "print(\" - John Hopkins state level test data exported to \", combined_datafile)\n",
    "with open(combined_datafile, 'wb') as pickle_file:\n",
    "    pickle.dump(test_combined_states_df, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_cnty_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime lists into strings\n",
    "test_combined_cnty_df['dates'] = test_combined_cnty_df['dates'].apply(COVIDdata.dates2strings)\n",
    "test_combined_states_df['dates'] = test_combined_states_df['dates'].apply(COVIDdata.dates2strings)\n",
    "\n",
    "#\n",
    "# Save the county and state-level test dataframes into CSV files\n",
    "#\n",
    "combined_datafile = test_dir + \"TEST_countylevel_combinedCDR.csv\"\n",
    "print(\" - John Hopkins county level test data also exported to \", combined_datafile)\n",
    "test_combined_cnty_df.to_csv(combined_datafile, index=False)\n",
    "\n",
    "combined_datafile = test_dir + \"TEST_statelevel_combinedCDR.csv\"\n",
    "print(\" - John Hopkins state level test data also exported to \", combined_datafile)\n",
    "test_combined_states_df.to_csv(combined_datafile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show demonstrations of plotting this data here by producing plots of data for Cass and Clay counties and North Dakota and Minnesota\n",
    "combined_cnty_df = test_combined_cnty_df.copy()\n",
    "combined_state_df = test_combined_states_df.copy()\n",
    "\n",
    "#\n",
    "# I will pull the data to plot into numpy arrays (notice I have to use [0] because it comes out at list of lists even for single row)\n",
    "#\n",
    "\n",
    "# County-level data for plotting\n",
    "dates_cty = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['dates'].to_list()[0], dtype='datetime64')\n",
    "clay_deaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "clay_death_rate = (clay_deaths/combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['PopEst2019'].values)*100000\n",
    "cass_deaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "cass_death_rate = (cass_deaths/combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['PopEst2019'].values)*100000\n",
    "clay_confirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "clay_confirmed_rate = (clay_confirmed/combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['PopEst2019'].values)*100000\n",
    "cass_confirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "cass_confirmed_rate = (cass_confirmed/combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['PopEst2019'].values)*100000\n",
    "\n",
    "# Set up a figure of 2 x 2 plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15,10))\n",
    "\n",
    "# Plot up deaths and death rates as plots 0 and 1\n",
    "this_axs = axs[0, 0]  # Row 0, column 0\n",
    "this_axs.plot(dates_cty, clay_deaths, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_deaths, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Deaths\")\n",
    "\n",
    "this_axs = axs[1, 0]  # Row 1, column 0\n",
    "this_axs.plot(dates_cty, clay_death_rate, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_death_rate, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Deaths per 100,000 people\")\n",
    "\n",
    "# Plot up confirmed infections and infection rates as plots 2 and 3\n",
    "this_axs = axs[0, 1]  # Row 0, column 1\n",
    "this_axs.plot(dates_cty, clay_confirmed, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_confirmed, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections\")\n",
    "\n",
    "this_axs = axs[1, 1]  # Row 1, column 1\n",
    "this_axs.plot(dates_cty, clay_confirmed_rate, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_confirmed_rate, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections per 100,000 people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# I will pull the data to plot into numpy arrays (notice I have to use [0] because it comes out at list of lists even for single row)\n",
    "#\n",
    "\n",
    "# State-level data for plotting\n",
    "dates_state = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['dates'].to_list()[0], dtype='datetime64')\n",
    "MN_deaths = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "MN_death_rate = (MN_deaths/combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['PopEst2019'].values)*100000\n",
    "ND_deaths = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "ND_death_rate = (ND_deaths/combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['PopEst2019'].values)*100000\n",
    "MN_confirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "MN_confirmed_rate = (MN_confirmed/combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['PopEst2019'].values)*100000\n",
    "ND_confirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "ND_confirmed_rate = (ND_confirmed/combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['PopEst2019'].values)*100000\n",
    "\n",
    "# Set up a figure of 2 x 2 plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15,10))\n",
    "\n",
    "# Plot up deaths and death rates as plots 0 and 1\n",
    "this_axs = axs[0, 0]  # Row 0, column 0\n",
    "this_axs.plot(dates_state, MN_deaths, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_deaths, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Deaths\")\n",
    "\n",
    "this_axs = axs[1, 0]  # Row 1, column 0\n",
    "this_axs.plot(dates_state, MN_death_rate, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_death_rate, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Deaths per 100,000 people\")\n",
    "\n",
    "# Plot up confirmed infections and infection rates as plots 2 and 3\n",
    "this_axs = axs[0, 1]  # Row 0, column 1\n",
    "this_axs.plot(dates_state, MN_confirmed, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_confirmed, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections\")\n",
    "\n",
    "this_axs = axs[1, 1]  # Row 1, column 1\n",
    "this_axs.plot(dates_state, MN_confirmed_rate, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_confirmed_rate, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections per 100,000 people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show demonstrations of plotting this data here by producing plots of data for Cass and Clay counties and North Dakota and Minnesota\n",
    "\n",
    "#\n",
    "# I will pull the data to plot into numpy arrays (notice I have to use [0] because it comes out at list of lists even for single row)\n",
    "#\n",
    "\n",
    "# County-level data for plotting\n",
    "dates_cty = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['dates'].to_list()[0], dtype='datetime64')\n",
    "clay_ddeaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['dDeaths'].to_list()[0])\n",
    "cass_ddeaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['dDeaths'].to_list()[0])\n",
    "clay_dconfirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['dConfirmed'].to_list()[0])\n",
    "cass_dconfirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['dConfirmed'].to_list()[0])\n",
    "\n",
    "# State-level data for plotting\n",
    "dates_state = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['dates'].to_list()[0], dtype='datetime64')\n",
    "MN_ddeaths = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['dDeaths'].to_list()[0])\n",
    "ND_ddeaths = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['dDeaths'].to_list()[0])\n",
    "MN_dconfirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['dConfirmed'].to_list()[0])\n",
    "ND_dconfirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['dConfirmed'].to_list()[0])\n",
    "\n",
    "# Set up a figure of 2 x 2 plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot up the deriviates in the infection and death rates for counties\n",
    "this_axs = axs[0, 0]  # row 0, column 0\n",
    "this_axs.plot(dates_cty, clay_dconfirmed, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_dconfirmed, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Infections/Day\")\n",
    "title = this_axs.set_title(\"COVID Infection Change Rate\")\n",
    "\n",
    "this_axs = axs[0, 1]  # row 0, column 1\n",
    "this_axs.plot(dates_cty, clay_ddeaths, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_ddeaths, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Deaths/Day\")\n",
    "title = this_axs.set_title(\"COVID Death Change Rate\")\n",
    "\n",
    "# Plot up the deriviates in the infection and death rates for states\n",
    "this_axs = axs[1, 0]  # row 1, column 0\n",
    "this_axs.plot(dates_state, MN_dconfirmed, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_dconfirmed, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Infections/Day\")\n",
    "title = this_axs.set_title(\"COVID Infection Change Rate\")\n",
    "\n",
    "this_axs = axs[1, 1]  # row 1, column 1\n",
    "this_axs.plot(dates_state, MN_ddeaths, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_ddeaths, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Deaths/Day\")\n",
    "title = this_axs.set_title(\"COVID Death Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Google Mobility Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Google Mobility dataframes\n",
    "(goog_mobility_cnty_df, goog_mobility_states_df) = COVIDdata.retrieve_goog_mobility_data(cnty_pop_df, state_pop_df)\n",
    "\n",
    "# Post-process Google Mobility Data to add postal codes\n",
    "COVIDdata.cleanGOOGdata(goog_mobility_cnty_df)\n",
    "COVIDdata.cleanGOOGdata(goog_mobility_states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create test subsets containing only local data\n",
    "test_goog_mobility_cnty_df= goog_mobility_cnty_df[(goog_mobility_cnty_df['FIPS'] == ClayFIPS) | (goog_mobility_cnty_df['FIPS'] == CassFIPS)].copy()\n",
    "test_goog_mobility_states_df= goog_mobility_states_df[(goog_mobility_states_df['FIPS'] == MNFIPS) | (goog_mobility_states_df['FIPS'] == NDFIPS)].copy()\n",
    "\n",
    "#\n",
    "# Create test county data\n",
    "#\n",
    "\n",
    "npts = 100  # Output only the first hundred days\n",
    "\n",
    "# Generate sine waves with amplitude 20 and different wavelengths in days\n",
    "dates_CLAY = test_goog_mobility_cnty_df['dates'][(goog_mobility_cnty_df['FIPS'] == ClayFIPS)].tolist()[0]\n",
    "dates_CASS = test_goog_mobility_cnty_df['dates'][(goog_mobility_cnty_df['FIPS'] == CassFIPS)].tolist()[0]\n",
    "test_goog_mobility_cnty_df['dates'] = [dates_CLAY[0:npts], dates_CASS[0:npts]]\n",
    "\n",
    "ptsCLAY = np.arange(npts)\n",
    "ptsCASS = np.arange(npts)\n",
    "\n",
    "wl = 30 # Wavelength of 30 days\n",
    "amp = 20\n",
    "signalCLAY = np.around(amp*np.sin(2 * np.pi * ptsCLAY/wl), decimals=3)\n",
    "signalCASS = np.around((amp/2)*np.sin(2 * np.pi * ptsCASS/(wl*2)), decimals=3)\n",
    "signalCLAY_list = signalCLAY.tolist()\n",
    "signalCASS_list = signalCASS.tolist()\n",
    "test_goog_mobility_cnty_df['retail_and_recreation_percent_change_from_baseline'] = [signalCLAY_list[0:npts], signalCASS_list[0:npts]]\n",
    "test_goog_mobility_cnty_df['grocery_and_pharmacy_percent_change_from_baseline'] = [signalCLAY_list[0:npts], signalCASS_list[0:npts]]\n",
    "signalCLAY /= 2\n",
    "signalCASS /= 2\n",
    "signalCLAY_list = signalCLAY.tolist()\n",
    "null_list = len(signalCLAY_list)*[np.nan]\n",
    "signalCASS_list = signalCASS.tolist()\n",
    "test_goog_mobility_cnty_df['parks_percent_change_from_baseline'] = [signalCLAY_list[0:npts], signalCASS_list[0:npts]]\n",
    "test_goog_mobility_cnty_df['transit_stations_percent_change_from_baseline'] = [null_list[0:npts], signalCASS_list[0:npts]]\n",
    "signalCLAY /= 2\n",
    "signalCASS /= 2\n",
    "signalCLAY_list = signalCLAY.tolist()\n",
    "signalCASS_list = signalCASS.tolist()\n",
    "test_goog_mobility_cnty_df['workplaces_percent_change_from_baseline'] = [signalCLAY_list[0:npts], signalCASS_list[0:npts]]\n",
    "test_goog_mobility_cnty_df['residential_percent_change_from_baseline'] = [signalCLAY_list[0:npts], signalCASS_list[0:npts]]\n",
    "\n",
    "#\n",
    "# Create test state data\n",
    "#\n",
    "\n",
    "# Generate cosine waves with amplitude 20 and different wavelengths in days\n",
    "dates_MN = test_goog_mobility_states_df['dates'][(test_goog_mobility_states_df['FIPS'] == MNFIPS)].tolist()[0]\n",
    "dates_ND = test_goog_mobility_states_df['dates'][(test_goog_mobility_states_df['FIPS'] == NDFIPS)].tolist()[0]\n",
    "test_goog_mobility_states_df['dates'] = [dates_MN[0:npts], dates_ND[0:npts]]\n",
    "\n",
    "pts_MN = np.arange(npts)\n",
    "pts_ND = np.arange(npts)\n",
    "\n",
    "wl = 30 # Wavelength of 30 days\n",
    "amp = 20\n",
    "signalMN = np.around(amp*np.cos(2 * np.pi * pts_MN/wl), decimals=3)\n",
    "signalND = np.around((amp/2)*np.cos(2 * np.pi * pts_ND/(wl*2)), decimals=3)\n",
    "signalMN_list = signalMN.tolist()\n",
    "signalND_list = signalND.tolist()\n",
    "test_goog_mobility_states_df['retail_and_recreation_percent_change_from_baseline'] = [signalMN_list[0:npts], signalND_list[0:npts]]\n",
    "test_goog_mobility_states_df['grocery_and_pharmacy_percent_change_from_baseline'] = [signalMN_list[0:npts], signalND_list[0:npts]]\n",
    "signalMN /= 2\n",
    "signalND /= 2\n",
    "signalMN_list = signalMN.tolist()\n",
    "null_list = len(signalMN_list)*[np.nan]\n",
    "signalND_list = signalND.tolist()\n",
    "test_goog_mobility_states_df['parks_percent_change_from_baseline'] = [signalMN_list[0:npts], signalND_list[0:npts]]\n",
    "test_goog_mobility_states_df['transit_stations_percent_change_from_baseline'] = [null_list[0:npts], signalND_list[0:npts]]\n",
    "signalMN /= 2\n",
    "signalND /= 2\n",
    "signalMN_list = signalMN.tolist()\n",
    "signalND_list = signalND.tolist()\n",
    "test_goog_mobility_states_df['workplaces_percent_change_from_baseline'] = [signalMN_list[0:npts], signalND_list[0:npts]]\n",
    "test_goog_mobility_states_df['residential_percent_change_from_baseline'] = [signalMN_list[0:npts], signalND_list[0:npts]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices before exporting\n",
    "test_goog_mobility_cnty_df.reset_index(drop=True, inplace=True)\n",
    "test_goog_mobility_states_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#\n",
    "# Save the same data to pickle files\n",
    "#\n",
    "goog_mobility_cnty_fname = test_dir + \"TEST_goog_mobility_cnty.p\"\n",
    "print(\" - Test Google county level mobility data exported to \", goog_mobility_cnty_fname)\n",
    "with open(goog_mobility_cnty_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_goog_mobility_cnty_df, pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "goog_mobility_states_fname = test_dir + \"TEST_goog_mobility_state.p\"\n",
    "print(\" - Test Google state level mobility data exported to \", goog_mobility_states_fname)\n",
    "with open(goog_mobility_states_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_goog_mobility_states_df, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime lists into strings\n",
    "test_goog_mobility_cnty_df['dates'] = test_goog_mobility_cnty_df['dates'].apply(COVIDdata.dates2strings)\n",
    "test_goog_mobility_states_df['dates'] = test_goog_mobility_states_df['dates'].apply(COVIDdata.dates2strings)\n",
    "\n",
    "# Export the google mobility data to CSV files\n",
    "goog_mobility_cnty_fname = test_dir + \"TEST_goog_mobility_cnty.csv\"\n",
    "print(\" - Google county level mobility data also exported to \", goog_mobility_cnty_fname)\n",
    "test_goog_mobility_cnty_df.to_csv(goog_mobility_cnty_fname, index=False)\n",
    "\n",
    "goog_mobility_states_fname = test_dir + \"TEST_goog_mobility_state.csv\"\n",
    "print(\" - Google state level mobility data also exported to \", goog_mobility_states_fname)\n",
    "test_goog_mobility_states_df.to_csv(goog_mobility_states_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Apple Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Retrieve the Apple Mobility dataframes\n",
    "(aapl_mobility_cnty_df, aapl_mobility_states_df) = COVIDdata.retrieve_aapl_mobility_data(cnty_pop_df, state_pop_df)\n",
    "\n",
    "# Post-process Apple Mobility dataframes to add postal codes\n",
    "COVIDdata.cleanAAPLdata(aapl_mobility_cnty_df)\n",
    "COVIDdata.cleanAAPLdata(aapl_mobility_states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test subsets containing only local data\n",
    "test_aapl_mobility_cnty_df= aapl_mobility_cnty_df[(aapl_mobility_cnty_df['FIPS'] == ClayFIPS) | (aapl_mobility_cnty_df['FIPS'] == CassFIPS)].copy()\n",
    "test_aapl_mobility_states_df= aapl_mobility_states_df[(aapl_mobility_states_df['FIPS'] == MNFIPS) | (aapl_mobility_states_df['FIPS'] == NDFIPS)].copy()\n",
    "\n",
    "#\n",
    "# Create test county data that is a sawtooth 1 week in duration\n",
    "#\n",
    "\n",
    "npts = 100  # Output only the first hundred days\n",
    "\n",
    "dates_CLAY = test_aapl_mobility_cnty_df['dates'][(test_aapl_mobility_cnty_df['FIPS'] == ClayFIPS)].tolist()[0]\n",
    "dates_CASS = test_aapl_mobility_cnty_df['dates'][(test_aapl_mobility_cnty_df['FIPS'] == CassFIPS)].tolist()[0]\n",
    "test_aapl_mobility_cnty_df['dates'] = [dates_CLAY[0:npts], dates_CASS[0:npts]]\n",
    "\n",
    "sawtooth = 30*(-3*np.ones(7)+np.arange(7))\n",
    "sawtooth_inv = -1*sawtooth\n",
    "signalCLAY = sawtooth.tolist()[0:npts]\n",
    "signalCASS = sawtooth_inv.tolist()[0:npts]\n",
    "test_aapl_mobility_cnty_df['driving_mobility'] = [signalCLAY, signalCASS]\n",
    "\n",
    "#\n",
    "# Create test state data that is a boxcar 8 days in duration\n",
    "#\n",
    "\n",
    "dates_MN = test_aapl_mobility_states_df['dates'][(test_aapl_mobility_states_df['FIPS'] == MNFIPS)].tolist()[0]\n",
    "dates_ND = test_aapl_mobility_states_df['dates'][(test_aapl_mobility_states_df['FIPS'] == NDFIPS)].tolist()[0]\n",
    "test_aapl_mobility_states_df['dates'] = [dates_MN[0:npts], dates_ND[0:npts]]\n",
    "\n",
    "box = [1,1,1,1,-1,-1,-1,-1]\n",
    "boxcar = 30*np.array(box)\n",
    "boxcar_inv = -0.5*boxcar\n",
    "signalMN = boxcar.tolist()[0:npts]\n",
    "signalND = boxcar_inv.tolist()[0:npts]\n",
    "test_aapl_mobility_states_df['driving_mobility'] = [signalMN, signalND]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the test Apple mobility data to CSV files\n",
    "print(\"\\nExporting Test Apple mobility data\")\n",
    "\n",
    "# Reset indices before exporting\n",
    "test_aapl_mobility_cnty_df.reset_index(drop=True, inplace=True)\n",
    "test_aapl_mobility_states_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#\n",
    "# Save the same data to pickle files\n",
    "#\n",
    "aapl_mobility_cnty_fname = test_dir + \"TEST_aapl_mobility_cnty.p\"\n",
    "print(\" - Apple county level mobility test data also exported to \", aapl_mobility_cnty_fname)\n",
    "with open(aapl_mobility_cnty_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_aapl_mobility_cnty_df, pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "aapl_mobility_states_fname = test_dir + \"TEST_aapl_mobility_state.p\"\n",
    "print(\" - Apple state level mobility test data also exported to \", aapl_mobility_states_fname)\n",
    "with open(aapl_mobility_states_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_aapl_mobility_states_df, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime lists into strings\n",
    "test_aapl_mobility_cnty_df['dates'] = test_aapl_mobility_cnty_df['dates'].apply(COVIDdata.dates2strings)\n",
    "test_aapl_mobility_states_df['dates'] = test_aapl_mobility_states_df['dates'].apply(COVIDdata.dates2strings)\n",
    "    \n",
    "aapl_mobility_cnty_fname = test_dir + \"TEST_aapl_mobility_cnty.csv\"\n",
    "print(\" - Apple county level mobility test data exported to \", aapl_mobility_cnty_fname)\n",
    "test_aapl_mobility_cnty_df.to_csv(aapl_mobility_cnty_fname, index=False)\n",
    "\n",
    "aapl_mobility_states_fname = test_dir + \"TEST_aapl_mobility_state.csv\"\n",
    "print(\" - Apple state level mobility test data exported to \", aapl_mobility_states_fname)\n",
    "test_aapl_mobility_states_df.to_csv(aapl_mobility_states_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating IMHE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve IMHE data\n",
    "(imhe_summary, imhe_hospitalizations) = COVIDdata.retrieve_imhe_data(cnty_pop_df, state_pop_df)\n",
    "\n",
    "## Summary data includes numbers or dates for the following for each state\n",
    "#             'peak_bed_day_mean', 'peak_bed_day_lower', 'peak_bed_day_upper': Mean/Lower/Upper Uncertainty peak bed use date\n",
    "# 'peak_icu_bed_day_mean', 'peak_icu_bed_day_lower', 'peak_icu_bed_day_upper': Mean/Lower/Upper Uncertainty ICU bed use date\n",
    "#          'peak_vent_day_mean', 'peak_vent_day_lower', 'peak_vent_day_upper': Mean/Lower/Upper Uncertainty Ventilator use date\n",
    "#    'all_bed_capacity', 'icu_bed_capacity', 'all_bed_usage', 'icu_bed_usage': Number of beds/ICU beds/avg beds used/avg ICU beds used\n",
    "#                          'travel_limit_start_date', 'travel_limit_end_date': Severe travel restrictions start/end dates\n",
    "#                                'stay_home_start_date', 'stay_home_end_date': Stay at home order start/end dates\n",
    "#                    'educational_fac_start_date', 'educational_fac_end_date': Educational facilities closure start/end dates\n",
    "#      'any_gathering_restrict_start_date', 'any_gathering_restrict_end_date': Any gathering restrictions start/end dates\n",
    "#                          'any_business_start_date', 'any_business_end_date': Any business closures start/end dates\n",
    "#          'all_non-ess_business_start_date', 'all_non-ess_business_end_date': Non-essential businesses ordered to close start/end dates\n",
    "#\n",
    "# 'NaN' present for dates means it isn't known.\n",
    "\n",
    "## Hospitalization data is time series date for the following projections by the IMHE:\n",
    "#                             'allbed_mean', 'allbed_lower','allbed_upper': Predicted COVID beds needed with upper/lower bounds\n",
    "#                            'ICUbed_mean', 'ICUbed_lower', 'ICUbed_upper': Predicted COVID ICU beds needed with upper/lower bounds\n",
    "#                            'InvVen_mean', 'InvVen_lower', 'InvVen_upper': Predicted COVID ventilators needed with upper/lower bounds\n",
    "#                            'deaths_mean', 'deaths_lower', 'deaths_upper': Predicted COVID daily deaths with upper/lower bounds\n",
    "#                               'admis_mean', 'admis_lower', 'admis_upper': Predicted hospital admissions with upper/lower bounds\n",
    "#                            'newICU_mean', 'newICU_lower', 'newICU_upper': Predicted new ICU admissions per day with upper/lower bounds\n",
    "#                            'totdea_mean', 'totdea_lower', 'totdea_upper': Predicted COVID cumilative deaths with upper/lower bounds\n",
    "# 'deaths_mean_smoothed', 'deaths_lower_smoothed', 'deaths_upper_smoothed': Smoothed version of predicted COVID daily deaths\n",
    "# 'totdea_mean_smoothed', 'totdea_lower_smoothed', 'totdea_upper_smoothed': Smoothed version of cumilative COVID deaths\n",
    "#                                   'total_tests_data_type', 'total_tests': observed/predicted tests and total number of tests\n",
    "#                                                   'confirmed_infections': Observed confirmed infections only\n",
    "#    'est_infections_mean', 'est_infections_lower', 'est_infections_upper': Predicted estimated infections with upper/lower bounds\n",
    "#\n",
    "# 'NaN' present for dates means it isn't known.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Retrieve IMHE data\n",
    "(imhe_summary, imhe_hospitalizations) = COVIDdata.retrieve_imhe_data(cnty_pop_df, state_pop_df)\n",
    "\n",
    "# Create test subsets containing only local data\n",
    "test_imhe_summary = imhe_summary[(imhe_summary['FIPS'] == MNFIPS) | (imhe_summary['FIPS'] == NDFIPS)].copy()\n",
    "test_imhe_hospitalizations = imhe_hospitalizations[(imhe_hospitalizations['FIPS'] == MNFIPS) | (imhe_hospitalizations['FIPS'] == NDFIPS)].copy()\n",
    "\n",
    "# Set fixed values for all the test summary data\n",
    "test_imhe_summary['peak_bed_day_mean'] = [ '2020-05-15', '2020-06-15']\n",
    "test_imhe_summary['peak_bed_day_lower'] = [ '2020-05-01', '2020-06-01']\n",
    "test_imhe_summary['peak_bed_day_upper'] = [ '2020-06-15', '2020-07-15']\n",
    "test_imhe_summary['peak_icu_bed_day_mean'] = [ '2020-05-15', '2020-06-15']\n",
    "test_imhe_summary['peak_icu_bed_day_lower'] = [ '2020-05-01', '2020-06-01']\n",
    "test_imhe_summary['peak_icu_bed_day_upper'] = [ '2020-06-15', '2020-07-15']\n",
    "test_imhe_summary['peak_vent_day_mean'] = [ '2020-05-15', '2020-06-15']\n",
    "test_imhe_summary['peak_vent_day_lower'] = [ '2020-05-01', '2020-06-01']\n",
    "test_imhe_summary['peak_vent_day_upper'] = [ '2020-06-15', '2020-07-15']\n",
    "test_imhe_summary['all_bed_capacity'] = [ 2000, 1000 ]\n",
    "test_imhe_summary['icu_bed_capacity'] = [ 200, 100 ]\n",
    "test_imhe_summary['all_bed_usage'] = [ 2000, 1000 ]\n",
    "test_imhe_summary['icu_bed_usage'] = [ 200, 100 ]\n",
    "test_imhe_summary['travel_limit_start_date'] = [ '2020-03-01',  np.nan ]\n",
    "test_imhe_summary['travel_limit_end_date'] = [ '2020-06-01', np.nan ]\n",
    "test_imhe_summary['stay_home_start_date'] = [ '2020-03-01',  np.nan]\n",
    "test_imhe_summary['stay_home_end_date'] = [ '2020-06-01',  np.nan ]\n",
    "test_imhe_summary['educational_fac_start_date'] = [ '2020-03-01',  np.nan ]\n",
    "test_imhe_summary['educational_fac_end_date'] = [ np.nan,  np.nan]\n",
    "test_imhe_summary['any_gathering_restrict_start_date'] = [ '2020-03-01',  np.nan ]\n",
    "test_imhe_summary['any_gathering_restrict_end_date'] = [ np.nan,  np.nan]\n",
    "test_imhe_summary['any_business_start_date'] = [ '2020-03-01',  np.nan ]\n",
    "test_imhe_summary['any_business_end_date'] = ['2020-06-01',  np.nan]\n",
    "test_imhe_summary['all_non-ess_business_start_date'] = [ '2020-03-01',  np.nan ]\n",
    "test_imhe_summary['all_non-ess_business_end_date'] = [ '2020-06-01',  np.nan]\n",
    "\n",
    "# Convert columns to have dates in datetime64 format\n",
    "cols_w_dates = ['peak_bed_day_mean', 'peak_bed_day_lower', 'peak_bed_day_upper',\n",
    "                'peak_icu_bed_day_mean', 'peak_icu_bed_day_lower', 'peak_icu_bed_day_upper',\n",
    "                'peak_vent_day_mean', 'peak_vent_day_lower', 'peak_vent_day_upper',\n",
    "                'travel_limit_start_date', 'travel_limit_end_date',\n",
    "                'stay_home_start_date', 'stay_home_end_date',\n",
    "                'educational_fac_start_date', 'educational_fac_end_date',\n",
    "                'any_gathering_restrict_start_date', 'any_gathering_restrict_end_date',\n",
    "                'any_business_start_date', 'any_business_end_date',\n",
    "                'all_non-ess_business_start_date', 'all_non-ess_business_end_date']\n",
    "for col in cols_w_dates:\n",
    "    test_imhe_summary[col]= pd.to_datetime(test_imhe_summary[col])\n",
    "        \n",
    "# Set fixed values for test hospitalizations but for a limited date range\n",
    "\n",
    "# Generate a test date range\n",
    "date1 = date(2020, 3, 22)\n",
    "date2 = date(2020, 8, 1)\n",
    "dates_str_list = list(daterange(date1, date2))\n",
    "dates = [datetime.fromisoformat(day).date() for day in dates_str_list]\n",
    "\n",
    "allbed = np.arange(len(dates))*1000\n",
    "allicu = allbed*0.1\n",
    "allvent = allbed*0.05\n",
    "test_imhe_hospitalizations['dates'] = 2*[dates] \n",
    "test_imhe_hospitalizations['allbed_mean'] = [allbed.astype(int).tolist(), (0.5*allbed).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['allbed_lower'] = [ (0.9*allbed).astype(int).tolist(), (0.9*0.5*allbed).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['allbed_upper'] = [ (1.1*allbed).astype(int).tolist(), (1.1*0.5*allbed).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['ICUbed_mean'] = [allicu.astype(int).tolist(), (0.5*allicu).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['ICUbed_lower'] = [ (0.9*allicu).astype(int).tolist(), (0.9*0.5*allicu).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['ICUbed_upper'] = [ (1.1*allicu).astype(int).tolist(), (1.1*0.5*allicu).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['InvVen_mean'] = [allvent.astype(int).tolist(), (0.5*allvent).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['InvVen_lower'] = [ (0.9*allvent).astype(int).tolist(), (0.9*0.5*allvent).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['InvVen_upper'] = [allvent.astype(int).tolist(), (0.5*allvent).astype(int).tolist() ]\n",
    "\n",
    "# Assume constant 10 deaths/100 admits/20 ICU a day\n",
    "deaths = np.ones(len(dates))*10\n",
    "admits = np.ones(len(dates))*10\n",
    "newicus = np.ones(len(dates))*10\n",
    "test_imhe_hospitalizations['deaths_mean'] = [ deaths.astype(int).tolist(), deaths.astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['deaths_lower'] = [ (0.9*deaths).astype(int).tolist(), (0.8*deaths).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['deaths_upper'] = [ (1.1*deaths).astype(int).tolist(), (1.2*deaths).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['admis_mean'] = [ admits.astype(int).tolist(), admits.astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['admis_lower'] = [ (0.9*admits).astype(int).tolist(), (0.8*admits).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['admis_upper'] = [ (1.1*admits).astype(int).tolist(), (1.2*admits).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['newICU_mean'] = [ newicus.astype(int).tolist(), newicus.astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['newICU_lower'] = [ (0.9*newicus).astype(int).tolist(), (0.8*newicus).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['newICU_upper'] = [ (1.1*newicus).astype(int).tolist(), (1.2*newicus).astype(int).tolist() ]\n",
    "\n",
    "# Compute total deaths\n",
    "cumildeaths = np.cumsum(deaths)\n",
    "test_imhe_hospitalizations['totdea_mean'] = [ np.cumsum(deaths).astype(int).tolist(), np.cumsum(deaths).astype(int).tolist()]\n",
    "test_imhe_hospitalizations['totdea_lower'] = [ np.cumsum(0.9*deaths).astype(int).tolist(), np.cumsum(0.8*deaths).astype(int).tolist() ]\n",
    "test_imhe_hospitalizations['totdea_upper'] = [ np.cumsum(1.1*deaths).astype(int).tolist(), np.cumsum(1.2*deaths).astype(int).tolist() ]\n",
    "\n",
    "# Don't bother with smoothing test data\n",
    "test_imhe_hospitalizations['deaths_mean_smoothed'] = test_imhe_hospitalizations['deaths_mean']\n",
    "test_imhe_hospitalizations['deaths_lower_smoothed'] = test_imhe_hospitalizations['deaths_lower']\n",
    "test_imhe_hospitalizations['deaths_upper_smoothed'] = test_imhe_hospitalizations['deaths_upper']\n",
    "test_imhe_hospitalizations['totdea_mean_smoothed'] = test_imhe_hospitalizations['totdea_mean']\n",
    "test_imhe_hospitalizations['totdea_lower_smoothed'] = test_imhe_hospitalizations['totdea_lower']\n",
    "test_imhe_hospitalizations['totdea_upper_smoothed'] = test_imhe_hospitalizations['totdea_upper']\n",
    "\n",
    "# Assume no tests first 10 days, then 10000 tests a day (observed for first 90, then projected)\n",
    "typelist = 10*[np.nan] + 90*['observed'] + (len(dates) - 100)*['predicted']\n",
    "tests = 10*[np.nan] + (len(dates) - 10)*[10000]\n",
    "test_imhe_hospitalizations['total_tests_data_type'] = [ typelist, typelist]\n",
    "test_imhe_hospitalizations['total_tests'] = [ tests, tests]\n",
    "\n",
    "# Infections should increase a 10 per day and 20 per day, make estimated rates \n",
    "infections = np.arange(len(dates))*10\n",
    "infections = np.arange(len(dates))*10\n",
    "\n",
    "test_imhe_hospitalizations['confirmed_infections'] = [ infections.astype(int).tolist(), (2*infections).astype(int).tolist()]\n",
    "test_imhe_hospitalizations['est_infections_mean'] = [ infections.astype(int).tolist(), (2*infections).astype(int).tolist()]\n",
    "test_imhe_hospitalizations['est_infections_lower'] = [ (0.8*infections).astype(int).tolist(), (1.95*infections).astype(int).tolist()]\n",
    "test_imhe_hospitalizations['est_infections_upper'] = [ (1.2*infections).astype(int).tolist(), (2.05*infections).astype(int).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save the same data to pickle files\n",
    "#\n",
    "\n",
    "# Reset indices before exporting\n",
    "test_imhe_summary.reset_index(drop=True, inplace=True)\n",
    "test_imhe_hospitalizations.reset_index(drop=True, inplace=True)\n",
    "\n",
    "imhe_summary_fname = test_dir + \"TEST_imhe_summary.p\"\n",
    "print(\" - IMHE state level summary test data exported to \", imhe_summary_fname)\n",
    "with open(imhe_summary_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_imhe_summary, pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "imhe_hospitalizations_fname = test_dir + \"TEST_imhe_hospitalizations.p\"\n",
    "print(\" - IMHE hospitalization level summary test data exported to \", imhe_summary_fname)\n",
    "with open(imhe_hospitalizations_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_imhe_hospitalizations, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime lists into strings\n",
    "test_imhe_hospitalizations['dates'] = test_imhe_hospitalizations['dates'].apply(COVIDdata.dates2strings)\n",
    "\n",
    "# Write out CSV files to disk\n",
    "imhe_summary_fname = test_dir + \"TEST_imhe_summary.csv\"\n",
    "print(\" - IMHE state level summary test data also exported to \", imhe_summary_fname)\n",
    "test_imhe_summary.to_csv(imhe_summary_fname, index=False)\n",
    "\n",
    "imhe_hospitalizations_fname = test_dir + \"TEST_imhe_hospitalizations.csv\"\n",
    "print(\" - IMHE hospitalization level summary test also data exported to \", imhe_summary_fname)\n",
    "test_imhe_hospitalizations.to_csv(imhe_hospitalizations_fname, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the start of processing\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"Entire process of executing this script took {end-start:0.2f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating $R_t$ Live data on Reproduction Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Rt Live data\n",
    "\n",
    "print(\"\\n- Retrieving Rt Live Effective Reproduction Rate Data\")\n",
    "\n",
    "# Retrieve the Rt live dataframe\n",
    "Rt_live_df = COVIDdata.retrieve_Rt_live_data(state_pop_df)\n",
    "\n",
    "# Post-process the Rt data to add postal codes\n",
    "COVIDdata.cleanRtdata(Rt_live_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test subsets containing only local data\n",
    "test_Rt_live_df= Rt_live_df[(Rt_live_df['FIPS'] == MNFIPS) | (Rt_live_df['FIPS'] == NDFIPS)].copy()\n",
    "test_Rt_live_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for MN and ND\n",
    "MNidx = test_Rt_live_df.index[test_Rt_live_df['FIPS'] == MNFIPS].tolist()[0]\n",
    "NDidx = test_Rt_live_df.index[test_Rt_live_df['FIPS'] == NDFIPS].tolist()[0]\n",
    "\n",
    "# Fill with bogus data\n",
    "dates_len = 100\n",
    "dates_list = test_Rt_live_df[(test_Rt_live_df['FIPS'] == MNFIPS)]['dates'].values.tolist()[0]\n",
    "dates_list = dates_list[0:dates_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign bogus MN values\n",
    "test_Rt_live_df.at[MNidx, 'dates'] = dates_list\n",
    "test_Rt_live_df.at[MNidx, 'Rt_mean'] = [0.9]*dates_len\n",
    "test_Rt_live_df.at[MNidx, 'Rt_median'] = [0.9]*dates_len\n",
    "test_Rt_live_df.at[MNidx, 'Rt_lower_80'] = [0.75]*dates_len\n",
    "test_Rt_live_df.at[MNidx, 'Rt_upper_80'] = [1.05]*dates_len\n",
    "\n",
    "# Assign bogus ND values\n",
    "test_Rt_live_df.at[NDidx, 'dates'] = dates_list\n",
    "test_Rt_live_df.at[NDidx, 'Rt_mean'] = [1.0]*dates_len\n",
    "test_Rt_live_df.at[NDidx, 'Rt_median'] = [1.0]*dates_len\n",
    "test_Rt_live_df.at[NDidx, 'Rt_lower_80'] = [0.85]*dates_len\n",
    "test_Rt_live_df.at[NDidx, 'Rt_upper_80'] = [1.15]*dates_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Rt live data\n",
    "print(\"   - Exporting test Rt live data\")\n",
    "    \n",
    "#\n",
    "# Save the data to pickle and CSV files\n",
    "#\n",
    "\n",
    "Rt_live_fname = test_dir + \"TEST_Rt_live.p\"\n",
    "print(\"   - Rt live data exported to \", Rt_live_fname)\n",
    "with open(Rt_live_fname, 'wb') as pickle_file:\n",
    "    pickle.dump(test_Rt_live_df, pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "# Convert datetime lists into strings\n",
    "test_Rt_live_df['dates'] = test_Rt_live_df['dates'].apply(COVIDdata.dates2strings)\n",
    "\n",
    "# Write out CSV files to disk\n",
    "Rt_live_fname = test_dir + \"TEST_Rt_live.csv\"\n",
    "print(\"   - Rt live data also exported to \", Rt_live_fname)\n",
    "test_Rt_live_df.to_csv(Rt_live_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
