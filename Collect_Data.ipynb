{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Condensing COVID Data\n",
    "\n",
    "This Jupyter notebook reads in the data from a variety of online sources that we need for the COVID Data Vizualization Project.  Some attempts are made to produce simpler to work with output files.  Depending on how long this notebook takes\n",
    "to execute, it may not make sense to 'condense' the data first.\n",
    "\n",
    "- **A Note on the use of Pandas:** I am currently using `Pandas` (aka Python Data Analysis Library, see https://pandas.pydata.org) to read in the CSV files and manipualte them.  This has advantages and annoyances, there may be much better ways to do this, but I was giving this a try for now.  One big annoyance is Pandas insists on labelling each row of data with a index number.  Luckily its pretty easy in many cases to convert Pandas dataframes into lists of lists or numpy arrays for easier data handling.  I do exactly this to very quickly compute the derivatives of the confirmed/deaths/recovered numbers in over 3000 counties in the US.\n",
    "\n",
    "- **A Note about FIPS:** Some of the data includes FIPS codes (a standard geographic identifier) which should ease the process of cross-matching of data.  Clay County is 27027 and Cass County is 38017.  Minnesota is 27, North Dakota is 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import git\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables of interest below\n",
    "data_dir = 'our_data/'    # Data directory for files we created\n",
    "\n",
    "## Define FIPS corresponding to various local areas\n",
    "ClayFIPS = 27027\n",
    "CassFIPS = 38017\n",
    "MNFIPS = 27\n",
    "NDFIPS = 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Census Data on Populations of States/Counties (FIPS Present)\n",
    "\n",
    "This data from the US Census Bureau estimates the population in July 2019.  Description of the file format is at https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf\n",
    "\n",
    "- **County Level Data**: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\n",
    "- **State Level Data**: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\n",
    "\n",
    "**Suggested Citation**:  U.S. Census Bureau, Population Division (Release Date: March 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Manipulate the US Census Bureau's population estimate data and save a reduced datafile\n",
    "##\n",
    "\n",
    "## When I retrieved the files, I got an error that `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 2: invalid continuation byte`, turns out it is encoded `latin-1`.\n",
    "#census_state_csv = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\"\n",
    "#state_columns_of_interest = {'STATE', 'NAME', 'CENSUS2010POP', 'N_POPCHG2019', 'POPESTIMATE2019'}\n",
    "#census_state_df = pd.read_csv(census_state_csv, usecols=state_columns_of_interest, encoding='latin-1')    # County totals\n",
    "\n",
    "# Create pandas dataframes containing the selected population data for each state/county\n",
    "census_county_csv = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\"\n",
    "county_columns_of_interest = {'STATE', 'COUNTY', 'STNAME', 'CTYNAME', 'NPOPCHG_2019', 'POPESTIMATE2019'}\n",
    "census_county_df = pd.read_csv(census_county_csv,usecols=county_columns_of_interest, encoding='latin-1')  \n",
    "\n",
    "# Separate state level data from county level data (by creating separate copies in memory)\n",
    "county_data_df = census_county_df[census_county_df['COUNTY'] != 0].copy()\n",
    "state_data_df = census_county_df[census_county_df['COUNTY'] == 0].copy()\n",
    "\n",
    "##\n",
    "## Manipulate the state-level population data (actually grabbed from county file, since its there anyway)\n",
    "##\n",
    "\n",
    "# Add FIPS column for state data then DROP county data and move FIPS to first column before exporting\n",
    "state_data_df['FIPS'] = state_data_df['STATE']\n",
    "state_data_df.drop(columns=['STATE','COUNTY','CTYNAME'], inplace=True)\n",
    "state_data_df = state_data_df.reindex(columns=(['FIPS'] + list([col for col in state_data_df.columns if col != 'FIPS']) ))\n",
    "\n",
    "# Compute percent change in population in 2018-19\n",
    "state_data_df['PPOPCHG_2019'] = 100*(state_data_df['NPOPCHG_2019']/state_data_df['POPESTIMATE2019'])\n",
    "\n",
    "# We may want to do a daily extrapolation of population since POPESTIMATE2019 is est. population on July 1, 2019 \n",
    "# and NPOPCHG_2019 is the estimated change between July 1, 2018 and July 1, 2019.  Realistically, this is probably\n",
    "# overkill since the increased deaths from Coronavirus are not taken into account in such an extrapolation.\n",
    "\n",
    "# Save the processed data file\n",
    "# out_states = data_dir + \"population_data_states.csv\"\n",
    "# state_data_df.to_csv(out_states, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Manipulate the county-level population data and save a reduced datafile\n",
    "##\n",
    "\n",
    "# In county data create FIPS column, remove redundant columns, and then move FIPS columns to first column\n",
    "county_data_df['FIPS'] = county_data_df['STATE']*1000 + county_data_df['COUNTY']\n",
    "county_data_df.drop(columns=['STATE','COUNTY'], inplace=True)\n",
    "county_data_df = county_data_df.reindex(columns=(['FIPS'] + list([col for col in county_data_df.columns if col != 'FIPS']) ))\n",
    "\n",
    "# Compute percent change in population in 2018-19\n",
    "county_data_df['PPOPCHG_2019'] = 100*(county_data_df['NPOPCHG_2019']/county_data_df['POPESTIMATE2019'])\n",
    "\n",
    "# We may want to do a daily extrapolation of population since POPESTIMATE2019 is est. population on July 1, 2019 \n",
    "# and NPOPCHG_2019 is the estimated change between July 1, 2018 and July 1, 2019.  Realistically, this is probably\n",
    "# overkill since the increased deaths from Coronavirus are not taken into account in such an extrapolation.\n",
    "\n",
    "# Save the processed data file\n",
    "# out_counties = data_dir + \"population_data_counties.csv\"\n",
    "# county_data_df.to_csv(out_counties, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the local state level population data\n",
    "print(\"STATE LEVEL DATA IN state_data_df() DATAFRAME\")\n",
    "print(state_data_df[(state_data_df['FIPS'] == MNFIPS) | (state_data_df['FIPS'] == NDFIPS)])\n",
    "\n",
    "# Showing the local county level population data\n",
    "print(\"\\nCOUNTY LEVEL DATA IN county_data_df() DATAFRAME\")\n",
    "print(county_data_df[(county_data_df['FIPS'] == ClayFIPS) | (county_data_df['FIPS'] == CassFIPS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Novel Coronavirus (COVID-19) Cases Data (FIPS Present)\n",
    "    - https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases\n",
    "\n",
    "This dataset is part of COVID-19 Pandemic Novel Corona Virus (COVID-19)\n",
    "epidemiological data since 22 January 2020. The data is compiled by the Johns\n",
    "Hopkins University Center for Systems Science and Engineering (JHU CCSE) from\n",
    "various sources including the World Health Organization (WHO), DXY.cn, BNO\n",
    "News, National Health Commission of the Peopleâ€™s Republic of China (NHC),\n",
    "China CDC (CCDC),Hong Kong Department of Health, Macau Government, Taiwan\n",
    "CDC, US CDC, Government of Canada, Australia Government Department of Health,\n",
    "European Centre for Disease Prevention and Control (ECDC), Ministry of Health\n",
    "Singapore (MOH), and others. JHU CCSE maintains the data on the 2019 Novel\n",
    "Coronavirus COVID-19 (2019-nCoV) Data Repository on Github\n",
    "(https://github.com/CSSEGISandData/COVID-19).\n",
    "\n",
    "### Notes about the data files:\n",
    "- County level daily confirmed/deaths/recovered data files changes format\n",
    "  several times before April 23, 2020, so I didn't include that data. \n",
    "- State level daily data files contain some additional data that county level\n",
    "  files do not contain, notably Incident_Rate, People_Tested,\n",
    "  People_Hospitalized, Mortality_Rate, Testing_Rate, and Hospitalization_Rate. \n",
    "  However, it only exists starting April 12, 2020.\n",
    "- Time-series data files contain more limited data (only confirmed cases and\n",
    "  deaths) and are essentially redundant data compared to the daily files, so\n",
    "  combining the daily files makes sense.\n",
    "\n",
    "### Notes about this process: \n",
    "- By processing the US Census Bureau's population data first, I can get a list\n",
    "  of legitimate 'FIPS' values to use, so that I can perform 'left' joins to a\n",
    "  list of FIPS addresses instead of costly 'outer' joins. \n",
    "- It turns out there are DUPLICATE FIPS entries for some of the John Hopkins\n",
    "  data.  So I went through those and picked out the ones with the higher values\n",
    "  for confirmed/deaths/recovered assuming they were entered later. \n",
    "- Once that was done, I constructed a SINGLE Pandas dataframe holding all the\n",
    "  time-series data and wrote it to a CSV file. Accessing this one file will be a\n",
    "  lot faster than looping through all the datafiles each time we load up the\n",
    "  data.\n",
    "  \n",
    "**Suggested Citation**: the COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the John Hopkins data directory\n",
    "JHdata_dir = \"JH_Data/\"\n",
    "\n",
    "# Git pull to sync up the data set to the current version on GitHub\n",
    "g = git.cmd.Git(JHdata_dir)\n",
    "status = g.pull()  # We should check status to see everything is good eventually, for now, I am using this to hide the status message from GitPython module\n",
    "\n",
    "# Daily tabulation of all confirmed/deaths/recovered data is in the following directories\n",
    "daily_cnty_dir = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports/\" # For each admin unit (in the US, that's county) for each day.\n",
    "daily_state_dir = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports_us/\" # For each state (somewhat redundant, but avoids recomputation I suppose)\n",
    "\n",
    "# Individual time series data for confirmed cases and deaths in the US counties and states\n",
    "ts_us_confirmed_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n",
    "ts_us_dead_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define various functions we will use below\n",
    "##\n",
    "\n",
    "def csvfiles(path):\n",
    "    contents = os.listdir(path);\n",
    "    csvs = [ file for file in contents if file.endswith(\".csv\") ]\n",
    "    \n",
    "    for file in sorted(csvs, key=date2num):\n",
    "        yield file\n",
    "\n",
    "        \n",
    "def isodate2num(s):\n",
    "    # Takes input filename of CSV file and returns it sorted by date implies assuming\n",
    "    # filenames with MM-DD-YYYY.csv format, returning YYYY+MM+DD for sorting purposes.\n",
    "    M = int(s[0:2])\n",
    "    D = int(s[3:5])\n",
    "    Y = int(s[6:10])\n",
    "    return (f\"{Y:04d}-{M:02d}-{D:02d}\")\n",
    "\n",
    "\n",
    "def date2num(s):\n",
    "    # Takes input filename of CSV file and returns it sorted by date implies assuming\n",
    "    # filenames with MM-DD-YYYY.csv format, returning YYYY+MM+DD for sorting purposes.\n",
    "    M = int(s[0:2])\n",
    "    D = int(s[3:5])\n",
    "    Y = int(s[6:10])\n",
    "    return (f\"{Y:04d}{M:02d}{D:02d}\")\n",
    "\n",
    "\n",
    "def iso2days(iso):\n",
    "    # To make computing the derivative easy, compute days since January 1, 2020\n",
    "    ref = datetime.fromisoformat(\"2020-01-01\")\n",
    "    return (datetime.fromisoformat(iso) - ref)/timedelta(days=1)\n",
    "\n",
    "\n",
    "def derivative(x, y):\n",
    "    \"\"\"\n",
    "    Compute forward difference estimate for the derivative of y with respect\n",
    "    to x.  The x and y arrays are 2-D with the rows being different data sets\n",
    "    and the columns being the x/y values in each data set.\n",
    "    \n",
    "    The input and must be the same size.\n",
    "\n",
    "    Note that we copy the first known derivative values into the zeroth column, since \n",
    "    the derivatve for the first point is not a known value.\n",
    "    \"\"\"\n",
    "    # Compute the numerator (y[i+1] - y[i]) for all rows in the entire array at once\n",
    "    dy = y[:, 1:] - y[:, 0:-1]\n",
    "    # Compute the denominator (x[i+1] - x[i]) for all rows in the for entire array at once\n",
    "    dx = x[:, 1:] - x[:, 0:-1]\n",
    "    # Compute the derivatives for all points in the array at once\n",
    "    dydx = dy / dx\n",
    "    # Get first column to horizontal stack with numpy array\n",
    "    first_col = dydx[:,0][..., None] # The [..., None] bit keeps (n, 1) shape (a la https://stackoverflow.com/questions/15815854/how-to-add-column-to-numpy-array)\n",
    "    return np.hstack((first_col, dydx))\n",
    "\n",
    "\n",
    "def reduce_local_dataframe(raw_df, fips_df):\n",
    "    # Reduce the raw dataframe of all COVID data to just the relevant entries for the US\n",
    "    # Match to the list of known FIPS values from Census data\n",
    "    \n",
    "    # Perform a left join to the FIPS list\n",
    "    reduced_df = fips_df.join(raw_df.set_index('FIPS'), on='FIPS', how='left').copy()\n",
    "\n",
    "    # Here's the fund part, turns out the John Hopkins data has duplicate lines for a few FIPS\n",
    "    # entries, so what I did was assume the HIGHER values of COVID confirmed, deaths, and recovered\n",
    "    # were the accurate ones and just kept those.  This meant checking all the duplicates and then\n",
    "    # purging them, re-writing the best values into the row of data.\n",
    "\n",
    "    update_confirmed = {}\n",
    "    update_deaths = {}\n",
    "    update_recovered = {}\n",
    "    # Identify duplicate FIPS entries (assumed mangled data)\n",
    "    duplicates = reduced_df[reduced_df.duplicated(['FIPS'], keep='first')]\n",
    "    # Loop through all data for each duplicate FIPS entry\n",
    "    for index, row in duplicates.iterrows():\n",
    "        max_confirmed = 0\n",
    "        max_deaths = 0\n",
    "        max_recovered = 0\n",
    "        checkFIPS = row['FIPS']\n",
    "        # Track the maximum value of confirmed/deaths/recovered for each FIPS\n",
    "        for dup_ind, dup_row in reduced_df[reduced_df['FIPS'] == row['FIPS']].iterrows():\n",
    "            if max_confirmed<dup_row['Confirmed']:\n",
    "                max_confirmed = dup_row['Confirmed']\n",
    "            if max_deaths<dup_row['Deaths']:\n",
    "                max_deaths = dup_row['Deaths']\n",
    "            if max_recovered<dup_row['Recovered']:\n",
    "                max_recovered = dup_row['Recovered']\n",
    "        update_confirmed.update( {checkFIPS : max_confirmed} )\n",
    "        update_deaths.update( {checkFIPS : max_deaths} )\n",
    "        update_recovered.update( {checkFIPS : max_recovered} )\n",
    "\n",
    "    # Drop duplicates from pandas dataframe\n",
    "    reduced_df.drop_duplicates(subset='FIPS', keep='first', inplace=True)\n",
    "\n",
    "    # Fix values in duplicate lines\n",
    "    for key in update_confirmed:\n",
    "        reduced_df.loc[(reduced_df['FIPS'] == key), ['Confirmed', 'Deaths', 'Recovered']] = [update_confirmed[key], update_deaths[key], update_recovered[key]]\n",
    "        \n",
    "    # Return the fixed dataframe\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Load the time series datafiles to experiment with them.  These only contain Deaths and Confirmed cases,\n",
    "## so I suspect we won't keep them, since I build the same data from the daily files above.\n",
    "##\n",
    "\n",
    "# Create pandas dataframes containing time-series data (We could reconstruct this by looping through all the daily data, since this is missing number of recovered)\n",
    "ts_us_dead_df = pd.read_csv(ts_us_dead_csv)            # Deaths in time series\n",
    "ts_us_confirmed_df = pd.read_csv(ts_us_confirmed_csv)  # Confirmed in time series\n",
    "\n",
    "# We could transpose the dataframe to allow easier extraction of time series data on a per county level\n",
    "tmp_df = ts_us_confirmed_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"confirmed\" }, inplace = True)\n",
    "confirmed_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')]  # Use pattern matching to find real dates and include\n",
    "\n",
    "tmp_df = ts_us_dead_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"dead\" }, inplace = True)\n",
    "dead_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')] # Use pattern matching to find real dates and include\n",
    "\n",
    "# Merge the confirmed ill and dead into one dataframe (would like recovered too, but that's not in\n",
    "# these times series files).  \n",
    "merged_clay = confirmed_clay.merge(dead_clay, left_index=True, right_index=True)\n",
    "plot = merged_clay.plot(figsize=(10,8))\n",
    "xlabel = plt.xlabel('Date')\n",
    "ylabel = plt.title('Confirmed COVID Infections and Deaths')\n",
    "title = plt.title('Clay County Confirmed COVID Infections and Deaths')\n",
    "\n",
    "# NOTE: This is using PANDAS to do the plotting, it will be a lot more flexible to extra data from Pandas and then\n",
    "# use matplotlib to make the plots.  For one thing, we could add labels to the plot more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## This is mostly just showing I can just grab a single dataset for the most recent daily data instead of \n",
    "## trying to grab everything and put it all together.  However, this only allows printing some data\n",
    "##\n",
    "\n",
    "# Grab complete list of all csvs to then...\n",
    "world_csvs = list(csvfiles(daily_cnty_dir))\n",
    "us_csvs = list(csvfiles(daily_state_dir))\n",
    "\n",
    "# .. grab the most recent CSV file to open the data.\n",
    "daily_world_csv = daily_cnty_dir+world_csvs[-1]\n",
    "daily_us_csv = daily_state_dir+us_csvs[-1]\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files \n",
    "# (contains number of confirmed/deaths/recovered on that date)\n",
    "daily_world_df = pd.read_csv(daily_world_csv)   # County/Admin totals\n",
    "daily_us_df = pd.read_csv(daily_us_csv)         # State totals\n",
    "\n",
    "# Print county data to screen\n",
    "print(\"LOCAL COUNTY DATA IN daily_world_df() DATAFRAME\")\n",
    "print(daily_world_df[ (daily_world_df['FIPS'] == ClayFIPS) | (daily_world_df['FIPS'] == CassFIPS) ])\n",
    "\n",
    "# Print state level data to screen (which has data on testing and hospitalization rates)\n",
    "print(\"\\nLOCAL STATE DATA IN daily_us_df() DATAFRAME\")\n",
    "print(daily_us_df[ (daily_us_df['FIPS'] == MNFIPS) | (daily_us_df['FIPS'] == NDFIPS) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Build combined county-level datafiles\n",
    "##\n",
    "\n",
    "# Build a dataframe containing legitimate FIPS values using county level data\n",
    "fips_df = county_data_df.copy()\n",
    "fips_df.drop(columns=['STNAME', 'CTYNAME', 'POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "\n",
    "# Scan through the more complete daily files of county level data and construct a single datafile for our use (restricting \n",
    "# to US only).  It turns out the format of these local level files changes with the date.  The files before March 22, 2020 \n",
    "# were in several different forms and the placenames were NOT consistent.  Thus, to make things managable, I am ignoring\n",
    "# that early local level data...\n",
    "sorted_csvs = []\n",
    "dates_list = []\n",
    "for file in csvfiles(daily_cnty_dir):\n",
    "    # County level date only starts on March 22, before then it is a mis-mosh of place names in the Province_State field\n",
    "    # So only keep that data\n",
    "    this_isodate = isodate2num(file)\n",
    "    this_date = date2num(file)\n",
    "    if (int(this_date) >= 20200322):\n",
    "        # Append to list of accessed csv files\n",
    "        sorted_csvs.append(daily_cnty_dir+file)\n",
    "\n",
    "        # Grab the data from the CSV file\n",
    "        raw_df = pd.read_csv(sorted_csvs[-1])\n",
    "\n",
    "        # Rename columns in early forms to late forms of column names for consistency\n",
    "        raw_df.rename(columns={ 'Province/State': 'Province_State', \n",
    "                               'Country/Region':'Country_Region', \n",
    "                               'Last Update':'Last_Update' }, inplace = True)\n",
    "        \n",
    "        # Match to the list of known FIPS values from Census data, also removing duplicate rows\n",
    "        reduced_df = reduce_local_dataframe(raw_df, fips_df)\n",
    "        \n",
    "        # Provide progress report\n",
    "        idx = len(sorted_csvs)\n",
    "        # print(f'Processing Date #{idx}: {this_isodate}')\n",
    "        \n",
    "        if (idx == 1):\n",
    "            # Create combined dataframe sorted by FIPS\n",
    "            combined_cols = ['FIPS', 'Admin2', 'Province_State', 'Lat', 'Long_']\n",
    "            combined_cnty_df = reduced_df[combined_cols].copy()\n",
    "        \n",
    "            # Create blank dataframes to store time series data\n",
    "            confirmed_df = fips_df.copy()\n",
    "            deaths_df = fips_df.copy()\n",
    "            recovered_df = fips_df.copy()\n",
    "            \n",
    "        ## Create dataframes for temporarily storing time series date\n",
    "        # Append date to list of dates\n",
    "        dates_list.append(this_isodate)\n",
    "            \n",
    "        # Store Confirmed by merging reduced list and renaming column\n",
    "        confirmed_df = pd.merge(confirmed_df,reduced_df[['FIPS','Confirmed']],on='FIPS', how='left', copy=True)\n",
    "        confirmed_col = \"C\"+f\"{idx:03d}\"\n",
    "        confirmed_df.rename(columns={'Confirmed': confirmed_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "        # Store Deaths by merging reduced list and renaming column\n",
    "        deaths_df = pd.merge(deaths_df,reduced_df[['FIPS','Deaths']],on='FIPS', how='left', copy=True)\n",
    "        deaths_col = \"D\"+f\"{idx:03d}\"\n",
    "        deaths_df.rename(columns={'Deaths': deaths_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "        # Store Recovered by merging reduced list and renaming column\n",
    "        recovered_df = pd.merge(recovered_df,reduced_df[['FIPS','Recovered']],on='FIPS', how='left', copy=True)\n",
    "        recovered_col = \"R\"+f\"{idx:03d}\"\n",
    "        recovered_df.rename(columns={'Recovered': recovered_col}, errors=\"raise\", inplace=True)\n",
    "            \n",
    "# Final cleanup (convert to integers and remove NaN for the confirmed and deaths [don't touch recovered yet])\n",
    "confirmed_df = confirmed_df.replace(np.nan,0).astype('int')\n",
    "deaths_df= deaths_df.replace(np.nan,0).astype('int')\n",
    "\n",
    "# Add lists of dates to the combined dataframe as a single 'Dates' column\n",
    "combined_cnty_df['Dates'] = [dates_list]*len(combined_cnty_df)\n",
    "# Add time-series list of confirmed to the combined dataframe as a single 'Confirmed' column\n",
    "confirmed_listOlists = confirmed_df[ confirmed_df.columns[confirmed_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_cnty_df['Confirmed'] = confirmed_listOlists\n",
    "# Add time-series list of deaths to the combined dataframe as a single 'Deaths' column\n",
    "deaths_listOlists = deaths_df[ deaths_df.columns[deaths_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_cnty_df['Deaths'] = deaths_listOlists\n",
    "# Add time-series list of recovered to the combined dataframe as a single 'Recovered' column\n",
    "recovered_listOlists = recovered_df[ recovered_df.columns[recovered_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_cnty_df['Recovered'] = recovered_listOlists\n",
    "\n",
    "# Convert the list of dates into numpy array of days since Jan. 1, 2020 for each observation\n",
    "dates = combined_cnty_df[combined_cnty_df['FIPS'] == ClayFIPS]['Dates'].tolist()[0]\n",
    "dates_list = []\n",
    "for dat in dates:\n",
    "    dates_list.append( iso2days(dat) )\n",
    "dates_arr = np.array([dates_list]*len(combined_cnty_df))\n",
    "\n",
    "# Convert confirmed/deaths/recovered into arrays\n",
    "confirmed_arr = np.array(confirmed_listOlists)\n",
    "deaths_arr = np.array(deaths_listOlists)\n",
    "\n",
    "# At this point I have arrays where the rows are individiual FIPS (counties) and the columns are \n",
    "# (depending on the array) the days since 1/1/2020, number of confirmed cases, number of deaths, \n",
    "# and number of recovered.\n",
    "\n",
    "# Compute the derivatives (using forward derivative approach)\n",
    "dconfirmed_arr = derivative(dates_arr, confirmed_arr)\n",
    "ddeaths_arr = derivative(dates_arr, deaths_arr)\n",
    "\n",
    "# Compute the second derivatives (a bit hinky to use forward derivative again, but...)\n",
    "d2confirmed_arr = derivative(dates_arr, dconfirmed_arr)\n",
    "d2deaths_arr = derivative(dates_arr, ddeaths_arr)\n",
    "\n",
    "# Convert numpy arrays to lists of lists for storage in combined dataframe\n",
    "combined_cnty_df['dConfirmed'] = dconfirmed_arr.tolist()\n",
    "combined_cnty_df['d2Confirmed'] = d2confirmed_arr.tolist()\n",
    "combined_cnty_df['dDeaths'] = ddeaths_arr.tolist()\n",
    "combined_cnty_df['d2Deaths'] = d2deaths_arr.tolist()\n",
    "\n",
    "# Add population data to same array\n",
    "combined_cnty_df = pd.merge(combined_cnty_df,county_data_df[['FIPS','POPESTIMATE2019', 'NPOPCHG_2019']], on='FIPS', how='left', copy=True)\n",
    "\n",
    "# Rename some columns before export\n",
    "combined_cnty_df.rename(columns={ 'Admin2': 'County', \n",
    "                                 'Province_State': 'State', \n",
    "                                  'POPESTIMATE2019' : 'PopEst2019',\n",
    "                                  'NPOPCHG_2019' : 'PopChg2019'}, inplace = True)\n",
    "\n",
    "# Save the processed time-series data into single file\n",
    "combined_datafile = data_dir + \"countylevel_combinedCDR.csv\"\n",
    "combined_cnty_df.to_csv(combined_datafile, index=False)\n",
    "\n",
    "# Clear variables\n",
    "del sorted_csvs, dates_list\n",
    "del fips_df, raw_df, confirmed_df, deaths_df, recovered_df\n",
    "del confirmed_listOlists, deaths_listOlists, recovered_listOlists\n",
    "del dates_arr, confirmed_arr, deaths_arr\n",
    "del dconfirmed_arr, ddeaths_arr, d2confirmed_arr, d2deaths_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMBINED DAILY DATA IN combined_cnty_df() DATAFRAME\")\n",
    "print(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS) | (combined_cnty_df['FIPS'] == CassFIPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Build combined state-level datafiles\n",
    "##\n",
    "\n",
    "# Build a dataframe containing legitimate FIPS values using county level data\n",
    "fips_df = state_data_df.copy()\n",
    "fips_df.drop(columns=['STNAME', 'POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "\n",
    "# Scan through the more complete daily files of state level data and construct a single datafile for our use (restricting \n",
    "# to US only).  These files are all the same format, but only start after April 12, 2020.  For April 18/19 they accidentally\n",
    "# included data from other nations.  So this will need to be purged\n",
    "sorted_csvs = []\n",
    "dates_list = []\n",
    "for file in csvfiles(daily_state_dir):\n",
    "    # Set up date information in memory\n",
    "    this_isodate = isodate2num(file)\n",
    "    \n",
    "    # Append to list of accessed csv files\n",
    "    sorted_csvs.append(daily_state_dir+file)\n",
    "\n",
    "    # Grab the data from the CSV file\n",
    "    raw_df = pd.read_csv(sorted_csvs[-1])\n",
    "    \n",
    "    # Match to the list of known FIPS values from Census data, also removing duplicate rows\n",
    "    reduced_df = reduce_local_dataframe(raw_df, fips_df)\n",
    "        \n",
    "    # Provide progress report\n",
    "    idx = len(sorted_csvs)\n",
    "    # print(f'Processing Date #{idx}: {this_isodate}')\n",
    "        \n",
    "    if (idx == 1):\n",
    "        # Create combined dataframe sorted by FIPS\n",
    "        combined_cols = ['FIPS', 'Province_State', 'Lat', 'Long_']\n",
    "        combined_state_df = reduced_df[combined_cols].copy()\n",
    "        \n",
    "        # Create blank dataframes to store time series data\n",
    "        confirmed_df = fips_df.copy()\n",
    "        deaths_df = fips_df.copy()\n",
    "        recovered_df = fips_df.copy()\n",
    "        incident_rate_df = fips_df.copy()\n",
    "        tested_df = fips_df.copy()\n",
    "        hospitalized_df = fips_df.copy()\n",
    "        mortality_df = fips_df.copy()\n",
    "        testing_rate_df = fips_df.copy()\n",
    "        hospitalization_rate_df = fips_df.copy()\n",
    "            \n",
    "    ## Create dataframes for temporarily storing time series date\n",
    "    # Append date to list of dates\n",
    "    dates_list.append(this_isodate)\n",
    "    \n",
    "    # Store Confirmed by merging reduced list and renaming column\n",
    "    confirmed_df = pd.merge(confirmed_df,reduced_df[['FIPS','Confirmed']],on='FIPS', how='left', copy=True)\n",
    "    confirmed_col = \"C\"+f\"{idx:03d}\"\n",
    "    confirmed_df.rename(columns={'Confirmed': confirmed_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "    # Store Deaths by merging reduced list and renaming column\n",
    "    deaths_df = pd.merge(deaths_df,reduced_df[['FIPS','Deaths']],on='FIPS', how='left', copy=True)\n",
    "    deaths_col = \"D\"+f\"{idx:03d}\"\n",
    "    deaths_df.rename(columns={'Deaths': deaths_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "    # Store Recovered by merging reduced list and renaming column\n",
    "    recovered_df = pd.merge(recovered_df,reduced_df[['FIPS','Recovered']],on='FIPS', how='left', copy=True)\n",
    "    recovered_col = \"R\"+f\"{idx:03d}\"\n",
    "    recovered_df.rename(columns={'Recovered': recovered_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "    # Store Incident Rate by merging reduced list and renaming column\n",
    "    incident_rate_df = pd.merge(incident_rate_df,reduced_df[['FIPS','Incident_Rate']],on='FIPS', how='left', copy=True)\n",
    "    incident_rate_col = \"I\"+f\"{idx:03d}\"\n",
    "    incident_rate_df.rename(columns={'Incident_Rate': incident_rate_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "    # Store People Testing by merging reduced list and renaming column\n",
    "    tested_df = pd.merge(tested_df,reduced_df[['FIPS','People_Tested']],on='FIPS', how='left', copy=True)\n",
    "    tested_col = \"T\"+f\"{idx:03d}\"\n",
    "    tested_df.rename(columns={'People_Tested': tested_col}, errors=\"raise\", inplace=True)\n",
    "    \n",
    "    # Store People Hospitalized by merging reduced list and renaming column\n",
    "    hospitalized_df = pd.merge(hospitalized_df,reduced_df[['FIPS','People_Hospitalized']],on='FIPS', how='left', copy=True)\n",
    "    hospitalized_col = \"H\"+f\"{idx:03d}\"\n",
    "    hospitalized_df.rename(columns={'People_Hospitalized': hospitalized_col}, errors=\"raise\", inplace=True)\n",
    "\n",
    "    # Store Mortality Rate by merging reduced list and renaming column\n",
    "    mortality_df = pd.merge(mortality_df,reduced_df[['FIPS','Mortality_Rate']],on='FIPS', how='left', copy=True)\n",
    "    mortality_col = \"M\"+f\"{idx:03d}\"\n",
    "    mortality_df.rename(columns={'Mortality_Rate': mortality_col}, errors=\"raise\", inplace=True)\n",
    "    \n",
    "    # Store Testing Rate by merging reduced list and renaming column\n",
    "    testing_rate_df = pd.merge(testing_rate_df,reduced_df[['FIPS','Testing_Rate']],on='FIPS', how='left', copy=True)\n",
    "    testing_rate_col = \"T\"+f\"{idx:03d}\"\n",
    "    testing_rate_df.rename(columns={'Testing_Rate': testing_rate_col}, errors=\"raise\", inplace=True)\n",
    "    \n",
    "    # Store Hospitalization Rate by merging reduced list and renaming column\n",
    "    hospitalization_rate_df = pd.merge(hospitalization_rate_df,reduced_df[['FIPS','Hospitalization_Rate']],on='FIPS', how='left', copy=True)\n",
    "    hospitalization_rate_col = \"H\"+f\"{idx:03d}\"\n",
    "    hospitalization_rate_df.rename(columns={'Hospitalization_Rate': hospitalization_rate_col}, errors=\"raise\", inplace=True)\n",
    "    \n",
    "# Final cleanup (convert values that are integers to to integers)\n",
    "confirmed_df = confirmed_df.replace(np.nan,0).astype('int')\n",
    "deaths_df = deaths_df.replace(np.nan,0).astype('int')\n",
    "\n",
    "# Add lists of dates to the combined dataframe as a single 'Dates' column\n",
    "combined_state_df['Dates'] = [dates_list]*len(combined_state_df)\n",
    "# Add time-series list of confirmed to the combined dataframe as a single 'Confirmed' column\n",
    "confirmed_listOlists = confirmed_df[ confirmed_df.columns[confirmed_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Confirmed'] = confirmed_listOlists\n",
    "# Add time-series list of deaths to the combined dataframe as a single 'Deaths' column\n",
    "deaths_listOlists = deaths_df[ deaths_df.columns[deaths_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Deaths'] = deaths_listOlists\n",
    "# Add time-series list of recovered to the combined dataframe as a single 'Recovered' column\n",
    "recovered_listOlists = recovered_df[ recovered_df.columns[recovered_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Recovered'] = recovered_listOlists\n",
    "# Add time-series list of incident rate to the combined dataframe as a single 'Incident_Rate' column\n",
    "incident_rate_listOlists = incident_rate_df[ incident_rate_df.columns[incident_rate_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Incident_Rate'] = incident_rate_listOlists\n",
    "# Add time-series list of people tested to the combined dataframe as a single 'People_Tested' column\n",
    "tested_listOlists = tested_df[ tested_df.columns[tested_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['People_Tested'] = tested_listOlists\n",
    "# Add time-series list of people hospitalized to the combined dataframe as a single 'People_Hospitalized' column\n",
    "hospitalized_listOlists = hospitalized_df[ hospitalized_df.columns[hospitalized_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['People_Hospitalized'] = hospitalized_listOlists\n",
    "# Add time-series list of mortality rates to the combined dataframe as a single 'Mortality_Rate' column\n",
    "mortality_listOlists = mortality_df[ mortality_df.columns[mortality_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Mortality_Rate'] = mortality_listOlists\n",
    "# Add time-series list of testing rates to the combined dataframe as a single 'Testing_Rate' column\n",
    "testing_rate_listOlists = testing_rate_df[ testing_rate_df.columns[testing_rate_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Testing_Rate'] = testing_rate_listOlists\n",
    "# Add time-series list of hospitalization rates to the combined dataframe as a single 'Hospitalization_Rate' column\n",
    "hospitalization_rate_listOlists = hospitalization_rate_df[ hospitalization_rate_df.columns[hospitalization_rate_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_state_df['Hospitalization_Rate'] = hospitalization_rate_listOlists\n",
    "\n",
    "# Convert the list of dates into numpy array of days since Jan. 1, 2020 for each observation\n",
    "dates = combined_state_df[combined_state_df['FIPS'] == MNFIPS]['Dates'].tolist()[0]\n",
    "dates_list = []\n",
    "for dat in dates:\n",
    "    dates_list.append( iso2days(dat) )\n",
    "dates_arr = np.array([dates_list]*len(combined_state_df))\n",
    "\n",
    "# Convert confirmed/deaths/recovered into arrays\n",
    "confirmed_arr = np.array(confirmed_listOlists)\n",
    "deaths_arr = np.array(deaths_listOlists)\n",
    "\n",
    "# At this point I have arrays where the rows are individiual FIPS (counties) and the columns are \n",
    "# (depending on the array) the days since 1/1/2020, number of confirmed cases, number of deaths, \n",
    "# and number of recovered.\n",
    "\n",
    "# Compute the derivatives (using forward derivative approach)\n",
    "dconfirmed_arr = derivative(dates_arr, confirmed_arr)\n",
    "ddeaths_arr = derivative(dates_arr, deaths_arr)\n",
    "\n",
    "# Compute the second derivatives (a bit hinky to use forward derivative again, but...)\n",
    "d2confirmed_arr = derivative(dates_arr, dconfirmed_arr)\n",
    "d2deaths_arr = derivative(dates_arr, ddeaths_arr)\n",
    "\n",
    "# Convert numpy arrays to lists of lists for storage in combined dataframe\n",
    "combined_state_df['dConfirmed'] = dconfirmed_arr.tolist()\n",
    "combined_state_df['d2Confirmed'] = d2confirmed_arr.tolist()\n",
    "combined_state_df['dDeaths'] = ddeaths_arr.tolist()\n",
    "combined_state_df['d2Deaths'] = d2deaths_arr.tolist()\n",
    "\n",
    "# Add population data to same array\n",
    "combined_state_df = pd.merge(combined_state_df,state_data_df[['FIPS','POPESTIMATE2019', 'NPOPCHG_2019']], on='FIPS', how='left', copy=True)\n",
    "\n",
    "# Rename some columns before export\n",
    "combined_state_df.rename(columns={ 'Province_State': 'State', \n",
    "                                  'POPESTIMATE2019' : 'PopEst2019',\n",
    "                                  'NPOPCHG_2019' : 'PopChg2019'}, inplace = True)\n",
    "\n",
    "# Save the processed time-series data into single file\n",
    "combined_datafile = data_dir + \"statelevel_combinedCDR.csv\"\n",
    "combined_state_df.to_csv(combined_datafile, index=False)\n",
    "\n",
    "# Clear variables\n",
    "del sorted_csvs, dates_list\n",
    "del fips_df, raw_df, confirmed_df, deaths_df, recovered_df\n",
    "del confirmed_listOlists, deaths_listOlists, recovered_listOlists, incident_rate_listOlists\n",
    "del tested_listOlists, hospitalized_listOlists, mortality_listOlists, testing_rate_listOlists, hospitalization_rate_listOlists\n",
    "del dates_arr, confirmed_arr, deaths_arr\n",
    "del dconfirmed_arr, ddeaths_arr, d2confirmed_arr, d2deaths_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMBINED DAILY DATA IN combined_state_df() DATAFRAME\")\n",
    "print(combined_state_df[(combined_state_df['FIPS'] == MNFIPS) | (combined_state_df['FIPS'] == NDFIPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show demonstrations of plotting this data here by producing plots of data for Cass and Clay counties and North Dakota and Minnesota\n",
    "\n",
    "#\n",
    "# I will pull the data to plot into numpy arrays (notice I have to use [0] because it comes out at list of lists even for single row)\n",
    "#\n",
    "\n",
    "# County-level data for plotting\n",
    "dates_cty = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['Dates'].to_list()[0], dtype='datetime64')\n",
    "clay_deaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "clay_death_rate = (clay_deaths/combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['PopEst2019'].values)*100000\n",
    "cass_deaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "cass_death_rate = (cass_deaths/combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['PopEst2019'].values)*100000\n",
    "clay_confirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "clay_confirmed_rate = (clay_confirmed/combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['PopEst2019'].values)*100000\n",
    "cass_confirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "cass_confirmed_rate = (cass_confirmed/combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['PopEst2019'].values)*100000\n",
    "\n",
    "# Set up a figure of 2 x 2 plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15,10))\n",
    "\n",
    "# Plot up deaths and death rates as plots 0 and 1\n",
    "this_axs = axs[0, 0]  # Row 0, column 0\n",
    "this_axs.plot(dates_cty, clay_deaths, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_deaths, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Deaths\")\n",
    "\n",
    "this_axs = axs[1, 0]  # Row 1, column 0\n",
    "this_axs.plot(dates_cty, clay_death_rate, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_death_rate, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Deaths per 100,000 people\")\n",
    "\n",
    "# Plot up confirmed infections and infection rates as plots 2 and 3\n",
    "this_axs = axs[0, 1]  # Row 0, column 1\n",
    "this_axs.plot(dates_cty, clay_confirmed, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_confirmed, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections\")\n",
    "\n",
    "this_axs = axs[1, 1]  # Row 1, column 1\n",
    "this_axs.plot(dates_cty, clay_confirmed_rate, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_confirmed_rate, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections per 100,000 people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# I will pull the data to plot into numpy arrays (notice I have to use [0] because it comes out at list of lists even for single row)\n",
    "#\n",
    "\n",
    "# State-level data for plotting\n",
    "dates_state = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['Dates'].to_list()[0], dtype='datetime64')\n",
    "MN_deaths = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "MN_death_rate = (MN_deaths/combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['PopEst2019'].values)*100000\n",
    "ND_deaths = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['Deaths'].to_list()[0],dtype='int')\n",
    "ND_death_rate = (ND_deaths/combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['PopEst2019'].values)*100000\n",
    "MN_confirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "MN_confirmed_rate = (MN_confirmed/combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['PopEst2019'].values)*100000\n",
    "ND_confirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['Confirmed'].to_list()[0],dtype='int')\n",
    "ND_confirmed_rate = (ND_confirmed/combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['PopEst2019'].values)*100000\n",
    "\n",
    "# Set up a figure of 2 x 2 plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15,10))\n",
    "\n",
    "# Plot up deaths and death rates as plots 0 and 1\n",
    "this_axs = axs[0, 0]  # Row 0, column 0\n",
    "this_axs.plot(dates_state, MN_deaths, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_deaths, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Deaths\")\n",
    "\n",
    "this_axs = axs[1, 0]  # Row 1, column 0\n",
    "this_axs.plot(dates_state, MN_death_rate, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_death_rate, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Deaths per 100,000 people\")\n",
    "\n",
    "# Plot up confirmed infections and infection rates as plots 2 and 3\n",
    "this_axs = axs[0, 1]  # Row 0, column 1\n",
    "this_axs.plot(dates_state, MN_confirmed, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_confirmed, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Number\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections\")\n",
    "\n",
    "this_axs = axs[1, 1]  # Row 1, column 1\n",
    "this_axs.plot(dates_state, MN_confirmed_rate, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_confirmed_rate, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"Rate / 100000 people\")\n",
    "title = this_axs.set_title(\"COVID Confirmed Infections per 100,000 people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show demonstrations of plotting this data here by producing plots of data for Cass and Clay counties and North Dakota and Minnesota\n",
    "\n",
    "#\n",
    "# I will pull the data to plot into numpy arrays (notice I have to use [0] because it comes out at list of lists even for single row)\n",
    "#\n",
    "\n",
    "# County-level data for plotting\n",
    "dates_cty = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['Dates'].to_list()[0], dtype='datetime64')\n",
    "clay_ddeaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['dDeaths'].to_list()[0])\n",
    "cass_ddeaths = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['dDeaths'].to_list()[0])\n",
    "clay_dconfirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == ClayFIPS)]['dConfirmed'].to_list()[0])\n",
    "cass_dconfirmed = np.array(combined_cnty_df[(combined_cnty_df['FIPS'] == CassFIPS)]['dConfirmed'].to_list()[0])\n",
    "\n",
    "# State-level data for plotting\n",
    "dates_state = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['Dates'].to_list()[0], dtype='datetime64')\n",
    "MN_ddeaths = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['dDeaths'].to_list()[0])\n",
    "ND_ddeaths = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['dDeaths'].to_list()[0])\n",
    "MN_dconfirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == MNFIPS)]['dConfirmed'].to_list()[0])\n",
    "ND_dconfirmed = np.array(combined_state_df[(combined_state_df['FIPS'] == NDFIPS)]['dConfirmed'].to_list()[0])\n",
    "\n",
    "# Set up a figure of 2 x 2 plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot up the deriviates in the infection and death rates for counties\n",
    "this_axs = axs[0, 0]  # row 0, column 0\n",
    "this_axs.plot(dates_cty, clay_dconfirmed, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_dconfirmed, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Infections/Day\")\n",
    "title = this_axs.set_title(\"COVID Infection Change Rate\")\n",
    "\n",
    "this_axs = axs[0, 1]  # row 0, column 1\n",
    "this_axs.plot(dates_cty, clay_ddeaths, label='Clay County')\n",
    "this_axs.plot(dates_cty, cass_ddeaths, label='Cass County')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Deaths/Day\")\n",
    "title = this_axs.set_title(\"COVID Death Change Rate\")\n",
    "\n",
    "# Plot up the deriviates in the infection and death rates for states\n",
    "this_axs = axs[1, 0]  # row 1, column 0\n",
    "this_axs.plot(dates_state, MN_dconfirmed, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_dconfirmed, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Infections/Day\")\n",
    "title = this_axs.set_title(\"COVID Infection Change Rate\")\n",
    "\n",
    "this_axs = axs[1, 1]  # row 1, column 1\n",
    "this_axs.plot(dates_state, MN_ddeaths, label='Minnesota')\n",
    "this_axs.plot(dates_state, ND_ddeaths, label='North Dakota')\n",
    "legend = this_axs.legend()\n",
    "xlabel = this_axs.set_xlabel(\"Date\")\n",
    "ylabel = this_axs.set_ylabel(\"New Deaths/Day\")\n",
    "title = this_axs.set_title(\"COVID Death Change Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next few blocks of code is grabbing the Google and Apple mobility data and cross-matching with US Census Bureau FIPS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Mobility Data (NO FIPS Information Provided)\n",
    "\n",
    "This data is described at https://www.google.com/covid19/mobility/ and can be downloaded in a single monolithic CSV file at https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n",
    "\n",
    "> The data shows how visitors to (or time spent in) categorized places change compared to our baseline days. A baseline day represents a normal value for that day of the week. The baseline day is the median value from the 5â€‘week period Jan 3 â€“ Feb 6, 2020.\n",
    "\n",
    "> For each region-category, the baseline isnâ€™t a single valueâ€”itâ€™s 7 individual values. The same number of visitors on 2 different days of the week, result in different percentage changes. So, we recommend the following:\n",
    "1. Donâ€™t infer that larger changes mean more visitors or smaller changes mean less visitors.\n",
    "2. Avoid comparing day-to-day changes. Especially weekends with weekdays. (https://support.google.com/covid19-mobility/answer/9824897?hl=en&ref_topic=9822927)\n",
    "\n",
    "> Note, *Parks* typically means official national parks and not the general outdoors found in rural areas.\n",
    "\n",
    "Also, I'll note that aggregated national data appears to be available by setting `sub_region_1` **and** `sub_region_2` to `NaN` and state-level data by setting only `sub_region_2` to `NaN`.\n",
    "\n",
    "**Suggested Citation**: Google LLC \"Google COVID-19 Community Mobility Reports\". https://www.google.com/covid19/mobility/ Accessed: `<Date>.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Mobility Data URL\n",
    "goog_mobility_csv_url = \"https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\"\n",
    "goog_mobility_df=pd.read_csv(goog_mobility_csv_url, low_memory=False)\n",
    "\n",
    "## Separate data into national-level, state-level, and county-level data deep copies\n",
    "goog_mobility_national = goog_mobility_df[(goog_mobility_df['country_region_code'] == 'US') & (goog_mobility_df['sub_region_1'].isna()) & (goog_mobility_df['sub_region_2'].isna())].copy()\n",
    "goog_mobility_states = goog_mobility_df[(goog_mobility_df['country_region_code'] == 'US') & (goog_mobility_df['sub_region_1'].notna()) & (goog_mobility_df['sub_region_2'].isna())].copy()\n",
    "goog_mobility_cnty = goog_mobility_df[(goog_mobility_df['country_region_code'] == 'US') & (goog_mobility_df['sub_region_1'].notna()) & (goog_mobility_df['sub_region_2'].notna())].copy()\n",
    "\n",
    "# District of Columbia is both FIPS 11 and FIPS 110, so add its data to county-level mobility data\n",
    "dc_rows = goog_mobility_states[goog_mobility_states['sub_region_1'] == 'District of Columbia'].copy()\n",
    "dc_rows['sub_region_2'] = dc_rows['sub_region_1']\n",
    "goog_mobility_cnty = goog_mobility_cnty.append(dc_rows, ignore_index=True)\n",
    "\n",
    "# Notice for Clay county we have NaN reported for Parks (see note above) and Transit Stations\n",
    "goog_mobility_clay = goog_mobility_df[ (goog_mobility_df['sub_region_1'] == 'Minnesota') & (goog_mobility_df['sub_region_2'] == 'Clay County')]\n",
    "print(\"FIRST ROW OF GOOGLE MOBILITY DATA IN goog_mobility_df() FOR CLAY COUNTY\")\n",
    "print(goog_mobility_clay.iloc[[0]])\n",
    "\n",
    "# Undefine the clay county subframe\n",
    "del goog_mobility_clay\n",
    "del goog_mobility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Cross match known US Bureau Census FIPS entries with Google Mobility Data here to allow easier cross-matching later.\n",
    "##\n",
    "\n",
    "# Build a dataframe containing legitimate FIPS values using state and county level data\n",
    "state_fips_df = state_data_df.copy()\n",
    "state_fips_df.drop(columns=['POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "cnty_fips_df = county_data_df.copy()\n",
    "cnty_fips_df.drop(columns=['POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "\n",
    "## Match state-level mobility data to FIPS and then drop redundant columns and rename state name to be consistent.  \n",
    "goog_mobility_states_cleaned = pd.merge(state_fips_df,goog_mobility_states,left_on='STNAME', right_on='sub_region_1', how='left', copy=True)\n",
    "goog_mobility_states_cleaned.drop(columns=['country_region_code', 'country_region', 'sub_region_1', 'sub_region_2'], inplace=True)\n",
    "goog_mobility_states_cleaned.rename(columns={ 'STNAME': 'state'}, inplace = True)\n",
    "\n",
    "##\n",
    "## Match county-level mobility data to FIPS (trickier as it requires a state and county name match\n",
    "##\n",
    "\n",
    "# Comparing the unique county names in cnty_fips_df['CTYNAME_MATCH'] versus goog_mobility_cnty['sub_region_2'] reveals a lot of mismatching is due to \n",
    "# US Census bureau naming convention including \n",
    "#  ' city' (for cities that are also counties [40 cases]) [This allows matching of Baltimore city versus Baltimore county also]\n",
    "#  ' Municipality' (for cities that are also counties)\n",
    "#  ' Census Area' (for rural areas, I think)\n",
    "cnty_fips_df['CTYNAME_MATCH'] = cnty_fips_df['CTYNAME']\n",
    "cnty_fips_df['CTYNAME_MATCH'] = cnty_fips_df['CTYNAME_MATCH'].str.replace(' city','')\n",
    "cnty_fips_df['CTYNAME_MATCH'] = cnty_fips_df['CTYNAME_MATCH'].str.replace(' Municipality','')\n",
    "cnty_fips_df['CTYNAME_MATCH'] = cnty_fips_df['CTYNAME_MATCH'].str.replace(' Census Area','')\n",
    "cnty_fips_df['CTYNAME_MATCH'] = cnty_fips_df['CTYNAME_MATCH'].str.replace(' City and Borough','')\n",
    "cnty_fips_df['CTYNAME_MATCH'] = cnty_fips_df['CTYNAME_MATCH'].str.replace(' Borough','')\n",
    "\n",
    "goog_mobility_cnty['sub_region_2_MATCH'] = goog_mobility_cnty['sub_region_2']\n",
    "goog_mobility_cnty['sub_region_2_MATCH'] = goog_mobility_cnty['sub_region_2_MATCH'].str.replace(' Borough','')\n",
    "# Fix alternate spelling of LaSalle Parish, Louisiana\n",
    "goog_mobility_cnty['sub_region_2_MATCH'] = goog_mobility_cnty['sub_region_2_MATCH'].str.replace('La Salle Parish','LaSalle Parish')\n",
    "\n",
    "# A lot of rural areas just appear to have no data in the Google Mobility dataset. The next cell (commented out) was used to confirm the only missing counies\n",
    "# were ones in which there was 'nan' for the date column, indicating no matches in the Google Mobility dataset.\n",
    "\n",
    "## This leftward match means EVERY county FIPS should still be represented, although need to confirm mismatches\n",
    "goog_mobility_cnty_cleaned = pd.merge(cnty_fips_df,goog_mobility_cnty,left_on=['STNAME', 'CTYNAME_MATCH'], right_on=['sub_region_1', 'sub_region_2_MATCH'], how='left', copy=True)\n",
    "\n",
    "##\n",
    "## Check the date column in the reduced data to see if it is a real match or just a marker for a non-match\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a test, loop through all the county FIPS codes and see which are NOT represented in the mobility data\n",
    "unmatched_cnt = 0\n",
    "cleared_states = [] \n",
    "states_list = []\n",
    "last_state = 'Alaska' # Set to avoid issue in loop below with undefined variable\n",
    "\n",
    "fatal_error = 0. # Count fatal_error and stop if we have any\n",
    "cnt = 0\n",
    "bad_entries = \"\"\n",
    "for FIPS in cnty_fips_df['FIPS']:\n",
    "    # Check this FIPS number\n",
    "    rows = goog_mobility_cnty_cleaned[goog_mobility_cnty_cleaned['FIPS'] == FIPS]\n",
    "    matches = rows.shape[0]\n",
    "    city = rows['CTYNAME_MATCH'].iloc[0]\n",
    "    state = rows['STNAME'].iloc[0]\n",
    "    date = rows['date'].iloc[0]\n",
    "    if (matches == 1 & pd.isna(date)):\n",
    "        \n",
    "        if state not in states_list:\n",
    "            states_list.append(state)\n",
    "            \n",
    "        if ((state != last_state) & (last_state not in cleared_states)):\n",
    "            # Print counties lists\n",
    "            reduced_ctys = goog_mobility_cnty_cleaned[goog_mobility_cnty_cleaned['STNAME'] == last_state]['CTYNAME_MATCH'].unique()\n",
    "            mobility_ctys = goog_mobility_cnty[goog_mobility_cnty['sub_region_1'] == last_state]['sub_region_2'].unique()\n",
    "            \n",
    "            mismatch = len(reduced_ctys) - len(mobility_ctys)  # Number of missing counties\n",
    "            if (cnt != mismatch):\n",
    "                print(f\"\\nFor State {last_state}:\")\n",
    "                print(bad_entries)\n",
    "                print(f\"\\n  reduced counties: {reduced_ctys} {len(reduced_ctys)}\\n\")\n",
    "                print(f\"  mobility counties: {mobility_ctys} {len(mobility_ctys)}\\n\")\n",
    "                for cty in reduced_ctys:\n",
    "                    if cty not in mobility_ctys:\n",
    "                        print(f\"{cty} missing from Mobility counties\")\n",
    "\n",
    "                print(f\"WARNING: {cnt} no real matches vs. {mismatch} fewer counties in Google mobility data!\\n\")\n",
    "                fatal_error += 1\n",
    "                \n",
    "            cnt = 0 # Reset the count of mismatches in this state\n",
    "            bad_entries = \"\"\n",
    "        \n",
    "        if (state not in cleared_states):\n",
    "            cnt += 1\n",
    "            # Count this as a mismatch\n",
    "            bad_entries += f'{cnt}) {city}, {state} ({FIPS}) contains no real matches.\\n'\n",
    "        \n",
    "        last_state = state\n",
    "        unmatched_cnt += 1\n",
    "\n",
    "# Print counties for last state considered=\n",
    "reduced_ctys = goog_mobility_cnty_cleaned[goog_mobility_cnty_cleaned['STNAME'] == last_state]['CTYNAME_MATCH'].unique()\n",
    "mobility_ctys = goog_mobility_cnty[goog_mobility_cnty['sub_region_1'] == last_state]['sub_region_2'].unique()\n",
    "mismatch = len(reduced_ctys) - len(mobility_ctys)  # Number of missing counties\n",
    "if (cnt != mismatch):\n",
    "    print(f\"\\nFor State {last_state}:\")\n",
    "    print(bad_entries)\n",
    "    print(f\"\\n  reduced counties: {reduced_ctys} {len(reduced_ctys)}\\n\")\n",
    "    print(f\"  mobility counties: {mobility_ctys} {len(mobility_ctys)}\\n\")\n",
    "    for cty in reduced_ctys:\n",
    "        if cty not in mobility_ctys:\n",
    "            print(f\"{cty} missing from Mobility counties\")\n",
    "    print(f\"WARNING: {cnt} no real matches vs. {mismatch} fewer counties in Google mobility data!\\n\")\n",
    "    fatal_error += 1\n",
    "        \n",
    "print(f\"A total of {unmatched_cnt} FIPS not matched to Google mobility data (if nothing printed above this, all US Census Bureau counties accounted for)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Add conversion of separate dataframe rows as dates into a single row per location with time series stored as lists\n",
    "## For the state data\n",
    "##\n",
    "goog_mobility_states_reduced = state_fips_df.copy()\n",
    "\n",
    "# Create blank lists of lists\n",
    "dates_listOlists = []\n",
    "retail_listOlists = []\n",
    "grocery_listOlists = []\n",
    "parks_listOlists = []\n",
    "transit_listOlists = []\n",
    "workplaces_listOlists = []\n",
    "residential_listOlists = []\n",
    "    \n",
    "for fips in state_fips_df['FIPS']:\n",
    "    #print(f\"Processing FIPS {fips}\")\n",
    "    # Pull only the data for this FIPS number and extract the time series\n",
    "    subset = goog_mobility_states_cleaned[goog_mobility_states_cleaned['FIPS'] == fips].copy()\n",
    "    timeseries = subset[subset.columns[(subset.columns!='FIPS') & (subset.columns!='state')]].copy()\n",
    "    timeseries = timeseries.set_index('date')\n",
    "    trans = timeseries.T\n",
    "\n",
    "    # Convert the time series into lists in memory\n",
    "    dates_list = trans[ trans.columns[(trans.columns!='date')]].columns.tolist()\n",
    "    retail_list = trans.loc['retail_and_recreation_percent_change_from_baseline'].values.tolist()\n",
    "    grocery_list = trans.loc['grocery_and_pharmacy_percent_change_from_baseline'].values.tolist()\n",
    "    parks_list = trans.loc['parks_percent_change_from_baseline'].values.tolist()\n",
    "    transit_list = trans.loc['transit_stations_percent_change_from_baseline'].values.tolist()\n",
    "    workplaces_list = trans.loc['workplaces_percent_change_from_baseline'].values.tolist()\n",
    "    residential_list = trans.loc['residential_percent_change_from_baseline'].values.tolist()\n",
    "    \n",
    "    # Add lists to lists\n",
    "    dates_listOlists.append(dates_list)\n",
    "    retail_listOlists.append(retail_list)\n",
    "    grocery_listOlists.append(grocery_list)\n",
    "    parks_listOlists.append(transit_list)\n",
    "    transit_listOlists.append(dates_list)\n",
    "    workplaces_listOlists.append(workplaces_list)\n",
    "    residential_listOlists.append(residential_list)\n",
    "    \n",
    "# Results in error ValueError: Length of values does not match length of index\n",
    "goog_mobility_states_reduced['dates'] = dates_listOlists\n",
    "goog_mobility_states_reduced['retail_and_recreation_percent_change_from_baseline'] = retail_listOlists\n",
    "goog_mobility_states_reduced['grocery_and_pharmacy_percent_change_from_baseline'] = grocery_listOlists\n",
    "goog_mobility_states_reduced['parks_percent_change_from_baseline'] = parks_listOlists\n",
    "goog_mobility_states_reduced['transit_stations_percent_change_from_baseline'] = transit_listOlists\n",
    "goog_mobility_states_reduced['workplaces_percent_change_from_baseline'] = workplaces_listOlists\n",
    "goog_mobility_states_reduced['residential_percent_change_from_baseline'] = residential_listOlists\n",
    "\n",
    "# Rename STNAME column to state\n",
    "goog_mobility_states_reduced.rename(columns={'STNAME': 'state'}, errors=\"raise\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Add conversion of separate dataframe rows as dates into a single row per location with time series stored as lists\n",
    "## For the county data\n",
    "##\n",
    "goog_mobility_cnty_reduced = cnty_fips_df.copy()\n",
    "\n",
    "# Create blank lists of lists\n",
    "dates_listOlists = []\n",
    "retail_listOlists = []\n",
    "grocery_listOlists = []\n",
    "parks_listOlists = []\n",
    "transit_listOlists = []\n",
    "workplaces_listOlists = []\n",
    "residential_listOlists = []\n",
    "    \n",
    "for fips in cnty_fips_df['FIPS']:\n",
    "    # print(f\"Processing FIPS {fips}\")\n",
    "    # Pull only the data for this FIPS number and extract the time series\n",
    "    # Convert fips to int64 to avoid numpy warning\n",
    "    fips64 = np.int64(fips)\n",
    "    subset = goog_mobility_cnty_cleaned[goog_mobility_cnty_cleaned['FIPS'] == fips64].copy()\n",
    "    timeseries = subset[subset.columns[(subset.columns!='FIPS') & (subset.columns!='state')]].copy()\n",
    "    timeseries = timeseries.set_index('date')\n",
    "    trans = timeseries.T\n",
    "\n",
    "    # Convert the time series into lists in memory\n",
    "    dates_list = trans[ trans.columns[(trans.columns!='date')]].columns.tolist()\n",
    "    retail_list = trans.loc['retail_and_recreation_percent_change_from_baseline'].values.tolist()\n",
    "    grocery_list = trans.loc['grocery_and_pharmacy_percent_change_from_baseline'].values.tolist()\n",
    "    parks_list = trans.loc['parks_percent_change_from_baseline'].values.tolist()\n",
    "    transit_list = trans.loc['transit_stations_percent_change_from_baseline'].values.tolist()\n",
    "    workplaces_list = trans.loc['workplaces_percent_change_from_baseline'].values.tolist()\n",
    "    residential_list = trans.loc['residential_percent_change_from_baseline'].values.tolist()\n",
    "    \n",
    "    # Add lists to lists\n",
    "    dates_listOlists.append(dates_list)\n",
    "    retail_listOlists.append(retail_list)\n",
    "    grocery_listOlists.append(grocery_list)\n",
    "    parks_listOlists.append(transit_list)\n",
    "    transit_listOlists.append(dates_list)\n",
    "    workplaces_listOlists.append(workplaces_list)\n",
    "    residential_listOlists.append(residential_list)\n",
    "    \n",
    "# Results in error ValueError: Length of values does not match length of index\n",
    "goog_mobility_cnty_reduced['dates'] = dates_listOlists\n",
    "goog_mobility_cnty_reduced['retail_and_recreation_percent_change_from_baseline'] = retail_listOlists\n",
    "goog_mobility_cnty_reduced['grocery_and_pharmacy_percent_change_from_baseline'] = grocery_listOlists\n",
    "goog_mobility_cnty_reduced['parks_percent_change_from_baseline'] = parks_listOlists\n",
    "goog_mobility_cnty_reduced['transit_stations_percent_change_from_baseline'] = transit_listOlists\n",
    "goog_mobility_cnty_reduced['workplaces_percent_change_from_baseline'] = workplaces_listOlists\n",
    "goog_mobility_cnty_reduced['residential_percent_change_from_baseline'] = residential_listOlists\n",
    "\n",
    "# Rename STNAME and CTYNAME columns to state and county, drop redundant columns\n",
    "goog_mobility_cnty_reduced.rename(columns={'STNAME': 'state', 'CTYNAME': 'county'}, errors=\"raise\", inplace=True)\n",
    "goog_mobility_cnty_reduced.drop(columns=['CTYNAME_MATCH'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIPS-matched Google Mobility data exported here!\n",
    "\n",
    "**Note to Developers:** Check the `date` column in the reduced data to see if it is a real match or just a marker for a non-match.  Furthermore be away Google has a lot of blank (`NaN`) entries in a lot of columns and variable numbers of entries for each county/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data has been checked, remove redundant columns and export to CSV for quick importing\n",
    "if (fatal_error == 0):\n",
    "    print(\"Exporting data, no fatal errors in matching\")\n",
    "    \n",
    "    goog_mobility_states_fname = data_dir + \"goog_mobility_state.csv\"\n",
    "    print(\" - Google state level mobility data exported to \", goog_mobility_states_fname)\n",
    "    goog_mobility_states_reduced.to_csv(goog_mobility_states_fname, index=False)\n",
    "    \n",
    "    goog_mobility_cnty_fname = data_dir + \"goog_mobility_cnty.csv\"\n",
    "    print(\" - Google county level mobility data exported to \", goog_mobility_cnty_fname)\n",
    "    goog_mobility_cnty_reduced.to_csv(goog_mobility_cnty_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Mobility Data (NO FIPS Information Provided)\n",
    "\n",
    "This data is described at https://www.apple.com/covid19/mobility and can be downloaded in a single monolithic CSV file at https://covid19-static.cdn-apple.com/covid19-mobility-data/2008HotfixDev42/v3/en-us/applemobilitytrends-2020-05-24.csv (That URL is hidden in the mobility page link and appears to be updated regularly.  We may need to scrape the page to identify the link).\n",
    "\n",
    "**About this Data (copied from Apple's site)**: The CSV file on this site show a relative volume of directions requests per country/region, sub-region or city compared to a baseline volume on January 13th, 2020. We define our day as midnight-to-midnight, Pacific time. Cities are defined as the greater metropolitan area and their geographic boundaries remain constant across the data set. In many countries/regions, sub-regions, and cities, relative volume has increased since January 13th, consistent with normal, seasonal usage of Apple Maps. Day of week effects are important to normalize as you use this data. Data that is sent from usersâ€™ devices to the Maps service is associated with random, rotating identifiers so Apple doesnâ€™t have a profile of individual movements and searches. Apple Maps has no demographic information about our users, so we canâ€™t make any statements about the representativeness of usage against the overall population.\n",
    "\n",
    "Apple tracks three kinds of Apple Maps routing requests: Driving, Walking, Transit.  But the only data available at the state and county level is the Driving data.\n",
    "\n",
    "**Developer Notes**: Apple's mobility data only exists for 2090 out of 3142 counties in the US. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the original Apple page was proving tricky as it had a bunch of javascript used to generate the URL, so I poked around and found a reference \n",
    "# at https://www.r-bloggers.com/get-apples-mobility-data/ to a JSON file at a stable URL that can be used to construct the appropriate URL for the current\n",
    "# datafile.\n",
    "\n",
    "aapl_mobility_json = \"https://covid19-static.cdn-apple.com/covid19-mobility-data/current/v3/index.json\"\n",
    "aapl_server = \"https://covid19-static.cdn-apple.com/\"\n",
    "result = requests.get(aapl_mobility_json)\n",
    "# Proceed if we successfully pulled the page (HTTP status code 200)\n",
    "if (result.status_code == 200):\n",
    "    # Apple Mobility Data URL\n",
    "    jsondata = result.json()\n",
    "    aapl_mobility_csv_url = aapl_server+jsondata['basePath']+jsondata['regions']['en-us']['csvPath']\n",
    "    aapl_mobility_df=pd.read_csv(aapl_mobility_csv_url, low_memory=False)\n",
    "    \n",
    "# There are four 'geo_types' (aapl_mobility_df['geo_type' == ].unique() returns ['country/region', 'city', 'sub-region', 'county'])\n",
    "# Checking those types here\n",
    "\n",
    "# Get Washington DC Data\n",
    "dc_entry = aapl_mobility_df[(aapl_mobility_df['country'] == 'United States') \n",
    "                            & (aapl_mobility_df['region'] =='Washington DC') \n",
    "                            & (aapl_mobility_df['transportation_type'] =='driving')].copy()\n",
    "dc_entry['region'] = 'District of Columbia'\n",
    "dc_entry['sub-region'] = 'District of Columbia'\n",
    "\n",
    "# Get state-level mobility data from Apple (only contains 'transportation_type' of 'driving')\n",
    "aapl_mobility_states = aapl_mobility_df[(aapl_mobility_df['geo_type'] == 'sub-region') & (aapl_mobility_df['country'] == 'United States') ].copy()\n",
    "# Append DC data to state-level data\n",
    "aapl_mobility_states = aapl_mobility_states.append(dc_entry, ignore_index=True)\n",
    "# Remove redundant columns and rename columns to be more precise\n",
    "aapl_mobility_states.drop(columns=['country', 'geo_type', 'sub-region'], inplace=True) \n",
    "aapl_mobility_states.rename(columns={ 'region': 'state'}, inplace = True)\n",
    "state_transport = aapl_mobility_states['transportation_type'].unique().tolist()\n",
    "#print(\"Apple mobility data at state level transportation types: \"+\",\".join(state_transport)+\"\\n\" )\n",
    "# Assuming there is still only a 'driving' transportation type, drop those redundant columns\n",
    "if (len(state_transport) == 1):\n",
    "    aapl_mobility_states.drop(columns=['transportation_type', 'alternative_name'], inplace=True)\n",
    "# Purge territories\n",
    "aapl_mobility_states = aapl_mobility_states[aapl_mobility_states.state != 'Guam'].copy()\n",
    "aapl_mobility_states = aapl_mobility_states[aapl_mobility_states.state != 'Puerto Rico'].copy()\n",
    "aapl_mobility_states = aapl_mobility_states[aapl_mobility_states.state != 'Virgin Islands'].copy()\n",
    "\n",
    "# Get county-level mobility data from Apple\n",
    "aapl_mobility_cnty = aapl_mobility_df[(aapl_mobility_df['geo_type'] == 'county') & (aapl_mobility_df['country'] == 'United States')].copy()\n",
    "aapl_mobility_cnty.drop(columns=['country', 'geo_type'], inplace=True) \n",
    "aapl_mobility_cnty.rename(columns={ 'sub-region': 'state', 'region': 'county'}, inplace = True)\n",
    "cnty_transport = aapl_mobility_cnty['transportation_type'].unique().tolist()\n",
    "#print(\"Apple mobility data at county level transportation types: \"+\",\".join(state_transport)+\"\\n\" )\n",
    "# Assuming there is still only a 'driving' transportation type, drop those redundant columns\n",
    "if (len(cnty_transport) == 1):\n",
    "    aapl_mobility_cnty.drop(columns=['transportation_type', 'alternative_name'], inplace=True)    \n",
    "\n",
    "# Purge complete Apple mobility dataframe once subsets built\n",
    "#del aapl_mobility_df\n",
    "\n",
    "# Notice only driving information is available at the county level here\n",
    "print(\"APPLE MOBILITY DATA IN aapl_mobility_clay() FOR CLAY COUNTY\")\n",
    "aapl_mobility_clay = aapl_mobility_cnty[(aapl_mobility_cnty['county'] == 'Clay County') & (aapl_mobility_cnty['state'] == 'Minnesota')]\n",
    "print(aapl_mobility_clay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Cross-match known US Bureau Census FIPS entries with Apple Mobility Data here to allow easier cross-matching later.\n",
    "##\n",
    "\n",
    "# Build a dataframe containing legitimate FIPS values using state and county level data\n",
    "state_fips_df = state_data_df.copy()\n",
    "state_fips_df.drop(columns=['POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "cnty_fips_df = county_data_df.copy()\n",
    "cnty_fips_df.drop(columns=['POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "\n",
    "##\n",
    "## Match state-level mobility data to FIPS and then drop redundant columns and rename state name to be consistent.  \n",
    "##\n",
    "aapl_mobility_states_cleaned = pd.merge(state_fips_df,aapl_mobility_states,left_on='STNAME', right_on='state', how='left', copy=True)\n",
    "aapl_mobility_states_cleaned.drop(columns=['STNAME'], inplace=True)\n",
    "\n",
    "# Convert all the mobility data into one massive list of lists (and columns into dates list), this will allow collapsing multiple columns into lists\n",
    "dates_list = aapl_mobility_states_cleaned[ aapl_mobility_states_cleaned.columns[(aapl_mobility_states_cleaned.columns!='FIPS') & (aapl_mobility_states_cleaned.columns!='state')] ].columns.tolist()\n",
    "driving_mobility_listOlists = aapl_mobility_states_cleaned[ aapl_mobility_states_cleaned.columns[(aapl_mobility_states_cleaned.columns!='FIPS') & (aapl_mobility_states_cleaned.columns!='state')] ].values.tolist()\n",
    "\n",
    "# Create reduced mobility data file with all the data collapsed into lists\n",
    "aapl_mobility_states_reduced = aapl_mobility_states_cleaned[['FIPS','state']].copy()\n",
    "aapl_mobility_states_reduced['dates'] = [dates_list]*len(aapl_mobility_states_reduced)\n",
    "aapl_mobility_states_reduced['driving_mobility'] = driving_mobility_listOlists\n",
    "\n",
    "##\n",
    "## Match county-level mobility data to FIPS \n",
    "##\n",
    "# Rename county names which are actually cities\n",
    "aapl_mobility_cnty['county'] = aapl_mobility_cnty['county'].str.replace(' City',' city')\n",
    "aapl_mobility_cnty['county'] = aapl_mobility_cnty['county'].str.replace(' city and Borough',' City and Borough')\n",
    "aapl_mobility_cnty['county'] = aapl_mobility_cnty['county'].str.replace('Carson city','Carson City')\n",
    "aapl_mobility_cnty['county'] = aapl_mobility_cnty['county'].str.replace('James city County','James City County')\n",
    "\n",
    "# Attempt the match\n",
    "aapl_mobility_cnty_cleaned = pd.merge(cnty_fips_df,aapl_mobility_cnty,left_on=['STNAME', 'CTYNAME'], right_on=['state', 'county'], how='left', copy=True)\n",
    "\n",
    "print(f\"There are {len(cnty_fips_df)} counties, Apple mobility data exists for {len(aapl_mobility_cnty)} counties.\")\n",
    "expected = len(cnty_fips_df)-len(aapl_mobility_cnty)\n",
    "nomatch = len(aapl_mobility_cnty_cleaned[aapl_mobility_cnty_cleaned['state'].isna()])\n",
    "n_errors = nomatch - expected\n",
    "print(f\"When cross-matching, there are {nomatch} Census counties with no match, expected {expected} [We need to account for {n_errors} errors].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = []\n",
    "cleared_states = []\n",
    "last_state = \"Alabama\"\n",
    "fatal_error = 0\n",
    "bad_entries = \"\"\n",
    "cnt = 0\n",
    "unmatched_cnt = 0\n",
    "\n",
    "for FIPS in cnty_fips_df['FIPS']:\n",
    "    # Check this FIPS number\n",
    "    row  = aapl_mobility_cnty_cleaned[aapl_mobility_cnty_cleaned['FIPS'] == FIPS]\n",
    "    city = row['CTYNAME'].iloc[0]\n",
    "    state = row['STNAME'].iloc[0]\n",
    "    aapl_state= row['state'].iloc[0]\n",
    "    if (pd.isna(aapl_state)):\n",
    "        \n",
    "        if state not in states_list:\n",
    "            states_list.append(state)\n",
    "            \n",
    "        if ((state != last_state) & (last_state not in cleared_states)):\n",
    "            # Check on number of counties matching on per state basis\n",
    "            reduced_ctys = aapl_mobility_cnty_cleaned[aapl_mobility_cnty_cleaned['STNAME'] == last_state]['CTYNAME'].unique()\n",
    "            mobility_ctys = aapl_mobility_cnty[aapl_mobility_cnty['state'] == last_state]['county'].unique()\n",
    "            mismatch = len(reduced_ctys) - len(mobility_ctys)\n",
    "            \n",
    "            if (cnt != mismatch):\n",
    "                print(f\"\\nFor State {last_state}:\")\n",
    "                print(bad_entries)\n",
    "                print(f\"\\n  reduced counties: {reduced_ctys} {len(reduced_ctys)}\\n\")\n",
    "                print(f\"  mobility counties: {mobility_ctys} {len(mobility_ctys)}\\n\")\n",
    "                \n",
    "                print(f\"WARNING: {cnt} not matched vs. {mismatch} fewer counties in Apple mobility data!\\n\")\n",
    "                fatal_error += 1\n",
    "            \n",
    "            cnt = 0 # Reset the count of mismatches in this state\n",
    "            bad_entries = \"\"\n",
    "        \n",
    "        if (state not in cleared_states):\n",
    "            cnt += 1\n",
    "            # Count this as a mismatch\n",
    "            bad_entries += f'{cnt}) {city}, {state} ({FIPS}) contains no real matches.\\n'\n",
    "        \n",
    "        last_state = state\n",
    "        unmatched_cnt += 1\n",
    "\n",
    "# Check lists of counties\n",
    "reduced_ctys = aapl_mobility_cnty_cleaned[aapl_mobility_cnty_cleaned['STNAME'] == last_state]['CTYNAME'].unique()\n",
    "mobility_ctys = aapl_mobility_cnty[aapl_mobility_cnty['state'] == last_state]['county'].unique()\n",
    "mismatch = len(reduced_ctys) - len(mobility_ctys)\n",
    "\n",
    "if (cnt != mismatch):\n",
    "    print(f\"\\nFor State {last_state}:\")\n",
    "    print(bad_entries)\n",
    "    print(f\"\\n  reduced counties: {reduced_ctys} {len(reduced_ctys)}\\n\")\n",
    "    print(f\"  mobility counties: {mobility_ctys} {len(mobility_ctys)}\\n\")\n",
    "\n",
    "    print(f\"WARNING: {cnt} not matched vs. {mismatch} fewer counties in Apple mobility data!\\n\")\n",
    "    fatal_error += 1\n",
    "print(f\"A total of {unmatched_cnt} FIPS not matched to Apple mobility data (if nothing printed above this, all US Census Bureau counties accounted for)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Process county level data if tests pass\n",
    "##\n",
    "if (fatal_error == 0):\n",
    "    # Purge Redundant Columns\n",
    "    aapl_mobility_cnty_cleaned.drop(columns=['county', 'state'], inplace=True)\n",
    "    aapl_mobility_cnty_cleaned.rename(columns={ 'STNAME': 'state', 'CTYNAME': 'county'}, inplace = True)\n",
    "    \n",
    "    # Convert all the county level mobility data into one massive list of lists (and columns into dates list), this will allow collapsing multiple columns into lists\n",
    "    dates_list = aapl_mobility_cnty_cleaned[ aapl_mobility_cnty_cleaned.columns[(aapl_mobility_cnty_cleaned.columns!='FIPS') & (aapl_mobility_cnty_cleaned.columns!='state') & (aapl_mobility_cnty_cleaned.columns!='county')] ].columns.tolist()\n",
    "    driving_mobility_listOlists = aapl_mobility_cnty_cleaned[ aapl_mobility_cnty_cleaned.columns[(aapl_mobility_cnty_cleaned.columns!='FIPS') & (aapl_mobility_cnty_cleaned.columns!='state')& (aapl_mobility_cnty_cleaned.columns!='county')] ].values.tolist()\n",
    "\n",
    "    # Create reduced mobility data file with all the data collapsed into lists \n",
    "    # Did this goofy line next becayse 'county' wasn't being recognized with shorthand approach.  WTF?\n",
    "    aapl_mobility_cnty_reduced = aapl_mobility_cnty_cleaned[ aapl_mobility_cnty_cleaned.columns[(aapl_mobility_cnty_cleaned.columns=='FIPS') | (aapl_mobility_cnty_cleaned.columns=='state') | (aapl_mobility_cnty_cleaned.columns=='county')] ].copy()\n",
    "    aapl_mobility_cnty_reduced['dates'] = [dates_list]*len(aapl_mobility_cnty_reduced)\n",
    "    aapl_mobility_cnty_reduced['driving_mobility'] = driving_mobility_listOlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data has been checked, remove redundant columns and export to CSV for quick importing\n",
    "if (fatal_error == 0):\n",
    "    print(\"Exporting data, no fatal errors in matching\")\n",
    "    \n",
    "    aapl_mobility_states_fname = data_dir + \"aapl_mobility_state.csv\"\n",
    "    print(\" - Apple state level mobility data exported to \", aapl_mobility_states_fname)\n",
    "    aapl_mobility_states_reduced.to_csv(aapl_mobility_states_fname, index=False)\n",
    "    \n",
    "    aapl_mobility_cnty_fname = data_dir + \"aapl_mobility_cnty.csv\"\n",
    "    print(\" - Apple county level mobility data exported to \", aapl_mobility_cnty_fname)\n",
    "    aapl_mobility_cnty_reduced.to_csv(aapl_mobility_cnty_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIPS-matched Apple Mobility data exported here!\n",
    "\n",
    "**Note to Developers:** Apple has fewer blank (`NaN`) entries when a county was included, but there are 1032 counties with no published data which are in this expoerted file as `NaN` for both dates and driving mobility information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Institute for Health Metrics and Evaluation (IMHE) Data on Local Resources (NO FIPS Information Provided)\n",
    "\n",
    "There is Institute for Health Metrics and Evaluation data on local resources at http://www.healthdata.org/covid/data-downloads although data only has state level resolution. \n",
    "\n",
    "**Suggested Citation**: Institute for Health Metrics and Evaluation (IHME). COVID-19 Hospital Needs and Death Projections. Seattle, United States of America: Institute for Health Metrics and Evaluation (IHME), University of Washington, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Retrieve the IMHE data which is in the form of a ZIP file\n",
    "##\n",
    "\n",
    "# I modeled the extraction of the ZIP data downloaded from a URL without writing to disk on examples found at\n",
    "# https://stackoverflow.com/questions/5710867/downloading-and-unzipping-a-zip-file-without-writing-to-disk\n",
    "\n",
    "# Retrieve the ZIP file into memory and get the filelist\n",
    "imhe_url = \"https://ihmecovid19storage.blob.core.windows.net/latest/ihme-covid19.zip\"\n",
    "imhe_result = requests.get(imhe_url).content\n",
    "zipfile = ZipFile(BytesIO(imhe_result))\n",
    "filelist = zipfile.namelist()\n",
    "\n",
    "# The challenge is that the files in the ZIP file are placed in a date based directory, so perform a search for the proper strings in the filenames\n",
    "summary_csv = [name for name in filelist if \"Summary_stats_all_locs\" in name][0]\n",
    "hospitalization_csv = [name for name in filelist if \"Hospital\" in name][0]\n",
    "\n",
    "# Get the CSV data into pandas dataframes\n",
    "imhe_summary_df=pd.read_csv(zipfile.open(summary_csv), low_memory=False)\n",
    "imhe_hospitalizations_df=pd.read_csv(zipfile.open(hospitalization_csv), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Summary data processing\n",
    "##\n",
    "\n",
    "## Summary data includes numbers or dates for the following for each state\n",
    "#             'peak_bed_day_mean', 'peak_bed_day_lower', 'peak_bed_day_upper': Mean/Lower/Upper Uncertainty peak bed use date\n",
    "# 'peak_icu_bed_day_mean', 'peak_icu_bed_day_lower', 'peak_icu_bed_day_upper': Mean/Lower/Upper Uncertainty ICU bed use date\n",
    "#          'peak_vent_day_mean', 'peak_vent_day_lower', 'peak_vent_day_upper': Mean/Lower/Upper Uncertainty Ventilator use date\n",
    "#    'all_bed_capacity', 'icu_bed_capacity', 'all_bed_usage', 'icu_bed_usage': Number of beds/ICU beds/avg beds used/avg ICU beds used\n",
    "#                          'travel_limit_start_date', 'travel_limit_end_date': Severe travel restrictions start/end dates\n",
    "#                                'stay_home_start_date', 'stay_home_end_date': Stay at home order start/end dates\n",
    "#                    'educational_fac_start_date', 'educational_fac_end_date': Educational facilities closure start/end dates\n",
    "#      'any_gathering_restrict_start_date', 'any_gathering_restrict_end_date': Any gathering restrictions start/end dates\n",
    "#                          'any_business_start_date', 'any_business_end_date': Any business closures start/end dates\n",
    "#          'all_non-ess_business_start_date', 'all_non-ess_business_end_date': Non-essential businesses ordered to close start/end dates\n",
    "#\n",
    "# 'NaN' present for dates means it isn't known.\n",
    "\n",
    "# Match to FIPS data\n",
    "state_fips_df = state_data_df.copy()\n",
    "state_fips_df.drop(columns=['POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "imhe_summary_cleaned = pd.merge(state_fips_df,imhe_summary_df,left_on='STNAME', right_on='location_name', how='left', copy=True)\n",
    "\n",
    "# Dropped redundant state name and excess capacity of beds, since computable from available columns\n",
    "imhe_summary_cleaned.drop(columns=['STNAME', 'available_all_nbr', 'available_icu_nbr'], inplace=True)\n",
    "imhe_summary_cleaned.rename(columns={ 'location_name': 'state' }, inplace = True)\n",
    "\n",
    "# Write out file to disk\n",
    "imhe_summary_fname = data_dir + \"imhe_summary.csv\"\n",
    "print(\" - IMHE state level summary data exported to \", imhe_summary_fname)\n",
    "imhe_summary_cleaned.to_csv(imhe_summary_fname, index=False)\n",
    "\n",
    "# Present summary data for local area\n",
    "print(\"\\nIMHE SUMMARY DATA IN imhe_summary_cleaned() FOR MN and ND\")\n",
    "imhe_summary_local = imhe_summary_cleaned[(imhe_summary_cleaned.FIPS == MNFIPS) | (imhe_summary_cleaned.FIPS == NDFIPS) ]\n",
    "print(imhe_summary_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Hospitalization data processing\n",
    "##\n",
    "\n",
    "## Hospitalization data is time series date for the following projections by the IMHE:\n",
    "#                             'allbed_mean', 'allbed_lower','allbed_upper': Predicted COVID beds needed with upper/lower bounds\n",
    "#                            'ICUbed_mean', 'ICUbed_lower', 'ICUbed_upper': Predicted COVID ICU beds needed with upper/lower bounds\n",
    "#                            'InvVen_mean', 'InvVen_lower', 'InvVen_upper': Predicted COVID ventilators needed with upper/lower bounds\n",
    "#                            'deaths_mean', 'deaths_lower', 'deaths_upper': Predicted COVID daily deaths with upper/lower bounds\n",
    "#                               'admis_mean', 'admis_lower', 'admis_upper': Predicted hospital admissions with upper/lower bounds\n",
    "#                            'newICU_mean', 'newICU_lower', 'newICU_upper': Predicted new ICU admissions per day with upper/lower bounds\n",
    "#                            'totdea_mean', 'totdea_lower', 'totdea_upper': Predicted COVID cumilative deaths with upper/lower bounds\n",
    "# 'deaths_mean_smoothed', 'deaths_lower_smoothed', 'deaths_upper_smoothed': Smoothed version of predicted COVID daily deaths\n",
    "# 'totdea_mean_smoothed', 'totdea_lower_smoothed', 'totdea_upper_smoothed': Smoothed version of cumilative COVID deaths\n",
    "#                                   'total_tests_data_type', 'total_tests': observed/predicted tests and total number of tests\n",
    "#                                                   'confirmed_infections': Observed confirmed infections only\n",
    "#    'est_infections_mean', 'est_infections_lower', 'est_infections_upper': Predicted estimated infections with upper/lower bounds\n",
    "#\n",
    "# 'NaN' present for dates means it isn't known.\n",
    "\n",
    "# Match to FIPS data\n",
    "imhe_hospitalizations_cleaned = pd.merge(state_fips_df,imhe_hospitalizations_df,left_on='STNAME', right_on='location_name', how='left', copy=True)\n",
    "\n",
    "# Dropped redundant state name columns (and data on mobility, and bed overuse since its computable from other data )\n",
    "imhe_hospitalizations_cleaned.drop(columns=['STNAME', 'V1', 'bedover_mean', 'bedover_lower', 'bedover_upper', \n",
    "                                            'icuover_mean', 'icuover_lower', 'icuover_upper', \n",
    "                                            'mobility_data_type', 'mobility_composite' ], inplace=True)\n",
    "imhe_hospitalizations_cleaned.rename(columns={ 'location_name': 'state' }, inplace = True)\n",
    "\n",
    "##\n",
    "## Add conversion of separate dataframe rows as dates into a single row per location with time series stored as lists\n",
    "## For the county data\n",
    "##\n",
    "imhe_hospitalizations_reduced = state_fips_df.copy()\n",
    "imhe_hospitalizations_reduced.rename(columns={ 'STNAME': 'state' }, inplace = True)\n",
    "\n",
    "# Create blank lists of lists\n",
    "dates_listOlists = []\n",
    "allbed_mean_listOlists = []\n",
    "allbed_lower_listOlists = []\n",
    "allbed_upper_listOlists = []\n",
    "ICUbed_mean_listOlists = []\n",
    "ICUbed_lower_listOlists = []\n",
    "ICUbed_upper_listOlists = []\n",
    "InvVen_mean_listOlists = []\n",
    "InvVen_lower_listOlists = []\n",
    "InvVen_upper_listOlists = []\n",
    "deaths_mean_listOlists = []\n",
    "deaths_lower_listOlists = []\n",
    "deaths_upper_listOlists = []\n",
    "admis_mean_listOlists = []\n",
    "admis_lower_listOlists = []\n",
    "admis_upper_listOlists = []\n",
    "newICU_mean_listOlists = []\n",
    "newICU_lower_listOlists = []\n",
    "newICU_upper_listOlists = []\n",
    "totdea_mean_listOlists = []\n",
    "totdea_lower_listOlists = []\n",
    "totdea_upper_listOlists = []\n",
    "deaths_mean_smoothed_listOlists = []\n",
    "deaths_lower_smoothed_listOlists = []\n",
    "deaths_upper_smoothed_listOlists = []\n",
    "totdea_mean_smoothed_listOlists = []\n",
    "totdea_lower_smoothed_listOlists = []\n",
    "totdea_upper_smoothed_listOlists = []\n",
    "total_tests_data_type_listOlists = []\n",
    "total_tests_listOlists = []\n",
    "confirmed_infections_listOlists = []\n",
    "est_infections_mean_listOlists = []\n",
    "est_infections_lower_listOlists = []\n",
    "est_infections_upper_listOlists = []\n",
    "    \n",
    "for fips in state_fips_df['FIPS']:\n",
    "    # Pull only the data for this FIPS number and extract the time series\n",
    "    subset = imhe_hospitalizations_cleaned[imhe_hospitalizations_cleaned['FIPS'] == fips].copy()\n",
    "    timeseries = subset[subset.columns[(subset.columns!='FIPS') & (subset.columns!='state')   ]].copy()\n",
    "    timeseries = timeseries.set_index('date')\n",
    "    trans = timeseries.T\n",
    "\n",
    "    # Convert the time series into lists in memory\n",
    "    dates_list = trans[ trans.columns[(trans.columns!='date')]].columns.tolist()\n",
    "    allbed_mean_list = trans.loc['allbed_mean'].values.tolist()\n",
    "    allbed_lower_list = trans.loc['allbed_lower'].values.tolist()\n",
    "    allbed_upper_list = trans.loc['allbed_upper'].values.tolist()\n",
    "    ICUbed_mean_list = trans.loc['ICUbed_mean'].values.tolist()\n",
    "    ICUbed_lower_list = trans.loc['ICUbed_lower'].values.tolist()\n",
    "    ICUbed_upper_list = trans.loc['ICUbed_upper'].values.tolist()\n",
    "    InvVen_mean_list = trans.loc['InvVen_mean'].values.tolist()\n",
    "    InvVen_lower_list = trans.loc['InvVen_lower'].values.tolist()\n",
    "    InvVen_upper_list = trans.loc['InvVen_upper'].values.tolist()\n",
    "    deaths_mean_list = trans.loc['deaths_mean'].values.tolist()\n",
    "    deaths_lower_list = trans.loc['deaths_lower'].values.tolist()\n",
    "    deaths_upper_list = trans.loc['deaths_upper'].values.tolist()\n",
    "    admis_mean_list = trans.loc['admis_mean'].values.tolist()\n",
    "    admis_lower_list = trans.loc['admis_lower'].values.tolist()\n",
    "    admis_upper_list = trans.loc['admis_upper'].values.tolist()\n",
    "    newICU_mean_list = trans.loc['newICU_mean'].values.tolist()\n",
    "    newICU_lower_list = trans.loc['newICU_lower'].values.tolist()\n",
    "    newICU_upper_list = trans.loc['newICU_upper'].values.tolist()\n",
    "    totdea_mean_list = trans.loc['totdea_mean'].values.tolist()\n",
    "    totdea_lower_list = trans.loc['totdea_lower'].values.tolist()\n",
    "    totdea_upper_list = trans.loc['totdea_upper'].values.tolist()\n",
    "    deaths_mean_smoothed_list = trans.loc['deaths_mean_smoothed'].values.tolist()\n",
    "    deaths_lower_smoothed_list = trans.loc['deaths_lower_smoothed'].values.tolist()\n",
    "    deaths_upper_smoothed_list = trans.loc['deaths_upper_smoothed'].values.tolist()\n",
    "    totdea_mean_smoothed_list = trans.loc['totdea_mean_smoothed'].values.tolist()\n",
    "    totdea_lower_smoothed_list = trans.loc['totdea_lower_smoothed'].values.tolist()\n",
    "    totdea_upper_smoothed_list = trans.loc['totdea_upper_smoothed'].values.tolist()\n",
    "    total_tests_data_type_list = trans.loc['total_tests_data_type'].values.tolist()\n",
    "    total_tests_list = trans.loc['total_tests'].values.tolist()\n",
    "    confirmed_infections_list = trans.loc['confirmed_infections'].values.tolist()\n",
    "    est_infections_mean_list = trans.loc['est_infections_mean'].values.tolist()\n",
    "    est_infections_lower_list = trans.loc['est_infections_lower'].values.tolist()\n",
    "    est_infections_upper_list = trans.loc['est_infections_upper'].values.tolist()\n",
    "    \n",
    "    # Add lists to lists\n",
    "    dates_listOlists.append(dates_list)\n",
    "    allbed_mean_listOlists.append(allbed_mean_list)\n",
    "    allbed_lower_listOlists.append(allbed_lower_list)\n",
    "    allbed_upper_listOlists.append(allbed_upper_list)\n",
    "    ICUbed_mean_listOlists.append(ICUbed_mean_list)\n",
    "    ICUbed_lower_listOlists.append(ICUbed_lower_list)\n",
    "    ICUbed_upper_listOlists.append(ICUbed_upper_list)\n",
    "    InvVen_mean_listOlists.append(InvVen_mean_list)\n",
    "    InvVen_lower_listOlists.append(InvVen_lower_list)\n",
    "    InvVen_upper_listOlists.append(InvVen_upper_list)\n",
    "    deaths_mean_listOlists.append(deaths_mean_list)\n",
    "    deaths_lower_listOlists.append(deaths_lower_list)\n",
    "    deaths_upper_listOlists.append(deaths_upper_list)\n",
    "    admis_mean_listOlists.append(admis_mean_list)\n",
    "    admis_lower_listOlists.append(admis_lower_list)\n",
    "    admis_upper_listOlists.append(admis_upper_list)\n",
    "    newICU_mean_listOlists.append(newICU_mean_list)\n",
    "    newICU_lower_listOlists.append(newICU_lower_list)\n",
    "    newICU_upper_listOlists.append(newICU_upper_list)\n",
    "    totdea_mean_listOlists.append(totdea_mean_list)\n",
    "    totdea_lower_listOlists.append(totdea_lower_list)\n",
    "    totdea_upper_listOlists.append(totdea_upper_list)\n",
    "    deaths_mean_smoothed_listOlists.append(deaths_mean_smoothed_list)\n",
    "    deaths_lower_smoothed_listOlists.append(deaths_lower_smoothed_list)\n",
    "    deaths_upper_smoothed_listOlists.append(deaths_upper_smoothed_list)\n",
    "    totdea_mean_smoothed_listOlists.append(totdea_mean_smoothed_list)\n",
    "    totdea_lower_smoothed_listOlists.append(totdea_lower_smoothed_list)\n",
    "    totdea_upper_smoothed_listOlists.append(totdea_upper_smoothed_list)\n",
    "    total_tests_data_type_listOlists.append(total_tests_data_type_list)\n",
    "    total_tests_listOlists.append(total_tests_list)\n",
    "    confirmed_infections_listOlists.append(confirmed_infections_list)\n",
    "    est_infections_mean_listOlists.append(est_infections_mean_list)\n",
    "    est_infections_lower_listOlists.append(est_infections_lower_list)\n",
    "    est_infections_upper_listOlists.append(est_infections_upper_list)    \n",
    "\n",
    "# Results in error ValueError: Length of values does not match length of index\n",
    "imhe_hospitalizations_reduced['dates'] = dates_listOlists\n",
    "imhe_hospitalizations_reduced['allbed_mean'] = allbed_mean_listOlists\n",
    "imhe_hospitalizations_reduced['allbed_lower'] = allbed_lower_listOlists\n",
    "imhe_hospitalizations_reduced['allbed_upper'] = allbed_upper_listOlists\n",
    "imhe_hospitalizations_reduced['ICUbed_mean'] = ICUbed_mean_listOlists\n",
    "imhe_hospitalizations_reduced['ICUbed_lower'] = ICUbed_lower_listOlists\n",
    "imhe_hospitalizations_reduced['ICUbed_upper'] = ICUbed_upper_listOlists\n",
    "imhe_hospitalizations_reduced['InvVen_mean'] = InvVen_mean_listOlists\n",
    "imhe_hospitalizations_reduced['InvVen_lower'] = InvVen_lower_listOlists\n",
    "imhe_hospitalizations_reduced['InvVen_upper'] = InvVen_upper_listOlists\n",
    "imhe_hospitalizations_reduced['deaths_mean'] = deaths_mean_listOlists\n",
    "imhe_hospitalizations_reduced['deaths_lower'] = deaths_lower_listOlists\n",
    "imhe_hospitalizations_reduced['deaths_upper'] = deaths_upper_listOlists\n",
    "imhe_hospitalizations_reduced['admis_mean'] = admis_mean_listOlists\n",
    "imhe_hospitalizations_reduced['admis_lower'] = admis_lower_listOlists\n",
    "imhe_hospitalizations_reduced['admis_upper'] = admis_upper_listOlists\n",
    "imhe_hospitalizations_reduced['newICU_mean'] = newICU_mean_listOlists\n",
    "imhe_hospitalizations_reduced['newICU_lower'] = newICU_lower_listOlists\n",
    "imhe_hospitalizations_reduced['newICU_upper'] = newICU_upper_listOlists\n",
    "imhe_hospitalizations_reduced['totdea_mean'] = totdea_mean_listOlists\n",
    "imhe_hospitalizations_reduced['totdea_lower'] = totdea_lower_listOlists\n",
    "imhe_hospitalizations_reduced['totdea_upper'] = totdea_upper_listOlists\n",
    "imhe_hospitalizations_reduced['deaths_mean_smoothed'] = deaths_mean_smoothed_listOlists\n",
    "imhe_hospitalizations_reduced['deaths_lower_smoothed'] = deaths_lower_smoothed_listOlists\n",
    "imhe_hospitalizations_reduced['deaths_upper_smoothed'] = deaths_upper_smoothed_listOlists\n",
    "imhe_hospitalizations_reduced['totdea_mean_smoothed'] = totdea_mean_smoothed_listOlists\n",
    "imhe_hospitalizations_reduced['totdea_lower_smoothed'] = totdea_lower_smoothed_listOlists\n",
    "imhe_hospitalizations_reduced['totdea_upper_smoothed'] = totdea_upper_smoothed_listOlists\n",
    "imhe_hospitalizations_reduced['total_tests_data_type'] = total_tests_data_type_listOlists\n",
    "imhe_hospitalizations_reduced['total_tests'] = total_tests_listOlists\n",
    "imhe_hospitalizations_reduced['confirmed_infections'] = confirmed_infections_listOlists\n",
    "imhe_hospitalizations_reduced['est_infections_mean'] = est_infections_mean_listOlists\n",
    "imhe_hospitalizations_reduced['est_infections_lower'] = est_infections_lower_listOlists\n",
    "imhe_hospitalizations_reduced['est_infections_upper'] = est_infections_upper_listOlists\n",
    "\n",
    "# Write out file to disk\n",
    "imhe_hospitalizations_fname = data_dir + \"imhe_hospitalizations.csv\"\n",
    "print(\" - IMHE hospitalization level summary data exported to \", imhe_summary_fname)\n",
    "imhe_hospitalizations_reduced.to_csv(imhe_hospitalizations_fname, index=False)\n",
    "\n",
    "# Present summary data for local area\n",
    "print(\"\\nIMHE SUMMARY DATA IN imhe_hospitalizations_reduced() FOR MN and ND\")\n",
    "imhe_hospitalizations_local = imhe_hospitalizations_reduced[(imhe_hospitalizations_reduced.FIPS == MNFIPS) | (imhe_hospitalizations_reduced.FIPS == NDFIPS) ]\n",
    "print(imhe_hospitalizations_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIPS-matched IMHE data exported here!\n",
    "\n",
    "**Note to Developers:** IMHE data has a few blank (`NaN`) entries for dates, presumably reflecting unknown values.  Also, some of the dates are from 2019, which suggests no known values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Times Data on Probable Deaths/Cases (FIPS Present)\n",
    "\n",
    "The NY Times has assembled data on COVID in a GitHub repository at https://github.com/nytimes/covid-19-data.  I have not examined that data yet, but it may well be interesting.\n",
    "\n",
    "Note their statement requiring credit:\n",
    "\n",
    "> In light of the current public health emergency, The New York Times Company is\n",
    "providing this database under the following free-of-cost, perpetual,\n",
    "non-exclusive license. Anyone may copy, distribute, and display the database, or\n",
    "any part thereof, and make derivative works based on it, provided  (a) any such\n",
    "use is for non-commercial purposes only and (b) credit is given to The New York\n",
    "Times in any public display of the database, in any publication derived in part\n",
    "or in full from the database, and in any other public use of the data contained\n",
    "in or derived from the database.\n",
    "\n",
    "Data is available at county, state, and national levels for live numbers (current cases/deaths as well as probable cases/deaths, updated daily).  That said, at least locally I don't think Probable cases are really making a difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Retrieve the NYT datafiles to see what is there that might be of interest\n",
    "##\n",
    "\n",
    "# Update the NYT Datafiles\n",
    "NYTdata_dir = \"NYT_Data/\"\n",
    "g = git.cmd.Git(NYTdata_dir)\n",
    "# We should check status to see everything is good eventually, \n",
    "# for now, I am using this to hide the status message from GitPython module\n",
    "status = g.pull()  \n",
    "\n",
    "# Grab the live data files\n",
    "live_county_csv = NYTdata_dir+\"live/us-counties.csv\"\n",
    "live_state_csv = NYTdata_dir+\"live/us-states.csv\"\n",
    "live_us_csv = NYTdata_dir+\"live/us.csv\"\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files (contains number of confirmed/deaths/recovered on that date)\n",
    "live_county_df = pd.read_csv(live_county_csv)   # County totals\n",
    "live_state_df = pd.read_csv(live_state_csv)    # State totals\n",
    "live_us_df = pd.read_csv(live_us_csv)       # National totals\n",
    "\n",
    "# Print county data to screen\n",
    "print(\"LOCAL COUNTY DATA IN live_county_df() DATAFRAME\")\n",
    "print(live_county_df[ (live_county_df['fips'] == ClayFIPS) | (live_county_df['fips'] == CassFIPS) ])\n",
    "\n",
    "# Print state level data to screen\n",
    "print(\"\\nLOCAL STATE DATA IN live_state_df() DATAFRAME\")\n",
    "print(live_state_df[ (live_state_df['fips'] == MNFIPS) | (live_state_df['fips'] == NDFIPS) ])\n",
    "\n",
    "# Print national data\n",
    "print(\"\\nNATIONAL DATA IN live_us_df() DATAFRAME\")\n",
    "print(live_us_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
