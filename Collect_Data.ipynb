{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Condensing COVID Data\n",
    "\n",
    "This Jupyter notebook reads in the data from a variety of online sources that we need for the COVID Data Vizualization Project.  Some attempts are made to produce simpler to work with output files.  Depending on how long this notebook takes\n",
    "to execute, it may not make sense to 'condense' the data first.\n",
    "\n",
    "- **A Note on the use of Pandas:** I am currently using `Pandas` (aka Python Data Analysis Library, see https://pandas.pydata.org) to read in the CSV files and manipualte them.  This has advantages and annoyances, there may be much better ways to do this, but I was giving this a try for now.\n",
    "\n",
    "- **A Note about FIPS:** Some of the data includes FIPS codes (a standard geographic identifier) which should ease the process of cross-matching of data.  Clay County is 27027 and Cass County is 38017.  Minnesota is 27, North Dakota is 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import git\n",
    "import requests\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables of interest below\n",
    "data_dir = 'our_data/'    # Data directory for files we created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Census Data on Populations of States/Counties (FIPS Present)\n",
    "\n",
    "This data from the US Census Bureau estimates the population in July 2019.  Description of the file format is at https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf\n",
    "\n",
    "- **County Level Data**: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\n",
    "- **State Level Data**: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve state level data (restricted to certain columns)\n",
    "## When I retrieved the files, I got an error that `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 2: invalid continuation byte`, turns out it is encoded `latin-1`.\n",
    "#census_state_csv = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/national/totals/nst-est2019-alldata.csv\"\n",
    "#state_columns_of_interest = {'STATE', 'NAME', 'CENSUS2010POP', 'N_POPCHG2019', 'POPESTIMATE2019'}\n",
    "#census_state_df = pd.read_csv(census_state_csv, usecols=state_columns_of_interest, encoding='latin-1')    # County totals\n",
    "\n",
    "# Create pandas dataframes containing the selected population data for each state/county\n",
    "census_county_csv = \"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\"\n",
    "county_columns_of_interest = {'STATE', 'COUNTY', 'STNAME', 'CTYNAME', 'NPOPCHG_2019', 'POPESTIMATE2019'}\n",
    "census_county_df = pd.read_csv(census_county_csv,usecols=county_columns_of_interest, encoding='latin-1')  \n",
    "\n",
    "# Separate state level data from county level data (by creating separate copies in memory)\n",
    "county_data_df = census_county_df[census_county_df['COUNTY'] != 0].copy()\n",
    "state_data_df = census_county_df[census_county_df['COUNTY'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate the state-leve data\n",
    "\n",
    "# Add FIPS column for state data then DROP county data and move FIPS to first column before exporting\n",
    "state_data_df['FIPS'] = state_data_df['STATE']\n",
    "state_data_df.drop(columns=['STATE','COUNTY','CTYNAME'], inplace=True)\n",
    "state_data_df = state_data_df.reindex(columns=(['FIPS'] + list([col for col in state_data_df.columns if col != 'FIPS']) ))\n",
    "\n",
    "# Compute percent change in population in 2018-19\n",
    "state_data_df['PPOPCHG_2019'] = 100*(state_data_df['NPOPCHG_2019']/state_data_df['POPESTIMATE2019'])\n",
    "\n",
    "# We may want to do a daily extrapolation of population since POPESTIMATE2019 is est. population on July 1, 2019 \n",
    "# and NPOPCHG_2019 is the estimated change between July 1, 2018 and July 1, 2019.  Realistically, this is probably\n",
    "# overkill since the increased deaths from Coronavirus are not taken into account in such an extrapolation.\n",
    "\n",
    "# Save the processed data file\n",
    "out_states = data_dir + \"population_data_states.csv\"\n",
    "state_data_df.to_csv(out_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the local state level population data\n",
    "state_data_df[(state_data_df['STNAME'] == 'Minnesota') | (state_data_df['STNAME'] == 'North Dakota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In county data create FIPS column, remove redundant columns, and then move FIPS columns to first column\n",
    "county_data_df['FIPS'] = county_data_df['STATE']*1000 + county_data_df['COUNTY']\n",
    "county_data_df.drop(columns=['STATE','COUNTY'], inplace=True)\n",
    "county_data_df = county_data_df.reindex(columns=(['FIPS'] + list([col for col in county_data_df.columns if col != 'FIPS']) ))\n",
    "\n",
    "# Compute percent change in population in 2018-19\n",
    "county_data_df['PPOPCHG_2019'] = 100*(county_data_df['NPOPCHG_2019']/county_data_df['POPESTIMATE2019'])\n",
    "\n",
    "# We may want to do a daily extrapolation of population since POPESTIMATE2019 is est. population on July 1, 2019 \n",
    "# and NPOPCHG_2019 is the estimated change between July 1, 2018 and July 1, 2019.  Realistically, this is probably\n",
    "# overkill since the increased deaths from Coronavirus are not taken into account in such an extrapolation.\n",
    "\n",
    "# Save the processed data file\n",
    "out_counties = data_dir + \"population_data_counties.csv\"\n",
    "county_data_df.to_csv(out_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the local county level population data\n",
    "county_data_df[(county_data_df['FIPS'] == 27027) | (county_data_df['FIPS'] == 38017)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Novel Coronavirus (COVID-19) Cases Data (FIPS Present)\n",
    "    - https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases\n",
    "\n",
    "This dataset is part of COVID-19 Pandemic Novel Corona Virus (COVID-19)\n",
    "epidemiological data since 22 January 2020. The data is compiled by the Johns\n",
    "Hopkins University Center for Systems Science and Engineering (JHU CCSE) from\n",
    "various sources including the World Health Organization (WHO), DXY.cn, BNO\n",
    "News, National Health Commission of the Peopleâ€™s Republic of China (NHC),\n",
    "China CDC (CCDC),Hong Kong Department of Health, Macau Government, Taiwan\n",
    "CDC, US CDC, Government of Canada, Australia Government Department of Health,\n",
    "European Centre for Disease Prevention and Control (ECDC), Ministry of Health\n",
    "Singapore (MOH), and others. JHU CCSE maintains the data on the 2019 Novel\n",
    "Coronavirus COVID-19 (2019-nCoV) Data Repository on Github\n",
    "(https://github.com/CSSEGISandData/COVID-19).\n",
    "\n",
    "### Notes about the data files:\n",
    "- County level daily confirmed/deaths/recovered data files changes format\n",
    "  several times before April 23, 2020, so I didn't include that data. \n",
    "- State level daily data files contain some additional data that county level\n",
    "  files do not contain, notably Incident_Rate, People_Tested,\n",
    "  People_Hospitalized, Mortality_Rate, Testing_Rate, and Hospitalization_Rate. \n",
    "- Time-series data files contain more limited data (only confirmed cases and\n",
    "  deaths) and are essentially redundant data compared to the daily files, so\n",
    "  combining the daily files makes sense.\n",
    "\n",
    "### Notes about this process: \n",
    "- By processing the US Census Bureau's population data first, I can get a list\n",
    "  of legitimate 'FIPS' values to use, so that I can perform 'left' joins to a\n",
    "  list of FIPS addresses instead of costly 'outer' joins. \n",
    "- It turns out there are DUPLICATE FIPS entries for some of the John Hopkins\n",
    "  data.  So I went through those and picked out the ones with the higher values\n",
    "  for confirmed/deaths/recovered assuming they were entered later. \n",
    "- Once that was done, I constructed a SINGLE Pandas dataframe holding all the\n",
    "  time-series data and wrote it to a CSV file. Accessing this one file will be a\n",
    "  lot faster than looping through all the datafiles each time we load up the\n",
    "  data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the John Hopkins data directory that I need to update\n",
    "JHdata_dir = \"JH_Data/\"\n",
    "\n",
    "# Git pull to sync up the data set to the current version on GitHub\n",
    "g = git.cmd.Git(JHdata_dir)\n",
    "status = g.pull()  # We should check status to see everything is good eventually, for now, I am using this to hide the status message from GitPython module\n",
    "\n",
    "# Daily tabulation of all confirmed/deaths/recovered data is in the following directories\n",
    "daily_cnty_dir = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports/\" # For each admin unit (in the US, that's county) for each day.\n",
    "daily_state_dir = JHdata_dir+\"csse_covid_19_data/csse_covid_19_daily_reports_us/\" # For each state (somewhat redundant, but avoids recomputation I suppose)\n",
    "\n",
    "# Individual time series data for confirmed cases and deaths in the US counties and states\n",
    "ts_us_confirmed_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n",
    "ts_us_dead_csv = JHdata_dir+\"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define various functions we will use below\n",
    "##\n",
    "\n",
    "def csvfiles(path):\n",
    "    contents = os.listdir(path);\n",
    "    csvs = [ file for file in contents if file.endswith(\".csv\") ]\n",
    "    \n",
    "    for file in sorted(csvs, key=date2num):\n",
    "        yield file\n",
    "\n",
    "        \n",
    "def isodate2num(s):\n",
    "    # Takes input filename of CSV file and returns it sorted by date implies assuming\n",
    "    # filenames with MM-DD-YYYY.csv format, returning YYYY+MM+DD for sorting purposes.\n",
    "    M = int(s[0:2])\n",
    "    D = int(s[3:5])\n",
    "    Y = int(s[6:10])\n",
    "    return (f\"{Y:04d}-{M:02d}-{D:02d}\")\n",
    "\n",
    "\n",
    "def date2num(s):\n",
    "    # Takes input filename of CSV file and returns it sorted by date implies assuming\n",
    "    # filenames with MM-DD-YYYY.csv format, returning YYYY+MM+DD for sorting purposes.\n",
    "    M = int(s[0:2])\n",
    "    D = int(s[3:5])\n",
    "    Y = int(s[6:10])\n",
    "    return (f\"{Y:04d}{M:02d}{D:02d}\")\n",
    "\n",
    "\n",
    "def iso2days(iso):\n",
    "    # To make computing the derivative easy, compute days since January 1, 2020\n",
    "    ref = datetime.fromisoformat(\"2020-01-01\")\n",
    "    return (datetime.fromisoformat(iso) - ref)/timedelta(days=1)\n",
    "\n",
    "\n",
    "def derivative(x, y):\n",
    "    \"\"\"\n",
    "    Compute forward difference estimate for the derivative of y with respect\n",
    "    to x.  The x and y arrays are 2-D with the rows being different data sets\n",
    "    and the columns being the x/y values in each data set.\n",
    "    \n",
    "    The input and must be the same size.\n",
    "\n",
    "    Note that we copy the first known derivative values into the zeroth column, since \n",
    "    the derivatve for the first point is not a known value.\n",
    "    \"\"\"\n",
    "    # Compute the numerator (y[i+1] - y[i]) for all rows in the entire array at once\n",
    "    dy = y[:, 1:] - y[:, 0:-1]\n",
    "    # Compute the denominator (x[i+1] - x[i]) for all rows in the for entire array at once\n",
    "    dx = x[:, 1:] - x[:, 0:-1]\n",
    "    # Compute the derivatives for all points in the array at once\n",
    "    dydx = dy / dx\n",
    "    # Get first column to horizontal stack with numpy array\n",
    "    first_col = dydx[:,0][..., None] # The [..., None] bit keeps (n, 1) shape (a la https://stackoverflow.com/questions/15815854/how-to-add-column-to-numpy-array)\n",
    "    return np.hstack((first_col, dydx))\n",
    "\n",
    "\n",
    "def reduce_local_dataframe(raw_df, fips_df):\n",
    "    # Reduce the raw dataframe of all COVID data to just the relevant entries for the US\n",
    "    # Match to the list of known FIPS values from Census data\n",
    "    \n",
    "    # Perform a left join to the FIPS list\n",
    "    reduced_df = fips_df.join(raw_df.set_index('FIPS'), on='FIPS', how='left').copy()\n",
    "\n",
    "    # Here's the fund part, turns out the John Hopkins data has duplicate lines for a few FIPS\n",
    "    # entries, so what I did was assume the HIGHER values of COVID confirmed, deaths, and recovered\n",
    "    # were the accurate ones and just kept those.  This meant checking all the duplicates and then\n",
    "    # purging them, re-writing the best values into the row of data.\n",
    "\n",
    "    update_confirmed = {}\n",
    "    update_deaths = {}\n",
    "    update_recovered = {}\n",
    "    # Identify duplicate FIPS entries (assumed mangled data)\n",
    "    duplicates = reduced_df[reduced_df.duplicated(['FIPS'], keep='first')]\n",
    "    # Loop through all data for each duplicate FIPS entry\n",
    "    for index, row in duplicates.iterrows():\n",
    "        max_confirmed = 0\n",
    "        max_deaths = 0\n",
    "        max_recovered = 0\n",
    "        checkFIPS = row['FIPS']\n",
    "        # Track the maximum value of confirmed/deaths/recovered for each FIPS\n",
    "        for dup_ind, dup_row in reduced_df[reduced_df['FIPS'] == row['FIPS']].iterrows():\n",
    "            if max_confirmed<dup_row['Confirmed']:\n",
    "                max_confirmed = dup_row['Confirmed']\n",
    "            if max_deaths<dup_row['Deaths']:\n",
    "                max_deaths = dup_row['Deaths']\n",
    "            if max_recovered<dup_row['Recovered']:\n",
    "                max_recovered = dup_row['Recovered']\n",
    "        update_confirmed.update( {checkFIPS : max_confirmed} )\n",
    "        update_deaths.update( {checkFIPS : max_deaths} )\n",
    "        update_recovered.update( {checkFIPS : max_recovered} )\n",
    "\n",
    "    # Drop duplicates from pandas dataframe\n",
    "    reduced_df.drop_duplicates(subset='FIPS', keep='first', inplace=True)\n",
    "\n",
    "    # Fix values in duplicate lines\n",
    "    for key in update_confirmed:\n",
    "        reduced_df.loc[(reduced_df['FIPS'] == key), ['Confirmed', 'Deaths', 'Recovered']] = [update_confirmed[key], update_deaths[key], update_recovered[key]]\n",
    "        \n",
    "    # Return the fixed dataframe\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## This is mostly just showing I can just grab a single datasets for the most recent daily data instead of \n",
    "## trying to grab everything and put it all together.  However, this only allows printing some data\n",
    "##\n",
    "\n",
    "# Grab complete list of all csvs to then...\n",
    "world_csvs = list(csvfiles(daily_cnty_dir))\n",
    "us_csvs = list(csvfiles(daily_state_dir))\n",
    "\n",
    "# .. grab the most recent CSV file to open the data.\n",
    "daily_world_csv = daily_cnty_dir+world_csvs[-1]\n",
    "daily_us_csv = daily_state_dir+us_csvs[-1]\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files \n",
    "# (contains number of confirmed/deaths/recovered on that date)\n",
    "daily_world_df = pd.read_csv(daily_world_csv)   # County/Admin totals\n",
    "daily_us_df = pd.read_csv(daily_us_csv)         # State totals\n",
    "\n",
    "# Print county data to screen\n",
    "print(\"LOCAL COUNTY DATA\")\n",
    "print(daily_world_df[ (daily_world_df['FIPS'] == 27027) | (daily_world_df['FIPS'] == 38017) ])\n",
    "\n",
    "# Print state level data to screen (which has data on testing and hospitalization rates)\n",
    "print(\"\\nLOCAL STATE DATA\")\n",
    "print(daily_us_df[ (daily_us_df['FIPS'] == 27) | (daily_us_df['FIPS'] == 38) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Build combined county-level datafiles\n",
    "##\n",
    "\n",
    "# Build a dataframe containing legitimate FIPS values using county level data\n",
    "fips_df = county_data_df.copy()\n",
    "fips_df.drop(columns=['STNAME', 'CTYNAME', 'POPESTIMATE2019', 'NPOPCHG_2019', 'PPOPCHG_2019'], inplace=True)\n",
    "\n",
    "# Scan through the more complete daily files of county level data and construct a single datafile for our use (restricting \n",
    "# to US only).  It turnss out the format of these local level files changes with the date.  The files before March 22, 2020 \n",
    "# were in several different forms and the placenames were NOT consistent.  Thus, to make things managable, I am ignoring\n",
    "# that early local level data...\n",
    "sorted_csvs = []\n",
    "dates_list = []\n",
    "for file in csvfiles(daily_cnty_dir):\n",
    "    # County level date only starts on March 22, before then it is a mis-mosh of place names in the Province_State field\n",
    "    # So only keep that data\n",
    "    this_isodate = isodate2num(file)\n",
    "    this_date = date2num(file)\n",
    "    if (int(this_date) >= 20200322):\n",
    "        # Append to list of accessed csv files\n",
    "        sorted_csvs.append(daily_cnty_dir+file)\n",
    "\n",
    "        # Grab the data from the CSV file\n",
    "        raw_df = pd.read_csv(sorted_csvs[-1])\n",
    "\n",
    "        # Rename columns in early forms to late forms of column names for consistency\n",
    "        raw_df.rename(columns={ 'Province/State': 'Province_State', \n",
    "                               'Country/Region':'Country_Region', \n",
    "                               'Last Update':'Last_Update' }, inplace = True)\n",
    "        \n",
    "        # Match to the list of known FIPS values from Census data, also removing duplicate rows\n",
    "        reduced_df = reduce_local_dataframe(raw_df, fips_df)\n",
    "        \n",
    "        # Provide progress report\n",
    "        idx = len(sorted_csvs)\n",
    "        # print(f'Processing Date #{idx}: {this_isodate}')\n",
    "        \n",
    "        if (idx == 1):\n",
    "            # Create combined dataframe sorted by FIPS\n",
    "            combined_cols = ['FIPS', 'Admin2', 'Province_State', 'Lat', 'Long_']\n",
    "            combined_cnty_df = reduced_df[combined_cols].copy()\n",
    "            confirmed_df = fips_df.copy()\n",
    "            deaths_df = fips_df.copy()\n",
    "            recovered_df = fips_df.copy()\n",
    "            \n",
    "        ## Create dataframes for temporarily storing time series date\n",
    "        # Append date to list of dates\n",
    "        dates_list.append(this_isodate)\n",
    "            \n",
    "        # Store Confirmed by merging reduced list and renaming column\n",
    "        confirmed_df = pd.merge(confirmed_df,reduced_df[['FIPS','Confirmed']],on='FIPS', how='left', copy=True)\n",
    "        confirmed_col = \"C\"+f\"{idx:03d}\"\n",
    "        confirmed_df.rename(columns={'Confirmed': confirmed_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "        # Store Deaths by merging reduced list and renaming column\n",
    "        deaths_df = pd.merge(deaths_df,reduced_df[['FIPS','Deaths']],on='FIPS', how='left', copy=True)\n",
    "        deaths_col = \"D\"+f\"{idx:03d}\"\n",
    "        deaths_df.rename(columns={'Deaths': deaths_col}, errors=\"raise\", inplace=True)\n",
    "        \n",
    "        # Store Recovered by merging reduced list and renaming column\n",
    "        recovered_df = pd.merge(recovered_df,reduced_df[['FIPS','Recovered']],on='FIPS', how='left', copy=True)\n",
    "        recovered_col = \"R\"+f\"{idx:03d}\"\n",
    "        recovered_df.rename(columns={'Recovered': recovered_col}, errors=\"raise\", inplace=True)\n",
    "            \n",
    "# Final cleanup (convert to integers and remove NaN)\n",
    "confirmed_df = confirmed_df.replace(np.nan,0).astype('int')\n",
    "deaths_df= deaths_df.replace(np.nan,0).astype('int')\n",
    "recovered_df= recovered_df.replace(np.nan,0).astype('int')\n",
    "\n",
    "# Add lists of dates to the combined dataframe as a single 'Dates' column\n",
    "combined_cnty_df['Dates'] = [dates_list]*len(combined_cnty_df)\n",
    "# Add time-series list of confirmed to the combined dataframe as a single 'Confirmed' column\n",
    "confirmed_listOlists = confirmed_df[ confirmed_df.columns[confirmed_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_cnty_df['Confirmed'] = confirmed_listOlists\n",
    "# Add time-series list of deaths to the combined dataframe as a single 'Deaths' column\n",
    "deaths_listOlists = deaths_df[ deaths_df.columns[deaths_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_cnty_df['Deaths'] = deaths_listOlists\n",
    "# Add time-series list of recovered to the combined dataframe as a single 'Recovered' column\n",
    "recovered_listOlists = recovered_df[ recovered_df.columns[recovered_df.columns!='FIPS'] ].values.tolist()\n",
    "combined_cnty_df['Recovered'] = recovered_listOlists\n",
    "\n",
    "# Convert the list of dates into numpy array of days since Jan. 1, 2020 for each observation\n",
    "dates = combined_cnty_df[combined_cnty_df['FIPS'] == 27027]['Dates'].tolist()[0]\n",
    "dates_list = []\n",
    "for dat in dates:\n",
    "    dates_list.append( iso2days(dat) )\n",
    "dates_arr = np.array([dates_list]*len(combined_cnty_df))\n",
    "\n",
    "# Convert confirmed/deaths/recovered into arrays\n",
    "confirmed_arr = np.array(confirmed_listOlists)\n",
    "deaths_arr = np.array(deaths_listOlists)\n",
    "recovered_arr = np.array(recovered_listOlists)\n",
    "# print(dates_arr.shape, confirmed_arr.shape,deaths_arr.shape, recovered_arr.shape)\n",
    "\n",
    "# At this point I have arrays where the rows are individiual FIPS (counties) and the columns are \n",
    "# (depending on the array) the days since 1/1/2020, number of confirmed cases, number of deaths, \n",
    "# and number of recovered.\n",
    "\n",
    "# Compute the derivatives (using forward derivative approach)\n",
    "dconfirmed_arr = derivative(dates_arr, confirmed_arr)\n",
    "ddeaths_arr = derivative(dates_arr, deaths_arr)\n",
    "drecovered_arr= derivative(dates_arr, recovered_arr)\n",
    "\n",
    "# Compute the second derivatives (a bit hinky to use forward derivative again, but...)\n",
    "d2confirmed_arr = derivative(dates_arr, dconfirmed_arr)\n",
    "d2deaths_arr = derivative(dates_arr, ddeaths_arr)\n",
    "d2recovered_arr= derivative(dates_arr, drecovered_arr)\n",
    "\n",
    "# Convert numpy arrays to lists of lists for storage in combined dataframe\n",
    "combined_cnty_df['dConfirmed'] = dconfirmed_arr.tolist()\n",
    "combined_cnty_df['d2Confirmed'] = d2confirmed_arr.tolist()\n",
    "combined_cnty_df['dDeaths'] = ddeaths_arr.tolist()\n",
    "combined_cnty_df['d2Deaths'] = d2deaths_arr.tolist()\n",
    "combined_cnty_df['dRecovered'] = drecovered_arr.tolist()\n",
    "combined_cnty_df['d2CRecovered'] = d2recovered_arr.tolist()\n",
    "\n",
    "# Add population data to same array\n",
    "combined_cnty_df = pd.merge(combined_cnty_df,county_data_df[['FIPS','POPESTIMATE2019', 'NPOPCHG_2019']], on='FIPS', how='left', copy=True)\n",
    "\n",
    "# Rename some columns before export\n",
    "combined_cnty_df.rename(columns={ 'Admin2': 'County', \n",
    "                                 'Province_State': 'State', \n",
    "                                  'POPESTIMATE2019' : 'PopEst2019',\n",
    "                                  'NPOPCHG_2019' : 'PopChg2019'}, inplace = True)\n",
    "\n",
    "# Save the processed time-series data into single file\n",
    "combined_datafile = data_dir + \"countylevel_combinedCDR.csv\"\n",
    "combined_cnty_df.to_csv(combined_datafile)\n",
    "\n",
    "# Clear variables\n",
    "del sorted_csvs, dates_list\n",
    "del fips_df, raw_df, confirmed_df, deaths_df, recovered_df\n",
    "del dates_arr, confirmed_arr, deaths_arr, recovered_arr\n",
    "del dconfirmed_arr, ddeaths_arr, drecovered_arr, d2confirmed_arr, d2deaths_arr, d2recovered_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cnty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show demonstrations of plotting this data here (NEED TO ADD THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Load the time series datafiles to experiment with them.  These only contain Deaths and Confirmed cases,\n",
    "## so I suspect we won't keep them, since I build the same data from the daily files above.\n",
    "##\n",
    "\n",
    "# Create pandas dataframes containing time-series data (We could reconstruct this by looping through all the daily data, since this is missing number of recovered)\n",
    "ts_us_dead_df = pd.read_csv(ts_us_dead_csv)            # Deaths in time series\n",
    "ts_us_confirmed_df = pd.read_csv(ts_us_confirmed_csv)  # Confirmed in time series\n",
    "\n",
    "# We could transpose the dataframe to allow easier extraction of time series data on a per county level\n",
    "tmp_df = ts_us_confirmed_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"confirmed\" }, inplace = True)\n",
    "confirmed_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')]  # Use pattern matching to find real dates and include\n",
    "\n",
    "tmp_df = ts_us_dead_df[ (ts_us_confirmed_df['Province_State'] == 'Minnesota') & (ts_us_confirmed_df['Admin2'] == 'Clay') ].T\n",
    "tmp_df.rename(columns={ tmp_df.columns[0]: \"dead\" }, inplace = True)\n",
    "dead_clay = tmp_df[tmp_df.index.str.match('[0-9]*/[0-9]*/[0-9]*')] # Use pattern matching to find real dates and include\n",
    "\n",
    "# Merge the confirmed ill and dead into one dataframe (would like recovered too, but that's not in\n",
    "# these times series files)\n",
    "merged_clay = confirmed_clay.merge(dead_clay, left_index=True, right_index=True)\n",
    "merged_clay.plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_clay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Mobility Data (NO FIPS Present)\n",
    "\n",
    "This data is described at https://www.google.com/covid19/mobility/ and can be downloaded in a single monolithic CSV file at https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n",
    "\n",
    "> The data shows how visitors to (or time spent in) categorized places change compared to our baseline days. A baseline day represents a normal value for that day of the week. The baseline day is the median value from the 5â€‘week period Jan 3 â€“ Feb 6, 2020.\n",
    "\n",
    "> For each region-category, the baseline isnâ€™t a single valueâ€”itâ€™s 7 individual values. The same number of visitors on 2 different days of the week, result in different percentage changes. So, we recommend the following:\n",
    "1. Donâ€™t infer that larger changes mean more visitors or smaller changes mean less visitors.\n",
    "2. Avoid comparing day-to-day changes. Especially weekends with weekdays. (https://support.google.com/covid19-mobility/answer/9824897?hl=en&ref_topic=9822927)\n",
    "\n",
    "> Note, *Parks* typically means official national parks and not the general outdoors found in rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Mobility Data URL\n",
    "goog_mobility_csv_url = \"https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\"\n",
    "goog_mobility_df=pd.read_csv(goog_mobility_csv_url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_mobility_clay = goog_mobility_df[ (goog_mobility_df['sub_region_1'] == 'Minnesota') & (goog_mobility_df['sub_region_2'] == 'Clay County')]\n",
    "\n",
    "# Notice for Clay county we have NaN reported for Parks (see note above) and Transit Stations\n",
    "goog_mobility_clay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Mobility Data (NO FIPS Present)\n",
    "\n",
    "This data is described at https://www.apple.com/covid19/mobility and can be downloaded in a single monolithic CSV file at https://covid19-static.cdn-apple.com/covid19-mobility-data/2008HotfixDev42/v3/en-us/applemobilitytrends-2020-05-24.csv (That URL is hidden in the mobility page link and appears to be updated regularly.  We may need to scrape the page to identify the link).\n",
    "\n",
    "Apple tracks three kinds of Apple Maps routing requests: Driving, Walking, Transit.  In some areas not all of these types of data are available, for example, in Clay County, only Driving route request data is available.  In fact, it looks like only driving data is available at the county level regardless of the county (at least in Minnesota). Cities can contain additional formation, although only very large cities appear to be in Apple's data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the original Apple page was proving tricky as it had a bunch of javascript used to generate the URL, so I poked around and found a reference \n",
    "# at https://www.r-bloggers.com/get-apples-mobility-data/ to a JSON file at a stable URL that can be used to construct the appropriate URL for the current\n",
    "# datafile.\n",
    "\n",
    "aapl_mobility_json = \"https://covid19-static.cdn-apple.com/covid19-mobility-data/current/v3/index.json\"\n",
    "aapl_server = \"https://covid19-static.cdn-apple.com/\"\n",
    "result = requests.get(aapl_mobility_json)\n",
    "# Proceed if we successfully pulled the page (HTTP status code 200)\n",
    "if (result.status_code == 200):\n",
    "    # Apple Mobility Data URL\n",
    "    jsondata = result.json()\n",
    "    aapl_mobility_csv_url = aapl_server+jsondata['basePath']+jsondata['regions']['en-us']['csvPath']\n",
    "    aapl_mobility_df=pd.read_csv(aapl_mobility_csv_url, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just showing how I can get clay county data specifically\n",
    "aapl_mobility_minneapolis = aapl_mobility_df[(aapl_mobility_df['region'] == 'Minneapolis') & (aapl_mobility_df['sub-region'] == 'Minnesota')]\n",
    "aapl_mobility_clay = aapl_mobility_df[(aapl_mobility_df['region'] == 'Clay County') & (aapl_mobility_df['sub-region'] == 'Minnesota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice only driving information is available here\n",
    "aapl_mobility_clay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice additional information is available for larger cities\n",
    "aapl_mobility_cities = aapl_mobility_df[(aapl_mobility_df['geo_type'] == 'city') & (aapl_mobility_df['country'] == 'United States')]\n",
    "aapl_mobility_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMHE Data on Local Resources\n",
    "\n",
    "There is IMHE data on local resources at http://www.healthdata.org/covid/data-downloads although I am not sure that data is available with county level resolution as I haven't fully investigated it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Times Data on Probable Deaths/Cases (FIPS Present)\n",
    "\n",
    "The NY Times has assembled data on COVID in a GitHub repository at https://github.com/nytimes/covid-19-data.  I have not examined that data yet, but it may well be interesting.\n",
    "\n",
    "Note their statement requiring credit:\n",
    "\n",
    "> In light of the current public health emergency, The New York Times Company is\n",
    "providing this database under the following free-of-cost, perpetual,\n",
    "non-exclusive license. Anyone may copy, distribute, and display the database, or\n",
    "any part thereof, and make derivative works based on it, provided  (a) any such\n",
    "use is for non-commercial purposes only and (b) credit is given to The New York\n",
    "Times in any public display of the database, in any publication derived in part\n",
    "or in full from the database, and in any other public use of the data contained\n",
    "in or derived from the database.\n",
    "\n",
    "Data is available at county, state, and national levels for live numbers (current cases/deaths as well as probable cases/deaths, updated daily).  That said, at least locally I don't think Probable cases are really making a difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the NYT Datafiles\n",
    "NYTdata_dir = \"NYT_Data/\"\n",
    "g = git.cmd.Git(NYTdata_dir)\n",
    "status = g.pull()  # We should check status to see everything is good eventually, for now, I am using this to hide the status message from GitPython module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the live data files\n",
    "live_county_csv = NYTdata_dir+\"live/us-counties.csv\"\n",
    "live_state_csv = NYTdata_dir+\"live/us-states.csv\"\n",
    "live_us_csv = NYTdata_dir+\"live/us.csv\"\n",
    "\n",
    "# Create pandas dataframes containing the daily data from the CSV files (contains number of confirmed/deaths/recovered on that date)\n",
    "live_county_df = pd.read_csv(live_county_csv)   # County totals\n",
    "live_state_df = pd.read_csv(live_state_csv)    # State totals\n",
    "live_us_df = pd.read_csv(live_us_csv)       # National totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_county_df[ ((live_county_df['state'] == 'Minnesota') & (live_county_df['county'] == 'Clay')) | ((live_county_df['state'] == 'North Dakota') & (live_county_df['county'] == 'Cass')) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_state_df[ (live_state_df['state'] == 'Minnesota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
